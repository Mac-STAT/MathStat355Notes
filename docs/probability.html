<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Probability: A Brief Review – MATH/STAT 355: Statistical Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./mle.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-12acdecf0bfd8bf8a7d30417284c8ed4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH/STAT 355: Statistical Theory</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylorokonek/MathematicalStatistics/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./math-stat-355.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Statistical Theory!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./consistency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Computational Optimization</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#concept-questions" id="toc-concept-questions" class="nav-link" data-scroll-target="#concept-questions"><span class="header-section-number">1.2</span> Concept Questions</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">1.3</span> Definitions</a>
  <ul class="collapse">
  <li><a href="#distributions-table" id="toc-distributions-table" class="nav-link" data-scroll-target="#distributions-table"><span class="header-section-number">1.3.1</span> Distributions Table</a></li>
  </ul></li>
  <li><a href="#theorems" id="toc-theorems" class="nav-link" data-scroll-target="#theorems"><span class="header-section-number">1.4</span> Theorems</a>
  <ul class="collapse">
  <li><a href="#transforming-continuous-random-variables" id="toc-transforming-continuous-random-variables" class="nav-link" data-scroll-target="#transforming-continuous-random-variables"><span class="header-section-number">1.4.1</span> Transforming Continuous Random Variables</a></li>
  </ul></li>
  <li><a href="#worked-examples" id="toc-worked-examples" class="nav-link" data-scroll-target="#worked-examples"><span class="header-section-number">1.5</span> Worked Examples</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><em>MATH/STAT 355</em> builds directly on topics covered in <em>MATH/STAT 354: Probability</em>. You’re not expected to perfectly remember everything from <em>Probability</em>, but you will need to have sufficient facility with the following topics covered in this review Chapter in order to grasp the majority of concepts covered in <em>MATH/STAT 355</em>.</p>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you should be able to…</p>
<ul>
<li><p>Distinguish between important probability models (e.g., Normal, Binomial)</p></li>
<li><p>Derive the expectation and variance of a single random variable or a sum of random variables</p></li>
<li><p>Define the moment generating function and use it to find moments or identify pdfs</p></li>
<li><p>Derive pdfs of transformations of continuous random variables</p></li>
</ul>
</section>
<section id="concept-questions" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="concept-questions"><span class="header-section-number">1.2</span> Concept Questions</h2>
<ol type="1">
<li><p>Which probability distributions are appropriate for <em>quantitative</em> (continuous) random variables?</p></li>
<li><p>Which probability distributions are appropriate for <em>categorical</em> random variables?</p></li>
<li><p><em>Independently and Identically Distributed (iid)</em> random variables are an incredibly important assumption involved in many statistical methods. Why do you think it might be important/useful for random variables to have this property?</p></li>
<li><p>Why might we want to be able to derive distribution functions for transformations of random variables? In what scenarios can you imagine this being useful?</p></li>
</ol>
</section>
<section id="definitions" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">1.3</span> Definitions</h2>
<p>You are expected to know the following definitions:</p>
<p><strong>Random Variable</strong></p>
<p>A random variable is a function that takes inputs from a sample space of all possible outcomes, and outputs real values or probabilities. As an example, consider a coin flip. The sample space of all possible outcomes consists of “heads” and “tails”, and each outcome is associated with a probability (50% each, for a fair coin). For our purposes, you should know that random variables have probability density (or mass) functions, and are either discrete or continuous based on the number of possible outcomes a random variable may take. Random variables are often denoted with capital Roman letters, like <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, <span class="math inline">\(Z\)</span>, etc.</p>
<p><strong>Probability density function</strong> (discrete, continuous)</p>
<ul>
<li>Note: I don’t care if you call a pmf a pdf… I will probably do this continuously throughout the semester. We don’t need to be picky about this in <em>MATH/STAT 355</em>.</li>
</ul>
<p>There are many different accepted ways to write the notation for a pdf of a random variables. Any of the following are perfectly appropriate for this class: <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(\pi(x)\)</span>, <span class="math inline">\(p(x)\)</span>, <span class="math inline">\(f_X(x)\)</span>. I typically use either <span class="math inline">\(\pi\)</span> or <span class="math inline">\(p\)</span>, but might mix it up occasionally.</p>
<p>Key things I want you to know about probability density functions:</p>
<ul>
<li><p><span class="math inline">\(\pi(x) \geq 0\)</span>, everywhere. This should make sense (hopefully) because probabilities cannot be negative!</p></li>
<li><p><span class="math inline">\(\int_{-\infty}^\infty \pi(x) = 1\)</span>. This should also (hopefully) makes sense. Probabilities can’t be <em>greater</em> than one, and the probability of event occurring <em>at all (ever)</em> should be equal to one, if the event <span class="math inline">\(x\)</span> is a random variable.</p></li>
</ul>
<p><strong>Cumulative distribution function</strong> (discrete, continuous)</p>
<p>Cumulative distribution functions we’ll typically write as <span class="math inline">\(F_X(x)\)</span>. or <span class="math inline">\(F(x)\)</span>, for short. It is important to know that</p>
<p><span class="math display">\[
F_X(x) = \Pr(X \leq x),
\]</span></p>
<p>or in words, “the cumulative distribution function is the probability that a random variable lies before <span class="math inline">\(x\)</span>.” If you write <span class="math inline">\(\Pr(X &lt; x)\)</span> instead of <span class="math inline">\(\leq\)</span>, you’re fine. The probability that a random variable is exactly one number (for an RV with a continuous pdf) is zero anyway, so these are the same thing. Key things I want you to know about cumulative distribution functions:</p>
<ul>
<li><p><span class="math inline">\(F(x)\)</span> is non-decreasing. This is in part where the “cumulative” piece comes in to play. Recall that probabilities are basically integrals or sums. If we’re integrating over something positive, and our upper bound for our integral <em>increases</em>, the area under the curve (cumulative probability) will increase as well.</p></li>
<li><p><span class="math inline">\(0 \leq F(x) \leq 1\)</span> (since probabilities have to be between zero and one!)</p></li>
<li><p><span class="math inline">\(\Pr(a &lt; X \leq b) = F(a) - F(b)\)</span> (because algebra)</p></li>
</ul>
<p><strong>Joint probability density function</strong></p>
<p>A joint probability density function is a probability distribution defined for more than one random variable at a time. For two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, we could write their joint density function as <span class="math inline">\(f_{X,Z}(x, z)\)</span> , or <span class="math inline">\(f(x,z)\)</span> for short. The joint density function encodes all sorts of fun information, including <em>marginal</em> distributions for <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, and conditional distributions (see next <strong>bold</strong> definition). We can think of the joint pdf as listing all possible pairs of outputs from the density function <span class="math inline">\(f(x,z)\)</span>, for varying values of <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>. Key things I want you to know about joint pdfs:</p>
<ul>
<li><p>How to get a marginal pdf from a joint pdf:</p>
<p>Suppose I want to know <span class="math inline">\(f_X(x)\)</span>, and I know <span class="math inline">\(f_{X,Z}(x,z)\)</span>. Then I can integrate or “average over” <span class="math inline">\(Z\)</span> to get</p>
<p><span class="math display">\[
f_X(x) = \int f_{X,Z}(x,z)dz
\]</span></p></li>
<li><p>The relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule</p></li>
<li><p>How to obtain a joint pdf for <em>independent</em> random variables: just multiply their marginal pdfs together! This is how we will (typically) think about likelihoods!</p></li>
<li><p>How to obtain a marginal pdf from a joint pdf when random variables are independent <em>without integrating</em> (think, “separability”)</p></li>
</ul>
<p><strong>Conditional probability density function</strong></p>
<p>A conditional pdf denotes the probability distribution for a (set of) random variable(s), <em>given that</em> the value for another (set of) random variable(s) is known. For two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, we could write the conditional distribution of <span class="math inline">\(X\)</span> “given” <span class="math inline">\(Z\)</span> as <span class="math inline">\(f_{X \mid Z}(x \mid z)\)</span> , where the “conditioning” is denoted by a vertical bar (in LaTeX, this is typeset using “\mid”). Key things I want you to know about conditional pdfs:</p>
<ul>
<li><p>The relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule</p></li>
<li><p>How to obtain a conditional pdf from a joint pdf (again, think Bayes’ rule)</p></li>
<li><p>Relationship between conditional pdfs and independence (see next <strong>bold</strong> definition)</p></li>
</ul>
<p><strong>Independence</strong></p>
<p>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are <em>independent</em> if and only if:</p>
<ul>
<li><p><span class="math inline">\(f_{X,Z}(x,z) = f_X(x) f_Z(z)\)</span> (their joint pdf is “separable”)</p></li>
<li><p><span class="math inline">\(f_{X\mid Z}(x\mid z) = f_X(x)\)</span> (the pdf for <span class="math inline">\(X\)</span> does not depend on <span class="math inline">\(Z\)</span> in any way)</p>
<p>Note that the “opposite” is also true: <span class="math inline">\(f_{Z\mid X}(z\mid x) = f_Z(z)\)</span></p></li>
</ul>
<p>In notation, we denote that two variables are independent as <span class="math inline">\(X \perp\!\!\!\perp Z\)</span>, or <span class="math inline">\(X \perp Z\)</span>. In LaTeX, the <em>latter</em> is typeset as “\perp”, and the former is typeset as “\perp\!\!\!\perp”. As a matter of personal preference, I (Taylor) prefer <span class="math inline">\(\perp\!\!\!\perp\)</span>, but I don’t like typing it out every time. Consider using the “\newcommand” functionality in LaTeX to create a shorthand for this for your documents!</p>
<p><strong>Jacobian Matrix</strong></p>
<p>Let <span class="math inline">\(f\)</span> be a 1-1 and onto function, where <span class="math inline">\(f(x_i) = y_i\)</span> for <span class="math inline">\(i = 1, \dots, n\)</span>. Then the Jacobian matrix of <span class="math inline">\(f\)</span> is the matrix of partial derivatives,</p>
<p><span class="math display">\[
J_f(x) = \begin{pmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \dots &amp; \frac{\partial y_n}{\partial x_1} \\
\vdots &amp; &amp; \vdots \\
\frac{\partial y_1}{\partial x_n} &amp; \dots &amp; \frac{\partial y_n}{\partial x_n}
\end{pmatrix}
\]</span></p>
<p>The Jacobian matrix is sometimes simply referred to as the “Jacobian”, but be careful when simply calling it the Jacobian, since this can sometimes refer to the <em>determinant</em> of the Jacobian matrix as well. For this course, we’ll always refer to the Jacobian matrix as a matrix, and the “Jacobian” as its determinant.</p>
<p><strong>Jacobian</strong></p>
<p>The Jacobian is the determinant of the Jacobian matrix, denoted by <span class="math inline">\(| J_f(x) |\)</span>. Recall from linear algebra that <span class="math inline">\(Det(A) = Det(A^\top)\)</span>. This is convenient, because it means we won’t have worry too much about remembering which order our partial derivatives go in our matrix, for 2x2 matrices (which is all we’ll be working with for this course).</p>
<p><strong>Expected Value / Expectation</strong></p>
<p>The expectation (or expected value) of a random variable is defined as:</p>
<p><span class="math display">\[
E[X] = \int_{-\infty}^\infty x f(x) dx
\]</span></p>
<p>Expected value is a weighted average, where the average is over all possible values a random variable can take, weighted by the probability that those values occur. Key things I want you to know about expectation:</p>
<ul>
<li><p>The relationship between expectation, variance, and moments (specifically, that <span class="math inline">\(E[X]\)</span> is the 1st moment!)</p></li>
<li><p>The “law of the unconscious statistician” (see the Theorems section of this chapter)</p></li>
<li><p>Expectation of linear transformations of random variables (see <strong>Theorems</strong> section of this chapter)</p></li>
</ul>
<p><strong>Variance</strong></p>
<p>The variance of a random variable is defined as:</p>
<p><span class="math display">\[
Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2
\]</span></p>
<p>In words, we can read this as “the expected value of the squared deviation from the mean” of a random variable <span class="math inline">\(X\)</span>. Key things I want you to know about variance:</p>
<ul>
<li><p>The relationship between expectation, variance, and moments (hopefully clear, given the formula for variance)</p></li>
<li><p>The relationship between variance and standard deviation: <span class="math inline">\(Var(X) = sd(X)^2\)</span></p></li>
<li><p>The relationship between variance and covariance: <span class="math inline">\(Var(X) = Cov(X, X)\)</span></p></li>
<li><p><span class="math inline">\(Var(X) \geq 0\)</span>. This should make sense, given that we’re taking the expectation of something “squared” in order to calculate it!</p></li>
<li><p><span class="math inline">\(Var(c) = 0\)</span> for any constant, <span class="math inline">\(c\)</span>.</p></li>
<li><p>Variance of linear transformations of random variables (see <strong>Theorems</strong> section of this chapter)</p></li>
</ul>
<p><span class="math inline">\(r^{th}\)</span> <strong>moment</strong></p>
<p>The <span class="math inline">\(r^{th}\)</span> moment of a probability distribution is given by <span class="math inline">\(E[X^r]\)</span>. For example, when <span class="math inline">\(r = 1\)</span>, the <span class="math inline">\(r^{th}\)</span> moment is just the expectation of the random variable <span class="math inline">\(X\)</span>. Key things I want you to know about moments:</p>
<ul>
<li><p>The relationship between moments, expectation, and variance</p>
<ul>
<li>For example, if you know the first and second moments of a distribution, you should be able to calculate the variance of a random variable with that distribution!</li>
</ul></li>
<li><p>The relationship between moments and <em>moment generating functions</em> (see <strong>Theorems</strong> section of this chapter)</p></li>
</ul>
<p><strong>Covariance</strong></p>
<p>The covariance of two random variables is a measure of their <em>joint</em> variability. We denote the covariance of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> as <span class="math inline">\(Cov(X,Z)\)</span>, and</p>
<p><span class="math display">\[
Cov(X, Z) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
\]</span></p>
<p>Some things I want you to know about covariance:</p>
<ul>
<li><p><span class="math inline">\(Cov(X, X) = Var(X)\)</span></p></li>
<li><p><span class="math inline">\(Cov(X, Y) = Cov(Y, X)\)</span> (order doesn’t matter)</p></li>
</ul>
<p><strong>Moment Generating Function (MGF)</strong></p>
<p>The moment generating function of a random variable <span class="math inline">\(X\)</span> is defined as</p>
<p><span class="math display">\[
M_X(t) = E[e^{tX}]
\]</span></p>
<p>A few things to note:</p>
<ul>
<li><p><span class="math inline">\(M_X(0) = 1\)</span>, always.</p></li>
<li><p>If two random variables have the same MGF, they have the same probability distribution!</p></li>
<li><p>MGFs are sometimes useful for showing how different random variables are related to each other</p></li>
</ul>
<section id="distributions-table" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="distributions-table"><span class="header-section-number">1.3.1</span> Distributions Table</h3>
<p>You are also expected to know the probability distributions contained in Table 1, below. Note that you <em>do not</em> need to memorize the pdfs for these distributions, but you <em>should</em> be familiar with what types of random variables (continuous/quantitative, categorical, integer-valued, etc.) may take on different distributions. The more familiar you are with the forms of the pdfs, the easier/faster it will be to work through problem sets and quizzes.</p>
<table class="caption-top table">
<caption><em>Table 1.</em> Table of main probability distributions we will work with for <em>MATH/STAT 355</em>.</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>PDF/PMF</th>
<th>Parameters</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Uniform</td>
<td><span class="math inline">\(\pi(x) = \frac{1}{\beta - \alpha}\)</span></td>
<td><span class="math inline">\(\alpha \in \mathbb{R}\)</span>, <span class="math inline">\(\beta\in \mathbb{R}\)</span></td>
<td><span class="math inline">\(x \in [\alpha, \beta]\)</span></td>
</tr>
<tr class="even">
<td>Normal</td>
<td><span class="math inline">\(\pi(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{1}{2\sigma^2} (x - \mu)^2)\)</span></td>
<td><span class="math inline">\(\mu \in \mathbb{R}\)</span>, <span class="math inline">\(\sigma &gt; 0\)</span></td>
<td><span class="math inline">\(x \in \mathbb{R}\)</span></td>
</tr>
<tr class="odd">
<td>Multivariate Normal</td>
<td><span class="math inline">\(\pi(\textbf{x}) = (2\pi)^{-k/2} |\Sigma|^{-1/2} \exp(-\frac{1}{2}(\textbf{x} - \mu)^\top \Sigma^{-1}(\textbf{x} - \mu)))\)</span></td>
<td><span class="math inline">\(\mu \in \mathbb{R}^k\)</span>, <span class="math inline">\(\Sigma \in \mathbb{R}^{k\times k}\)</span> , positive semi-definite (in practice, almost always positive definite)</td>
<td><span class="math inline">\(x \in \mathbb{R}^{k}\)</span></td>
</tr>
<tr class="even">
<td>Gamma</td>
<td><span class="math inline">\(\pi(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1} e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\alpha \text{ (shape)}, \beta \text{ (rate)} &gt; 0\)</span></td>
<td><span class="math inline">\(x \in (0,\infty)\)</span></td>
</tr>
<tr class="odd">
<td>Chi-squared</td>
<td><span class="math inline">\(\pi(x) = \frac{2^{-\nu/2}}{\Gamma(\nu/2)} x^{\nu/2 - 1}e^{-x/2}\)</span></td>
<td><span class="math inline">\(\nu &gt; 0\)</span></td>
<td><span class="math inline">\(x \in [0, \infty)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(F\)</span></td>
<td><span class="math inline">\(\pi(x) = \frac{\Gamma(\frac{\nu_1 + \nu_2}{2})}{\Gamma(\frac{\nu_1}{2}) \Gamma(\frac{\nu_2}{2})} \left( \frac{\nu_1}{\nu_2}\right)^{\nu_1/2} \left( \frac{x^{(\nu_1 - 2)/2}}{\left( 1 + \left( \frac{\nu_1}{\nu_2}\right)x\right)^{(\nu_1 + \nu_2)/2}}\right)\)</span></td>
<td><span class="math inline">\(\nu_1, \nu_2 &gt; 0\)</span></td>
<td><span class="math inline">\(x \in [0, \infty)\)</span></td>
</tr>
<tr class="odd">
<td>Exponential</td>
<td><span class="math inline">\(\pi(x) = \beta e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\beta &gt; 0\)</span></td>
<td><span class="math inline">\(x \in [0, \infty)\)</span></td>
</tr>
<tr class="even">
<td>Laplace (Double Exponential)</td>
<td><span class="math inline">\(\pi(x) = \frac{1}{2b} \exp( - \frac{|x - \mu|}{b})\)</span></td>
<td><span class="math inline">\(\mu \in \mathbb{R}\)</span>, <span class="math inline">\(b &gt; 0\)</span></td>
<td><span class="math inline">\(x \in \mathbb{R}\)</span></td>
</tr>
<tr class="odd">
<td>Student-<span class="math inline">\(t\)</span></td>
<td><span class="math inline">\(\pi(x) = \frac{\Gamma((\nu + 1)/2)}{\Gamma(\nu/2) \sqrt{\nu \pi}} (1 + \frac{x^2}{\nu})^{-(\nu + 1)/2}\)</span></td>
<td><span class="math inline">\(\nu &gt; 0\)</span></td>
<td><span class="math inline">\(x \in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td>Beta</td>
<td><span class="math inline">\(\pi(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1}(1 - x)^{\beta - 1}\)</span></td>
<td><span class="math inline">\(\alpha, \beta &gt; 0\)</span></td>
<td><span class="math inline">\(x \in [0,1]\)</span></td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td><span class="math inline">\(\pi(x) = \frac{\lambda^x e^{-\lambda}}{x!}\)</span></td>
<td><span class="math inline">\(\lambda &gt; 0\)</span></td>
<td><span class="math inline">\(x \in \mathbb{N}\)</span></td>
</tr>
<tr class="even">
<td>Binomial</td>
<td><span class="math inline">\(\pi(x) = \binom{n}{x} p^{x} (1 - p)^{n - x}\)</span></td>
<td><span class="math inline">\(p \in [0,1], n = \{0, 1, 2, \dots\}\)</span></td>
<td><span class="math inline">\(x \in \{0, 1, \dots, n\}\)</span></td>
</tr>
<tr class="odd">
<td>Multinomial</td>
<td><span class="math inline">\(\pi(\textbf{x}) = \frac{n!}{x_1! \dots x_k!} p_1^{x_1} \dots p_k^{x_k}\)</span></td>
<td><span class="math inline">\(p_i &gt; 0\)</span>, <span class="math inline">\(p_1 + \dots + p_k = 1\)</span>, <span class="math inline">\(n = \{0, 1, 2, \dots \}\)</span></td>
<td><span class="math inline">\(\{ x_1, \dots, x_k \mid \sum_{i = 1}^k x_i = n, x_i \geq 0 (i = 1, \dots, k)\}\)</span></td>
</tr>
<tr class="even">
<td>Negative Binomial</td>
<td><span class="math inline">\(\pi(x) = \binom{x + r - 1}{x} (1-p)^x p^r\)</span></td>
<td><span class="math inline">\(r &gt; 0\)</span>, <span class="math inline">\(p \in [0,1]\)</span></td>
<td><span class="math inline">\(x \in \{0, 1, \dots\}\)</span></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="theorems" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="theorems"><span class="header-section-number">1.4</span> Theorems</h2>
<ul>
<li><p>Law of Total Probability</p>
<p><span class="math display">\[
P(A) = \sum_n P(A \cap B_n),
\]</span>or</p>
<p><span class="math display">\[
P(A) = \sum_n P(A \mid B_n) P(B_n)
\]</span></p></li>
<li><p>Bayes’ Theorem</p>
<p><span class="math display">\[
\pi(A \mid B) = \frac{\pi(B \mid A) \pi(A)}{\pi(B)}
\]</span></p></li>
<li><p>Relationship between pdf and cdf</p>
<p><span class="math display">\[
F_Y(y) = \int_{-\infty}^y f_Y(t)dt
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial y}F_Y(y) = f_Y(y)
\]</span></p></li>
<li><p>Expectation of random variables</p>
<p><span class="math display">\[
E[X] = \int_{-\infty}^\infty x f(x) dx
\]</span></p>
<p><span class="math display">\[
E[X^2] = \int_{-\infty}^\infty x^2 f(x) dx
\]</span></p>
<ul>
<li><p>“Law of the Unconscious Statistician”</p>
<p><span class="math display">\[
E[g(X)] = \int_{-\infty}^\infty g(x)f(x)dx
\]</span></p></li>
</ul></li>
<li><p>Expectation and variance of linear transformations of random variables</p>
<p><span class="math display">\[
E[cX + b] = c E[X] + b
\]</span></p>
<p><span class="math display">\[
Var[cX + b] = c^2 Var[X]
\]</span></p></li>
<li><p>Relationship between mean and variance</p>
<p><span class="math display">\[
Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2
\]</span></p>
<p>Also, recall that <span class="math inline">\(Cov[X, X] = Var[X]\)</span>.</p></li>
<li><p>Finding a marginal pdf from a joint pdf</p>
<p><span class="math display">\[
f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x, y) dy
\]</span></p></li>
<li><p>Independence of random variables and joint pdfs</p>
<p>If two random variables are independent, their joint pdf will be <em>separable</em>. For example, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we could write</p>
<p><span class="math display">\[
f_{X,Y}(x, y) = f_{X}(x)f_Y(y)
\]</span></p></li>
<li><p>Expected value of a product of independent random variables</p>
<p>Suppose random variables <span class="math inline">\(X_1, \dots, X_n\)</span> are independent. Then we can write,</p>
<p><span class="math display">\[
E\left[\prod_{i = 1}^n X_i\right] = \prod_{i = 1}^n E[X_i]
\]</span></p></li>
<li><p>Covariance of independent random variables</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(Cov(X, Y) = 0\)</span>. We can show this by noting that</p></li>
</ul>
<p><span class="math display">\[\begin{align}
Cov(X, Y) &amp; = E[(X - E[X])(Y - E[Y])] \\
&amp; = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\
&amp; = E[XY] - E[XE[Y]] - E[YE[X]] + E[X]E[Y] \\
&amp; =  2E[X]E[Y] - 2E[X]E[Y] \\
&amp; = 0
\end{align}\]</span></p>
<ul>
<li><p>Using MGFs to find moments</p>
<p>Recall that the moment generating function of a random variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(M_X(t)\)</span> is</p>
<p><span class="math display">\[
M_X(t) = E[e^{tX}]
\]</span></p>
<p>Then the <span class="math inline">\(n\)</span>th moment of the probability distribution for <span class="math inline">\(X\)</span> , <span class="math inline">\(E[X^n]\)</span>, is given by</p>
<p><span class="math display">\[
\frac{\partial M_X}{\partial t^n} \Bigg|_{t = 0}
\]</span></p>
<p>where the above reads as “the <span class="math inline">\(n\)</span>th derivative of the moment generating function, evaluated at <span class="math inline">\(t = 0\)</span>.”</p></li>
<li><p>Using MGFs to identify pdfs</p>
<p>MGFs uniquely identify probability density functions. If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two random variables where for all values of <span class="math inline">\(t\)</span>, <span class="math inline">\(M_X(t) = M_Y(t)\)</span>, then <span class="math inline">\(F_X(x) = F_Y(y)\)</span>.</p></li>
<li><p>Central Limit Theorem</p>
<p>The classical CLT states that for independent and identically distributed (iid) random variables <span class="math inline">\(X_1, \dots, X_n\)</span>, with expected value <span class="math inline">\(E[X_i] = \mu\)</span> and <span class="math inline">\(Var[X_i] = \sigma^2 &lt; \infty\)</span>, the sample average (centered and standardized) converges in distribution to a standard normal distribution at a root-<span class="math inline">\(n\)</span> rate. Notationally, this is written as</p>
<p><span class="math display">\[
\sqrt{n} (\bar{X} - \mu) \overset{d}{\to} N(0, \sigma^2)
\]</span></p>
<p><img src="images/chilipepper.png" width="20" height="16"> A fun aside: this is only <em>one</em> CLT, often referred to as the Levy CLT. There are other CLTs, such as the Lyapunov CLT and Lindeberg-Feller CLT!</p></li>
</ul>
<section id="transforming-continuous-random-variables" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="transforming-continuous-random-variables"><span class="header-section-number">1.4.1</span> Transforming Continuous Random Variables</h3>
<p>We will <em>often</em> take at face value previously proven <em>relationships</em> between random variables. What I mean by this, as an example, is that it is a nice (convenient) fact that a sum of two independent normal random variables is <em>still</em> normally distributed, with a nice form for the mean and variance. In particular, if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> and <span class="math inline">\(Y \sim N(\theta, \nu^2)\)</span>, then <span class="math inline">\(X + Y \sim N(\mu + \theta, \sigma^2 + \nu^2)\)</span>. Most frequently used examples of these sorts of relationships can be found in the ``Related Distributions” section of the Wikipedia page for a given probability distribution. Unless I explicitly ask you to derive/show how certain variables are related to each other, you can just state the known relationship, use it, and move on!</p>
<p>If I <em>do</em> ask you to derive/show these things, there a few different ways we can go about this. For this course, I expect you to know the “CDF method” for <em>one function</em> of <em>one random variable</em>, <strong>and</strong> the “Jacobian method” for a function of <em>more than one</em> random variable. We’ll demonstrate the former below.</p>
<section id="cdf-method-one-random-variable" class="level4" data-number="1.4.1.1">
<h4 data-number="1.4.1.1" class="anchored" data-anchor-id="cdf-method-one-random-variable"><span class="header-section-number">1.4.1.1</span> CDF Method: One random variable</h4>
<p><strong>Theorem</strong>. Let <span class="math inline">\(X\)</span> be a continuous random variable with pdf <span class="math inline">\(f_X(x)\)</span>. Define a new random variable <span class="math inline">\(Y = g(X)\)</span>, for nice* functions <span class="math inline">\(g\)</span>. Then <span class="math inline">\(f_Y(y) = f_X(g^{-1}(y)) \times \frac{1}{g'(g^{-1}(y))}\)</span>.</p>
<p><img src="images/chilipepper.png" width="20" height="16"> *By <em>nice</em> functions we mean functions that are strictly increasing and smooth <em>on the required range</em>. As an example, <span class="math inline">\(exp(x)\)</span> is a smooth, strictly increasing function; <span class="math inline">\(|x|\)</span> is not on the <em>whole real line</em>, but <em>is</em> from <span class="math inline">\((0, \infty)\)</span> (where a lot of useful pdfs are defined). For the purposes of this class, every function that you will need to do this for will be “nice.” Note that there are also considerations that need to be taken regarding the <em>range</em> of continuous random variables when considering transforming them. We will mostly ignore these considerations in this class, but a technically complete derivation (or proof) must consider them.</p>
<p><strong>Proof</strong>. We can write</p>
<p><span class="math display">\[\begin{align*}
    f_Y(y) &amp; = \frac{\partial}{\partial y} F_Y(y) \\
    &amp; = \frac{\partial}{\partial y} \Pr(Y \leq y) \\
    &amp; = \frac{\partial}{\partial y} \Pr(g(X) \leq y) \\
    &amp; = \frac{\partial}{\partial y} \Pr(X \leq g^{-1}(y)) \\
    &amp; = \frac{\partial}{\partial y} F_X(g^{-1}(y)) \\
    &amp; = f_X(g^{-1}(y)) \times \frac{\partial}{\partial y} g^{-1}(y)
\end{align*}\]</span></p>
<p>where to obtain the last equality we use chain rule! Now we require some statistical trickery to continue… (note that this method is called the “CDF method” because we go <em>through</em> the CDF to derive the distribution for <span class="math inline">\(Y\)</span>)</p>
<p>You will <em>especially</em> see this in the Bayes chapter of our course notes, but it is often true that our lives are made easier as statisticians if we multiply things by one, or add zero. What exactly do I mean? Rearranging gross looking formulas into things we are familiar with (like pdfs, for example) often makes our lives easier and allows us to avoid dealing with such grossness. Here, the grossness is less obvious, but nonetheless relevant. Note that we can write</p>
<p><span class="math display">\[\begin{align*}
    y &amp; = y \\
    y &amp; = g(g^{-1}(y)) \\
    \frac{\partial}{\partial y} y &amp; = \frac{\partial}{\partial y} g(g^{-1}(y)) \\
    1 &amp; = g'(g^{-1}(y)) \frac{\partial}{\partial y} g^{-1}(y) \hspace{1cm} \text{(chain rule again!)} \\
    \frac{1}{g'(g^{-1}(y))} &amp; = \frac{\partial}{\partial y} g^{-1}(y)
\end{align*}\]</span></p>
<p>The right-hand side should look familiar: it is exactly what we needed to “deal with” in our proof! Returning to that proof, we have</p>
<p><span class="math display">\[\begin{align*}
    f_Y(y) &amp; = f_X(g^{-1}(y)) \times \frac{\partial}{\partial y} g^{-1}(y) \\
    &amp; = f_X(g^{-1}(y)) \times \frac{1}{g'(g^{-1}(y))}
\end{align*}\]</span></p>
<p>as desired.</p>
</section>
<section id="jacobian-method-more-than-one-random-variable" class="level4" data-number="1.4.1.2">
<h4 data-number="1.4.1.2" class="anchored" data-anchor-id="jacobian-method-more-than-one-random-variable"><span class="header-section-number">1.4.1.2</span> Jacobian Method: More than one random variable</h4>
<p>Transformations of single random variables are great, but we’ll need to work with transformations of <em>more than one</em> random variable if we want to be able to manipulate joint pdfs. Suppose, for example, we have <span class="math inline">\(X \sim Gamma(\alpha, \lambda)\)</span> and <span class="math inline">\(Y \sim Gamma(\beta, \lambda)\)</span>, where <span class="math inline">\(X \perp\!\!\!\perp Y\)</span>. Let <span class="math inline">\(U = X + Y\)</span> and <span class="math inline">\(W = \frac{X}{X + Y}\)</span>. How do we show that <span class="math inline">\(U\)</span> and <span class="math inline">\(W\)</span> are independent? Through finding the joint pdf! Which means we need a method for transforming more than one, continuous random variable. Enter the Jacobian Method.</p>
<p><strong>Theorem</strong>. Let <span class="math inline">\(X = (x_1, \dots, x_n)\)</span> be a vector of random variables (a random vector) with pdf <span class="math inline">\(\pi_{X}(x)\)</span>. Suppose that <span class="math inline">\(f(x) = y\)</span> is a 1-1 and onto function, and that <span class="math inline">\(|J_f(x)| &gt; 0\)</span> almost everywhere. Then <span class="math inline">\(\pi_Y(y)\)</span> is given by</p>
<p><span class="math display">\[
\pi_Y(y) = \pi_{X}(f^{-1}(y)) \times \bigg| \frac{\partial x}{\partial y} \bigg| \times I_Y\{y\}
\]</span> where <span class="math inline">\(I_Y \{y \}\)</span> denotes the support of <span class="math inline">\(Y\)</span>.</p>
<p>NOTE: <span class="math inline">\(| J_f(y) | = | \frac{\partial x}{\partial y} |\)</span>, above, where <span class="math inline">\(f(x) = y\)</span>. This means that the numerators and denominators in the Jacobian Matrix in the definition in the Course Notes are <em>flipped</em> here. See Worked Example 7.</p>
<p><strong>Proof</strong>.</p>
<p>Note that if <span class="math inline">\(f\)</span> is both 1-1 and onto, then <span class="math inline">\(f\)</span> is <em>either</em> monotone increasing or monotone decreasing. We’ll prove the case where <span class="math inline">\(f\)</span> is increasing, and we’ll note (but not show) how the decreasing case follows directly.</p>
<p>Let <span class="math inline">\(f\)</span> be an increasing, continuous function. For some subset <span class="math inline">\(C \subseteq \mathbb{R}^n\)</span>,</p>
<p><span class="math display">\[\begin{align*}
\int_C \pi_Y(y) dy &amp; = \Pr(Y \in C) \\
&amp; = \Pr(f(X) \in C) \\
&amp; = \Pr(X \in f^{-1}(C)) \\
&amp; = \int_{f^{-1}(C)} \pi_X(x) dx
\end{align*}\]</span></p>
<p>Now we’ll use a (convoluted) <span class="math inline">\(u\)</span>-substitution to make this look like what we want it to look like. Let <span class="math inline">\(u = f^{-1}(y)\)</span>. Note that this also means <span class="math inline">\(u = x\)</span>. Then <span class="math inline">\(du = (f^{-1}(y))' dy = dx\)</span>. Proceeding with <span class="math inline">\(u\)</span>-substitution, we have</p>
<p><span class="math display">\[\begin{align*}
\int_C \pi_Y(y) dy &amp; = \int_{f^{-1}(C)} \pi_X(x) dx \\
&amp; = \int_C \pi_X(u) du \\
&amp; = \int_C \pi_X(f^{-1}(y)) (f^{-1}(y))' dy \\
&amp; = \int_C \pi_X(f^{-1}(y))
\bigg| \frac{dx}{dy} \bigg| dy
\end{align*}\]</span> which implies <span class="math inline">\(\pi_Y(y) = \pi_X(f^{-1}(y))
\bigg| \frac{dx}{dy} \bigg|\)</span>, as desired. The absolute value signs (the Jacobian piece) come into play to help us deal with the decreasing case.</p>
</section>
</section>
</section>
<section id="worked-examples" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="worked-examples"><span class="header-section-number">1.5</span> Worked Examples</h2>
<p><strong>Problem 1:</strong> Suppose <span class="math inline">\(X \sim Exponential(\lambda)\)</span>. Calculate <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(Var[X]\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>We know that <span class="math inline">\(f(x) = \lambda e^{-\lambda x}\)</span>. If we can calculate <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[X^2]\)</span>, then we’re basically done! We can write</p>
<p><span class="math display">\[\begin{align*}    
E[X] &amp; = \int_0^\infty x \lambda e^{-\lambda x} dx \\    
&amp; = \lambda \int_0^\infty x e^{-\lambda x} dx
\end{align*}\]</span></p>
<p>And now we need integration by parts! Set <span class="math inline">\(u = x\)</span>, <span class="math inline">\(dv = e^{-\lambda x} dx\)</span>. Then <span class="math inline">\(du = 1dx\)</span> and <span class="math inline">\(v = \frac{-1}{\lambda} e^{-\lambda x}\)</span>. Since <span class="math inline">\(\int u dv = uv - \int vdu\)</span>, we can continue</p>
<p><span class="math display">\[\begin{align*}    
E[X] &amp; = \lambda \int_0^\infty x e^{-\lambda x} dx \\    
&amp; = \lambda \left( -\frac{x}{\lambda} e^{-\lambda x} \bigg|_0^\infty  - \int_0^\infty \frac{-1}{\lambda} e^{-\lambda x} dx \right) \\    
&amp; = \lambda \left( - \int_0^\infty \frac{-1}{\lambda} e^{-\lambda x} dx \right) \\    
&amp; = \lambda \left( \frac{-1}{\lambda^2} e^{-\lambda x}  \bigg|_0^\infty \right) \\    
&amp; = \frac{-1}{\lambda} e^{-\lambda x}  \bigg|_0^\infty \\    
&amp; = \frac{1}{\lambda} e^{-0} \\    
&amp; = \frac{1}{\lambda}
\end{align*}\]</span></p>
<p>We can follow a similar process to get <span class="math inline">\(E[X^2]\)</span> (using the law of the unconscious statistician!). We can write</p>
<p><span class="math display">\[\begin{align*}
    E[X^2] &amp; = \int_0^\infty x^2 \lambda e^{-\lambda x} dx \\
    &amp; = \lambda \int_0^\infty x^2 e^{-\lambda x} dx
\end{align*}\]</span></p>
<p>And now we need integration by parts again! Set <span class="math inline">\(u = x^2\)</span>, <span class="math inline">\(dv = e^{-\lambda x} dx\)</span>. Then <span class="math inline">\(du = 2xdx\)</span> and <span class="math inline">\(v = \frac{-1}{\lambda} e^{-\lambda x}\)</span>. Since <span class="math inline">\(\int u dv = uv - \int vdu\)</span>, we can continue</p>
<p><span class="math display">\[\begin{align*}
    E[X] &amp; = \lambda \int_0^\infty x^2 e^{-\lambda x} dx \\
    &amp; = \lambda \left( -\frac{x^2}{\lambda} e^{-\lambda x} \bigg|_0^\infty  - \int_0^\infty \frac{-2}{\lambda} xe^{-\lambda x} dx \right) \\
    &amp; = \lambda \left( -\frac{x^2}{\lambda} e^{-\lambda x} \bigg|_0^\infty  + \frac{2}{\lambda} \int_0^\infty  xe^{-\lambda x} dx \right) \\
    &amp; = \lambda \left( -\frac{x^2}{\lambda} e^{-\lambda x} \bigg|_0^\infty  + \frac{2}{\lambda^3} \right)\\
    &amp; = \lambda \left( 0  + \frac{2}{\lambda^3} \right) \\
    &amp; = \frac{2}{\lambda^2}
\end{align*}\]</span></p>
<p>Now we can calculate <span class="math inline">\(Var[X] = E[X^2] - E[X]^2\)</span> as <span class="math display">\[
Var[X] = E[X^2] - E[X]^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}
\]</span> And so we have <span class="math inline">\(E[X] = \frac{1}{\lambda}\)</span> and <span class="math inline">\(Var[X] = \frac{1}{\lambda^2}\)</span>.</p>
</details>
<p><strong>Problem 2:</strong> Show that an exponentially distributed random variable is “memoryless”, i.e.&nbsp;show that <span class="math inline">\(\Pr(X &gt; s + x \mid X &gt; s) = \Pr(X &gt; x)\)</span>, <span class="math inline">\(\forall s\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>Recall that the CDF of an exponential distribution is given by <span class="math inline">\(F(x) = 1-e^{-\lambda x}\)</span>. Thanks to Bayes rule, we can write</p>
<p><span class="math display">\[\begin{align*}
    \Pr(X &gt; s + x \mid X &gt; s) &amp; = \frac{\Pr(X &gt; s + x , X &gt; s)}{\Pr(X &gt; s)} \\
    &amp; = \frac{\Pr(X &gt; s + x)}{\Pr(X &gt; s)} \\
    &amp; = \frac{1 - \Pr(X &lt; s + x)}{1 - \Pr(X &lt; s)} \\
    &amp; = \frac{1 - F(s + x)}{1 - F(s)}
\end{align*}\]</span></p>
<p>where the second equality is true because <span class="math inline">\(x &gt; 0\)</span>. Then we can write</p>
<p><span class="math display">\[\begin{align*}
    \Pr(X &gt; s + x \mid X &gt; s) &amp; = \frac{1 - F(s + x)}{1 - F(s)} \\
    &amp; = \frac{1 - \left(1 - e^{-\lambda(s + x)}\right)}{1 - \left(1 - e^{-\lambda s}\right)} \\
    &amp; = \frac{e^{-\lambda(s + x)}}{e^{-\lambda s}} \\
    &amp; = \frac{e^{-\lambda s - \lambda x}}{e^{-\lambda s}} \\
    &amp; = e^{-\lambda x} \\
    &amp; = 1 - F(x) \\
    &amp; = \Pr(X &gt; x)
\end{align*}\]</span></p>
<p>and we’re done!</p>
</details>
<p><strong>Problem 3:</strong> Suppose <span class="math inline">\(X \sim Exponential(1/\lambda)\)</span>, and <span class="math inline">\(Y \mid X \sim Poisson(X)\)</span>. Show that <span class="math inline">\(Y \sim Geometric(1/(1 + \lambda))\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>Note that we can write <span class="math inline">\(f(x, y) = f(y \mid x) f(x)\)</span>, and <span class="math inline">\(f(y) = \int f(x, y) dx\)</span>. Then</p>
<p><span class="math display">\[
f(x, y) = \left( \frac{1}{\lambda} e^{-x/\lambda} \right) \left( \frac{x^y e^{-x}}{y!} \right)
\]</span> And so,</p>
<p><span class="math display">\[\begin{align*}
    f(y) &amp; = \int f(x, y) dx \\
    &amp; = \int \left( \frac{1}{\lambda} e^{-x/\lambda} \right) \left( \frac{x^y e^{-x}}{y!} \right) dx \\
    &amp; = \frac{1}{\lambda y!} \int x^y e^{-x(1 + \lambda)/\lambda} dx
\end{align*}\]</span></p>
<p>And we can again use integration by parts! Let <span class="math inline">\(u = x^y\)</span> and <span class="math inline">\(dv = e^{-x(1 + \lambda)/\lambda} dx\)</span>. Then we have <span class="math inline">\(du = yx^{y-1} dx\)</span> and <span class="math inline">\(v = -\frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda}\)</span>, and we can write</p>
<p><span class="math display">\[\begin{align*}
    f(y) &amp; = \frac{1}{\lambda y!} \int x^y e^{-x(1 + \lambda)/\lambda} dx \\
    &amp; = \frac{1}{\lambda y!} \left( -x^y \frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda} \bigg|_{x = 0}^{x = \infty}  + \int \frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda} yx^{y-1} dx\right) \\
    &amp; = \frac{1}{\lambda y!} \left(  \int \frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda} yx^{y-1} dx \right) \\
    &amp; = \frac{1}{\lambda y!} \left( \frac{\lambda }{1 + \lambda} \right) y \left(  \int e^{-x(1 + \lambda)/\lambda} x^{y-1} dx \right)
\end{align*}\]</span></p>
<p>This <em>looks</em> gross, but it’s actually not so bad. Note that, since <span class="math inline">\(Y\)</span> is Poisson, it can only take integer values beginning at 1! Then we can <em>repeat</em> the process of integration by parts <span class="math inline">\(y\)</span> <em>times</em> in order to get rid of <span class="math inline">\(x^{y\dots}\)</span> term on the inside of the integral. Specifically, each time we do this process we will pull out a <span class="math inline">\(\left( \frac{\lambda }{1 + \lambda} \right)\)</span>, and a <span class="math inline">\(y - i\)</span> for the <span class="math inline">\(i\)</span>th integration by parts step (try this one or two steps for yourself to see how it will simplify if you find this unintuitive!). We end up with,</p>
<p><span class="math display">\[\begin{align*}
    f(y) &amp; = \frac{1}{\lambda y!} \left( \frac{\lambda }{1 + \lambda} \right)^y y! \\
    &amp; = \frac{1}{\lambda} \left(\frac{\lambda}{1 + \lambda}\right)^y
\end{align*}\]</span></p>
<p>Now let <span class="math inline">\(p = \frac{1}{1 + \lambda}\)</span>. If we can show that <span class="math inline">\(f(y) \sim Geometric(p)\)</span> then we’re done. Note that <span class="math inline">\(1 - p = \lambda/(1 + \lambda)\)</span>. We have</p>
<p><span class="math display">\[\begin{align*}
    f(y) &amp; = \frac{1}{\lambda} (1 - p)^y \\
    &amp; = \frac{1}{\lambda} (1 - p)^{y-1} (1-p) \\
    &amp; = (1 - p)^{y-1} \frac{1}{\lambda} \left( \frac{\lambda}{1 + \lambda} \right) \\
    &amp; = (1 - p)^{y-1} \left( \frac{1}{1 + \lambda} \right) \\
    &amp; = (1 - p)^{y-1} p
\end{align*}\]</span></p>
<p>which is exactly the pdf of a geometric random variable with parameter <span class="math inline">\(p\)</span> and trials that begin at 1.</p>
<p>An <em>alternative</em> solution (which perhaps embodies the phrase “work smarter, not harder”) actually doesn’t involve integration by parts at all! As statisticians, we typically like to avoid actually integrating anything whenever possible, and this is often achieved by manipulating algebra enough to essentially “create” a pdf out of what we see (since pdfs integrate to <span class="math inline">\(1\)</span>!). Massive props to a student for solving this problem in a much “easier” way, answer below:</p>
<p><span class="math display">\[\begin{align*}
        f(y) &amp;= \int_{0}^{\infty} f(y \mid x) f(x) dx \\
        &amp;= \int_{0}^{\infty} (\frac{1}{\lambda}e^{-\frac{x}{\lambda}}) (\frac{x^y}{y!} e^{-x}) dx \\
        &amp;= \frac{1}{\lambda y!} \int_{0}^{\infty} x^y e^{-\frac{x}{\lambda}(1 + \lambda)} dx \\
        &amp;= \frac{1}{\lambda y!} \int_{0}^{\infty} \frac{(\frac{1+\lambda}{\lambda})^{y+1}}{(\frac{1+\lambda}{\lambda})^{y+1}} \frac{\Gamma(y+1)}{\Gamma(y+1)} x^{(y+1)-1} e^{-\frac{x}{\lambda}(1 + \lambda)} dx\\
        &amp;= \frac{\Gamma(y+1)}{\lambda y! (\frac{1+\lambda}{\lambda})^{y+1}} \int_{0}^{\infty} \frac{(\frac{1+\lambda}{\lambda})^{y+1}}{\Gamma(y+1)} x^{(y+1)-1} e^{-\frac{x}{\lambda}(1 + \lambda)} dx\\
        &amp;= \frac{\Gamma(y+1)}{\lambda y! (\frac{1+\lambda}{\lambda})^{y+1}} (1)\\
        &amp;= \frac{y!}{\lambda y! (\frac{1+\lambda}{\lambda})^{y+1}} \\
        &amp;= \frac{\lambda^{-1}}{(\frac{1+\lambda}{\lambda})^{y+1}}\\
        &amp;= \frac{\lambda^y}{(1+\lambda)^{y+1}}\\
        &amp;= \frac{1}{(1+\lambda)} \frac{\lambda^y}{(1+\lambda)^y}\\
        &amp;= \frac{1}{(1+\lambda)} (1 - \frac{1}{(1+\lambda)})^y \\
        &amp;=p(1-p)^y\qquad (\text{where }p=\frac{1}{1+\lambda})
    \end{align*}\]</span></p>
<p>Note that we arrive at a slightly different answer with this approach. Specifically, we arrive at the pdf of a geometric random variable with parameter <span class="math inline">\(p\)</span> and trials that begin at 0, as opposed to 1. There’s some subtlety here that we’re going to choose to ignore.</p>
</details>
<p><strong>Problem 4:</strong> Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, and let <span class="math inline">\(Y = \frac{X - \mu}{\sigma}\)</span>. Find the distribution of <span class="math inline">\(Y\)</span> (simplifying all of your math will be useful for this problem).</p>
<details>
<summary>
Solution:
</summary>
<p>To solve this problem, we can use the theorem on transforming continuous random variables. We must first define our function <span class="math inline">\(g\)</span> that relates <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In this case, we have <span class="math inline">\(g(a) = \frac{a - \mu}{\sigma}\)</span>. Now all we need to do is collect the mathematical “pieces” we need to use theorem: <span class="math inline">\(g^{-1}(a)\)</span>, and <span class="math inline">\(g'(a)\)</span>, and finally, the pdf of a normal random variable. We have</p>
<p><span class="math display">\[\begin{align*}
    f_X(x) &amp; = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{1}{2\sigma^2} (x - \mu)^2) \\
    g^{-1}(a) &amp; = \sigma a + \mu \\
    g'(a) &amp; = \frac{\partial}{\partial a} \left(\frac{a - \mu}{\sigma}\right) = \frac{1}{\sigma}
\end{align*}\]</span></p>
<p>Putting it all together, we have</p>
<p><span class="math display">\[\begin{align*}
    f_Y(y) &amp; = f_X(g^{-1}(y)) \times \frac{1}{g'(g^{-1}(y))} \\
    &amp; = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{1}{2\sigma^2} (\sigma y + \mu - \mu)^2) \times \sigma \\
    &amp; = \frac{1}{\sqrt{2 \pi} \sigma} \exp(-\frac{1}{2\sigma^2} (\sigma y)^2) \times \sigma \\
    &amp; = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{1}{2\sigma^2} \sigma^2 y^2) \\
    &amp; = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{1}{2} y^2)
\end{align*}\]</span></p>
<p>and note that this is the pdf of a normally distributed random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>! Thus, we have shown that <span class="math inline">\(\frac{X - \mu}{\sigma} \sim N(0,1)\)</span>. <strong>Fun Fact:</strong> If this random variable reminds you of a Z-score, <em>it should</em>!</p>
</details>
<p><strong>Problem 5:</strong> Suppose the joint pdf of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by <span class="math inline">\(f_{X,Y}(x,y) = \lambda \beta e^{-x\lambda - y\beta}\)</span>. Determine if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, showing why or why not.</p>
<details>
<summary>
Solution:
</summary>
<p>To determine whether <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent (or not), we need to determine if their joint pdf is “separable.” Doing some algebra, we can see that</p>
<p><span class="math display">\[\begin{align*}    
f_{X,Y}(x,y) &amp; = \lambda \beta e^{-x \lambda - y\beta} \\    
&amp; = \lambda \beta e^{-x \lambda} e^{-y \beta} \\   
&amp; = \left( \lambda  e^{-x \lambda} \right) \left( \beta e^{-y \beta} \right)
\end{align*}\]</span></p>
<p>and so since we can write the joint distribution as a function of <span class="math inline">\(X\)</span> multiplied by a function of <span class="math inline">\(Y\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent (and in this case, both have exponential distributions).</p>
</details>
<p><strong>Problem 6:</strong> Suppose the joint pdf of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by <span class="math inline">\(f_{X,Y}(x,y) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \binom{n}{y} x^{y + \alpha - 1} (1-x)^{n-y + \beta - 1}\)</span>. Determine if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, showing why or why not.</p>
<details>
<summary>
Solution:
</summary>
<p>To determine whether <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent (or not), we need to determine if their joint pdf is “separable.” Right away, we should note that a piece of the pdf contains <span class="math inline">\(x^y\)</span>, and therefore we are <em>never</em> going to be able to fully separate out this joint pdf into a function of <span class="math inline">\(x\)</span> times a function <span class="math inline">\(y\)</span>. Therefore, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>not</em> independent. In this case, we actually have <span class="math inline">\(X \sim Beta(\alpha, \beta)\)</span>, and <span class="math inline">\(Y \mid X \sim Binomial(n, y)\)</span> (we’ll return to this example in the Bayes chapter!).</p>
</details>
<p><strong>Problem 7:</strong> Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables with <span class="math inline">\(X \sim Exponential(1)\)</span> and <span class="math inline">\(Y \sim Exponential(1)\)</span>. Find the joint distribution of <span class="math inline">\(Z = X - Y\)</span> and <span class="math inline">\(W = X + Y\)</span>, and use this joint distribution to show that <span class="math inline">\(Z \sim Laplace(0, 1)\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables with <span class="math inline">\(X \sim Exponential(1)\)</span> and <span class="math inline">\(Y \sim Exponential(1)\)</span>. Find the joint distribution of <span class="math inline">\(Z = X - Y\)</span> and <span class="math inline">\(W = X + Y\)</span>, and use this joint distribution to show that <span class="math inline">\(Z \sim Laplace(0, 1)\)</span>.</p>
<p>We’ll need a couple things before we can directly apply the Jacobian method:</p>
<ul>
<li><p>The joint distribution, <span class="math inline">\(\pi_{X,Y}(x,y)\)</span></p></li>
<li><p>Our function <span class="math inline">\(f(x,y)\)</span> and its inverse</p></li>
<li><p>Our Jacobian matrix, given by <span class="math inline">\(J_{f}(z,w) = \begin{pmatrix} \frac{\partial x}{\partial z} &amp; \frac{\partial x}{\partial w} \\\frac{\partial y}{\partial z} &amp; \frac{\partial y}{\partial w} \end{pmatrix}\)</span></p></li>
<li><p>Our Jacobian, given by <span class="math inline">\(\bigg| J_{f}(z,w) \bigg|\)</span></p></li>
<li><p>The support of the joint distribution <span class="math inline">\(\pi_{Z,W}(z,w)\)</span> (we’ll do this step at the end).</p></li>
</ul>
<p>Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we have <span class="math display">\[
\pi_{X,Y}(x,y) = e^{-x}e^{-y} = e^{-x - y}
\]</span> Now we must determine what our function <span class="math inline">\(f\)</span> is, and its inverse. From the problem set-up, we have <span class="math inline">\(f(x, y) \longmapsto (x - y, x + y)\)</span>. To find the inverse function, we can rearrange these outputs to define <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> solely in terms of <span class="math inline">\(z\)</span> and <span class="math inline">\(w\)</span>. Some algebra included below: <span class="math display">\[\begin{align*}
    x &amp; = z + y \\
    y &amp; = w - x \\
    x &amp; = z + w - x \\
    2x &amp; = z + w \\
    x &amp; = \frac{z + w}{2} \quad \text{(Our first equation)!} \\
    y &amp; = w - \frac{z + w}{2} \\
    y &amp; = \frac{2w - z - w}{2} \\
    y &amp; = \frac{w - z}{2} \quad \text{(Our second equation!)}
\end{align*}\]</span> Which gives us <span class="math inline">\(f^{-1}(z,w) \longmapsto (\frac{z + w}{2}, \frac{w - z}{2})\)</span>. The Jacobian matrix is then given by <span class="math display">\[
J_f(z,w) = \begin{pmatrix}
    \frac{\partial (\frac{z + w}{2})}{\partial z} &amp; \frac{\partial (\frac{z + w}{2})}{\partial w} \\
     \frac{\partial (\frac{w - z}{2})}{\partial z} &amp; \frac{\partial (\frac{w - z}{2})}{\partial w}
\end{pmatrix} = \begin{pmatrix}
     \frac{1}{2} &amp; \frac{1}{2} \\
     \frac{-1}{2} &amp; \frac{1}{2}
     \end{pmatrix}
\]</span> and the Jacobian is given by <span class="math inline">\(|J_f(z,w)| = (1/2)(1/2) - (1/2)(-1/2) = 1/2\)</span>.</p>
<p>Now we determine the support of the distribution <span class="math inline">\(\pi_{Z,W} (z,w)\)</span>. Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are exponential, we know that <span class="math display">\[\begin{align*}
    &amp; 0 \leq x &lt; \infty \\
    &amp; 0 \leq y &lt; \infty
\end{align*}\]</span> Plugging in some things and rearranging, we get <span class="math display">\[\begin{align*}
    &amp; 0 \leq \frac{z + w}{2} &lt; \infty \\
    &amp; 0 \leq z + w &lt; \infty \\
    &amp; -z \leq w &lt; \infty \quad \text{(or)} -w \leq z &lt; \infty
\end{align*}\]</span> and<br>
<span class="math display">\[\begin{align*}
    &amp; 0 \leq \frac{w - z}{2} &lt; \infty \\
    &amp; 0 \leq w - z &lt; \infty \\
    &amp; z \leq w &lt; \infty
\end{align*}\]</span></p>
<p>Putting these together, we have <span class="math inline">\(-w \leq z \leq w &lt; \infty\)</span>, so the support for <span class="math inline">\(z\)</span>, marginally is given by <span class="math inline">\([-w, w]\)</span>. The support for <span class="math inline">\(w\)</span>, marginally, is given by <span class="math inline">\([0, \infty)\)</span>, since it is a sum of two random variables that have that same support. Note that this therefore means <span class="math inline">\(z\)</span> can range from <span class="math inline">\((-\infty, \infty)\)</span>, depending on the value of <span class="math inline">\(w\)</span>.</p>
<p>We can now finally apply the Jacobian method to obtain <span class="math inline">\(\pi_{Z,W}(z,w)\)</span> using the separate pieces we have calculated, obtaining: <span class="math display">\[\begin{align*}
    \pi_{Z,W}(z,w) &amp; = \pi_{X,Y}(f^{-1}(z,w)) \times | J_f(z,w) |  \times I \{ -w \leq z \leq w, 0 \leq w &lt; \infty \}\\
    &amp; = \exp(-\frac{z + w}{2} - \frac{w-z}{2}) \times \frac{1}{2} \times I \{ -w \leq z \leq w, 0 \leq w &lt; \infty \}\\
    &amp; = \frac{1}{2} \exp(\frac{-z - w - w + z}{2}) \times I \{ -w \leq z \leq w, 0 \leq w &lt; \infty \}\\
    &amp; = \frac{1}{2} e^{-w} \times I \{ -w \leq z \leq w, 0 \leq w &lt; \infty \}
\end{align*}\]</span> Now that we have the joint distribution <span class="math inline">\(\pi_{Z,W}(z,w)\)</span>, we must integrate with respect to <span class="math inline">\(W\)</span> to get the marginal distribution of <span class="math inline">\(Z\)</span>. Recall that we have both <span class="math inline">\(-z \leq w &lt; \infty\)</span> and <span class="math inline">\(z \leq w &lt; \infty\)</span>, so we consider these two cases separately. We have <span class="math display">\[
\pi_Z(z) = \begin{cases}
    \int_z^\infty \frac{1}{2} e^{-w} dw = - \frac{1}{2} e^{-w} \big|_z^\infty = \frac{1}{2}e^{-z} &amp; 0 \leq z &lt; \infty\\
    \int_{-z}^\infty \frac{1}{2} e^{-w} dw = - \frac{1}{2} e^{-w} \big|_{-z}^\infty = \frac{1}{2}e^{z} &amp; -\infty &lt; z \leq 0
\end{cases}
\]</span> which can equivalently be written as <span class="math display">\[
\pi_Z(z) = \frac{1}{2} e^{-|z|} \quad -\infty &lt; z &lt; \infty
\]</span> which implies <span class="math inline">\(Z \sim Laplace(0,1)\)</span>.</p>
</details>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/math-stat-355\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Welcome to Statistical Theory!">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome to Statistical Theory!</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mle.html" class="pagination-link" aria-label="Maximum Likelihood Estimation">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>