<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MATH/STAT 455: Mathematical Statistics - 1&nbsp; Probability: A Brief Review</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./mle.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./probability.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH/STAT 455: Mathematical Statistics</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylorokonek/MathematicalStatistics/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./math-stat-455.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Mathematical Statistics!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./consistency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  <li><a href="#reading-guide" id="toc-reading-guide" class="nav-link" data-scroll-target="#reading-guide"><span class="header-section-number">1.2</span> Reading Guide</a>
  <ul class="collapse">
  <li><a href="#reading-questions" id="toc-reading-questions" class="nav-link" data-scroll-target="#reading-questions"><span class="header-section-number">1.2.1</span> Reading Questions</a></li>
  </ul></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">1.3</span> Definitions</a></li>
  <li><a href="#theorems" id="toc-theorems" class="nav-link" data-scroll-target="#theorems"><span class="header-section-number">1.4</span> Theorems</a>
  <ul class="collapse">
  <li><a href="#transforming-continuous-random-variables" id="toc-transforming-continuous-random-variables" class="nav-link" data-scroll-target="#transforming-continuous-random-variables"><span class="header-section-number">1.4.1</span> Transforming Continuous Random Variables</a></li>
  </ul></li>
  <li><a href="#worked-examples" id="toc-worked-examples" class="nav-link" data-scroll-target="#worked-examples"><span class="header-section-number">1.5</span> Worked Examples</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p><em>MATH/STAT 455</em> builds directly on topics covered in <em>MATH/STAT 354: Probability</em>. You’re not expected to perfectly remember everything from <em>Probability</em>, but you will need to have sufficient facility with the following topics covered in this review Chapter in order to grasp the majority of concepts covered in <em>MATH/STAT 455</em>.</p>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you should be able to…</p>
<ul>
<li><p>Distinguish between important probability models (e.g., Normal, Binomial)</p></li>
<li><p>Derive the expectation and variance of a single random variable or a sum of random variables</p></li>
<li><p>Define the moment generating function and use it to find moments or identify pdfs</p></li>
</ul>
</section>
<section id="reading-guide" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="reading-guide"><span class="header-section-number">1.2</span> Reading Guide</h2>
<p>Associated Readings: Chapters 2-4 (pages 15-277)</p>
<section id="reading-questions" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="reading-questions"><span class="header-section-number">1.2.1</span> Reading Questions</h3>
<ol type="1">
<li><p>Which probability distributions are appropriate for <em>quantitative</em> (continuous) random variables?</p></li>
<li><p>Which probability distributions are appropriate for <em>categorical</em> random variables?</p></li>
<li><p><em>Independently and Identically Distributed (iid)</em> random variables are an incredibly important assumption involved in many statistical methods. Why do you think it might be important/useful for random variables to have this property?</p></li>
</ol>
</section>
</section>
<section id="definitions" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">1.3</span> Definitions</h2>
<p>You are expected to know the following definitions:</p>
<p><strong>Random Variable</strong></p>
<p>A random variable is a function that takes inputs from a sample space of all possible outcomes, and outputs real values or probabilities. As an example, consider a coin flip. The sample space of all possible outcomes consists of “heads” and “tails”, and each outcome is associated with a probability (50% each, for a fair coin). For our purposes, you should know that random variables have probability density (or mass) functions, and are either discrete or continuous based on the number of possible outcomes a random variable may take. Random variables are often denoted with capital Roman letters, like <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, <span class="math inline">\(Z\)</span>, etc.</p>
<p><strong>Probability density function</strong> (discrete, continuous)</p>
<ul>
<li>Note: I don’t care if you call a pmf a pdf… I will probably do this continuously throughout the semester. We don’t need to be picky about this in <em>MATH/STAT 455</em>.</li>
</ul>
<p>There are many different accepted ways to write the notation for a pdf of a random variables. Any of the following are perfectly appropriate for this class: <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(\pi(x)\)</span>, <span class="math inline">\(p(x)\)</span>, <span class="math inline">\(f_X(x)\)</span>. I typically use either <span class="math inline">\(\pi\)</span> or <span class="math inline">\(p\)</span>, but might mix it up occasionally.</p>
<p>Key things I want you to know about probability density functions:</p>
<ul>
<li><p><span class="math inline">\(\pi(x) \geq 0\)</span>, everywhere. This should make sense (hopefully) because probabilities cannot be negative!</p></li>
<li><p><span class="math inline">\(\int_{-\infty}^\infty \pi(x) = 1\)</span>. This should also (hopefully) makes sense. Probabilities can’t be <em>greater</em> than one, and the probability of event occurring <em>at all (ever)</em> should be equal to one, if the event <span class="math inline">\(x\)</span> is a random variable.</p></li>
</ul>
<p><strong>Cumulative distribution function</strong> (discrete, continuous)</p>
<p>Cumulative distribution functions we’ll typically write as <span class="math inline">\(F_X(x)\)</span>. or <span class="math inline">\(F(x)\)</span>, for short. It is important to know that</p>
<p><span class="math display">\[
F_X(x) = \Pr(X \leq x),
\]</span></p>
<p>or in words, “the cumulative distribution function is the probability that a random variable lies before <span class="math inline">\(x\)</span>.” If you write <span class="math inline">\(\Pr(X &lt; x)\)</span> instead of <span class="math inline">\(\leq\)</span>, you’re fine. The probability that a random variable is exactly one number (for an RV with a continuous pdf) is zero anyway, so these are the same thing. Key things I want you to know about cumulative distribution functions:</p>
<ul>
<li><p><span class="math inline">\(F(x)\)</span> is non-decreasing. This is in part where the “cumulative” piece comes in to play. Recall that probabilities are basically integrals or sums. If we’re integrating over something positive, and our upper bound for our integral <em>increases</em>, the area under the curve (cumulative probability) will increase as well.</p></li>
<li><p><span class="math inline">\(0 \leq F(x) \leq 1\)</span> (since probabilities have to be between zero and one!)</p></li>
<li><p><span class="math inline">\(\Pr(a &lt; X \leq b) = F(a) - F(b)\)</span> (because algebra)</p></li>
</ul>
<p><strong>Joint probability density function</strong></p>
<p>A joint probability density function is a probability distribution defined for more than one random variable at a time. For two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, we could write their joint density function as <span class="math inline">\(f_{X,Z}(x, z)\)</span> , or <span class="math inline">\(f(x,z)\)</span> for short. The joint density function encodes all sorts of fun information, including <em>marginal</em> distributions for <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, and conditional distributions (see next <strong>bold</strong> definition). We can think of the joint pdf as listing all possible pairs of outputs from the density function <span class="math inline">\(f(x,z)\)</span>, for varying values of <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span>. Key things I want you to know about joint pdfs:</p>
<ul>
<li><p>How to get a marginal pdf from a joint pdf:</p>
<p>Suppose I want to know <span class="math inline">\(f_X(x)\)</span>, and I know <span class="math inline">\(f_{X,Z}(x,z)\)</span>. Then I can integrate or “average over” <span class="math inline">\(Z\)</span> to get</p>
<p><span class="math display">\[
f_X(x) = \int f_{X,Z}(x,z)dz
\]</span></p></li>
<li><p>The relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule</p></li>
<li><p>How to obtain a joint pdf for <em>independent</em> random variables: just multiply their marginal pdfs together! This is how we will (typically) think about likelihoods!</p></li>
<li><p>How to obtain a marginal pdf from a joint pdf when random variables are independent <em>without integrating</em> (think, “separability”)</p></li>
</ul>
<p><strong>Conditional probability density function</strong></p>
<p>A conditional pdf denotes the probability distribution for a (set of) random variable(s), <em>given that</em> the value for another (set of) random variable(s) is known. For two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, we could write the conditional distribution of <span class="math inline">\(X\)</span> “given” <span class="math inline">\(Z\)</span> as <span class="math inline">\(f_{X \mid Z}(x \mid z)\)</span> , where the “conditioning” is denoted by a vertical bar (in LaTeX, this is typeset using “\mid”). Key things I want you to know about conditional pdfs:</p>
<ul>
<li><p>The relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule</p></li>
<li><p>How to obtain a conditional pdf from a joint pdf (again, think Bayes’ rule)</p></li>
<li><p>Relationship between conditional pdfs and independence (see next <strong>bold</strong> definition)</p></li>
</ul>
<p><strong>Independence</strong></p>
<p>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are <em>independent</em> if and only if:</p>
<ul>
<li><p><span class="math inline">\(f_{X,Z}(x,z) = f_X(x) f_Z(z)\)</span> (their joint pdf is “separable”)</p></li>
<li><p><span class="math inline">\(f_{X\mid Z}(x\mid z) = f_X(x)\)</span> (the pdf for <span class="math inline">\(X\)</span> does not depend on <span class="math inline">\(Z\)</span> in any way)</p>
<p>Note that the “opposite” is also true: <span class="math inline">\(f_{Z\mid X}(z\mid x) = f_Z(z)\)</span></p></li>
</ul>
<p>In notation, we denote that two variables are independent as <span class="math inline">\(X \perp\!\!\!\perp Z\)</span>, or <span class="math inline">\(X \perp Z\)</span>. In LaTeX, the <em>latter</em> is typeset as “\perp”, and the former is typeset as “\perp\!\!\!\perp”. As a matter of personal preference, I (Taylor) prefer <span class="math inline">\(\perp\!\!\!\perp\)</span>, but I don’t like typing it out every time. Consider using the “\newcommand” functionality in LaTeX to create a shorthand for this for your documents!</p>
<p><strong>Expected Value / Expectation</strong></p>
<p>The expectation (or expected value) of a random variable is defined as:</p>
<p><span class="math display">\[
E[X] = \int_{-\infty}^\infty x f(x) dx
\]</span></p>
<p>Expected value is a weighted average, where the average is over all possible values a random variable can take, weighted by the probability that those values occur. Key things I want you to know about expectation:</p>
<ul>
<li><p>The relationship between expectation, variance, and moments (specifically, that <span class="math inline">\(E[X]\)</span> is the 1st moment!)</p></li>
<li><p>The “law of the unconscious statistician” (see the Theorems section of this chapter)</p></li>
<li><p>Expectation of linear transformations of random variables (see <strong>Theorems</strong> section of this chapter)</p></li>
</ul>
<p><strong>Variance</strong></p>
<p>The variance of a random variable is defined as:</p>
<p><span class="math display">\[
Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2
\]</span></p>
<p>In words, we can read this as “the expected value of the squared deviation from the mean” of a random variable <span class="math inline">\(X\)</span>. Key things I want you to know about variance:</p>
<ul>
<li><p>The relationship between expectation, variance, and moments (hopefully clear, given the formula for variance)</p></li>
<li><p>The relationship between variance and standard deviation: <span class="math inline">\(Var(X) = sd(X)^2\)</span></p></li>
<li><p>The relationship between variance and covariance: <span class="math inline">\(Var(X) = Cov(X, X)\)</span></p></li>
<li><p><span class="math inline">\(Var(X) \geq 0\)</span>. This should make sense, given that we’re taking the expectation of something “squared” in order to calculate it!</p></li>
<li><p><span class="math inline">\(Var(c) = 0\)</span> for any constant, <span class="math inline">\(c\)</span>.</p></li>
<li><p>Variance of linear transformations of random variables (see <strong>Theorems</strong> section of this chapter)</p></li>
</ul>
<p><span class="math inline">\(r^{th}\)</span> <strong>moment</strong></p>
<p>The <span class="math inline">\(r^{th}\)</span> moment of a probability distribution is given by <span class="math inline">\(E[X^r]\)</span>. For example, when <span class="math inline">\(r = 1\)</span>, the <span class="math inline">\(r^{th}\)</span> moment is just the expectation of the random variable <span class="math inline">\(X\)</span>. Key things I want you to know about moments:</p>
<ul>
<li><p>The relationship between moments, expectation, and variance</p>
<ul>
<li>For example, if you know the first and second moments of a distribution, you should be able to calculate the variance of a random variable with that distribution!</li>
</ul></li>
<li><p>The relationship between moments and <em>moment generating functions</em> (see <strong>Theorems</strong> section of this chapter)</p></li>
</ul>
<p><strong>Covariance</strong></p>
<p>The covariance of two random variables is a measure of their <em>joint</em> variability. We denote the covariance of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> as <span class="math inline">\(Cov(X,Z)\)</span>, and</p>
<p><span class="math display">\[
Cov(X, Z) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
\]</span></p>
<p>Some things I want you to know about covariance:</p>
<ul>
<li><p><span class="math inline">\(Cov(X, X) = Var(X)\)</span></p></li>
<li><p><span class="math inline">\(Cov(X, Y) = Cov(Y, X)\)</span> (order doesn’t matter)</p></li>
</ul>
<p><strong>Moment Generating Function</strong></p>
<p>You are also expected to know the probability distributions contained in Table 1, below. Note that you <em>do not</em> need to memorize the pdfs for these distributions, but you <em>should</em> be familiar with what types of random variables (continuous/quantitative, categorical, integer-valued, etc.) may take on different distributions. The more familiar you are with the forms of the pdfs, the easier/faster it will be to work through problem sets and quizzes.</p>
<table class="table">
<caption><em>Table 1.</em> Table of main probability distributions we will work with for <em>MATH/STAT 455</em>.</caption>
<colgroup>
<col style="width: 7%">
<col style="width: 44%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>PDF/PMF</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Uniform</td>
<td><span class="math inline">\(\pi(x) = \frac{1}{\beta - \alpha}\)</span></td>
<td><span class="math inline">\(\alpha \in \mathbb{R}\)</span>, <span class="math inline">\(\beta\in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td>Normal</td>
<td><span class="math inline">\(\pi(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{1}{2\sigma^2} (x - \mu)^2)\)</span></td>
<td><span class="math inline">\(\mu \in \mathbb{R}\)</span>, <span class="math inline">\(\sigma &gt; 0\)</span></td>
</tr>
<tr class="odd">
<td>Multivariate Normal</td>
<td><span class="math inline">\(\pi(\textbf{x}) - (2\pi)^{-k/2} |\Sigma|^{-1/2} \exp(-\frac{1}{2}(\textbf{x} - \mu)^\top \Sigma^{-1}(\textbf{x} - \mu)))\)</span></td>
<td><span class="math inline">\(\mu \in \mathbb{R}^k\)</span>, <span class="math inline">\(\Sigma \in \mathbb{R}^{k\times k}\)</span> , positive semi-definite (in practice, almost always positive definite)</td>
</tr>
<tr class="even">
<td>Gamma</td>
<td><span class="math inline">\(\pi(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1} e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\alpha \text{ (shape)}, \beta \text{ (rate)} &gt; 0\)</span></td>
</tr>
<tr class="odd">
<td>Chi-square</td>
<td><span class="math inline">\(\pi(x) = \frac{2^{-\nu/2}}{\Gamma(\nu/2)} x^{\nu/2 - 1}e^{-x/2})\)</span></td>
<td><span class="math inline">\(\nu &gt; 0\)</span></td>
</tr>
<tr class="even">
<td>Exponential</td>
<td><span class="math inline">\(\pi(x) = \beta e^{-\beta x}\)</span></td>
<td><span class="math inline">\(\beta &gt; 0\)</span></td>
</tr>
<tr class="odd">
<td>Student-$t$</td>
<td><span class="math inline">\(\pi(x) = \frac{\Gamma((\nu + 1)/2)}{\Gamma(\nu/2) \sqrt{\nu \pi}} (1 + \frac{x^2}{\nu})^{-(\nu + 1)/2}\)</span></td>
<td><span class="math inline">\(\nu &gt; 0\)</span></td>
</tr>
<tr class="even">
<td>Beta</td>
<td><span class="math inline">\(\pi(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha - 1}(1 - x)^{\beta - 1}\)</span></td>
<td><span class="math inline">\(\alpha, \beta &gt; 0\)</span></td>
</tr>
<tr class="odd">
<td>Poisson</td>
<td><span class="math inline">\(\pi(x) = \frac{\lambda^k e^{-\lambda}}{k!}\)</span></td>
<td><span class="math inline">\(\lambda &gt; 0\)</span></td>
</tr>
<tr class="even">
<td>Binomial</td>
<td><span class="math inline">\(\pi(x) = \binom{n}{x} p^{x} (1 - p)^{n - x}\)</span></td>
<td><span class="math inline">\(p \in [0,1], n = \{0, 1, 2, \dots\}\)</span></td>
</tr>
<tr class="odd">
<td>Multinomial</td>
<td><span class="math inline">\(\pi(\textbf{x}) = \frac{n!}{x_1! \dots x_k!} p_1^{x_1} \dots p_k^{x_k}\)</span></td>
<td><span class="math inline">\(p_i &gt; 0\)</span>, <span class="math inline">\(p_1 + \dots + p_k = 1\)</span>, <span class="math inline">\(n = \{0, 1, 2, \dots \}\)</span></td>
</tr>
<tr class="even">
<td>Negative Binomial</td>
<td><span class="math inline">\(\pi(x) = \binom{k + r - 1}{k} (1-p)^k p^r\)</span></td>
<td><span class="math inline">\(r &gt; 0\)</span>, <span class="math inline">\(p \in [0,1]\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="theorems" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="theorems"><span class="header-section-number">1.4</span> Theorems</h2>
<ul>
<li><p>Law of Total Probability</p>
<p><span class="math display">\[
P(A) = \sum_n P(A \cap B_n),
\]</span>or</p>
<p><span class="math display">\[
P(A) = \sum_n P(A \mid B_n) P(B_n)
\]</span></p></li>
<li><p>Bayes’ Theorem</p>
<p><span class="math display">\[
\pi(A \mid B) = \frac{\pi(B \mid A) \pi(A)}{\pi(B)}
\]</span></p></li>
<li><p>Relationship between pdf and cdf</p>
<p><span class="math display">\[
F_Y(y) = \int_{-\infty}^y f_Y(t)dt
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial y}F_Y(y) = f_Y(y)
\]</span></p></li>
<li><p>Expectation of random variables</p>
<p><span class="math display">\[
E[X] = \int_{-\infty}^\infty x f(x) dx
\]</span></p>
<p><span class="math display">\[
E[X^2] = \int_{-\infty}^\infty x^2 f(x) dx
\]</span></p>
<ul>
<li><p>“Law of the Unconscious Statistician”</p>
<p><span class="math display">\[
E[g(X)] = \int_{-\infty}^\infty g(x)f(x)dx
\]</span></p></li>
</ul></li>
<li><p>Expectation and variance of linear transformations of random variables</p>
<p><span class="math display">\[
E[cX + b] = c E[X] + b
\]</span></p>
<p><span class="math display">\[
Var[cX + b] = c^2 Var[X]
\]</span></p></li>
<li><p>Relationship between mean and variance</p>
<p><span class="math display">\[
Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2
\]</span></p>
<p>Also, recall that <span class="math inline">\(Cov[X, X] = Var[X]\)</span>.</p></li>
<li><p>Finding a marginal pdf from a joint pdf</p>
<p><span class="math display">\[
f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x, y) dy
\]</span></p></li>
<li><p>Independence of random variables and joint pdfs</p>
<p>If two random variables are independent, their joint pdf will be <em>separable</em>. For example, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we could write</p>
<p><span class="math display">\[
f_{X,Y}(x, y) = f_{X}(x)f_Y(y)
\]</span></p></li>
<li><p>Expected value of a product of independent random variables</p>
<p>Suppose random variables <span class="math inline">\(X_1, \dots, X_n\)</span> are independent. Then we can write,</p>
<p><span class="math display">\[
E\left[\prod_{i = 1}^n X_i\right] = \prod_{i = 1}^n E[X_i]
\]</span></p></li>
<li><p>Covariance of independent random variables</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(Cov(X, Y) = 0\)</span>. We can show this by noting that</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
Cov(X, Y) &amp; = E[(X - E[X])(Y - E[Y])] \\
&amp; = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\
&amp; = E[XY] - E[XE[Y]] - E[YE[X]] + E[X]E[Y] \\
&amp; =  2E[X]E[Y] - 2E[X]E[Y] \\
&amp; = 0
\end{align}
\]</span></p>
<ul>
<li><p>Using MGFs to find moments</p>
<p>Recall that the moment generating function of a random variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(M_X(t)\)</span> is</p>
<p><span class="math display">\[
M_X(t) = E[e^{tX}]
\]</span></p>
<p>Then the <span class="math inline">\(n\)</span>th moment of the probability distribution for <span class="math inline">\(X\)</span> , <span class="math inline">\(E[X^n]\)</span>, is given by</p>
<p><span class="math display">\[
\frac{\partial M_X}{\partial t^n} \Bigg|_{t = 0}
\]</span></p>
<p>where the above reads as “the <span class="math inline">\(n\)</span>th derivative of the moment generating function, evaluated at <span class="math inline">\(t = 0\)</span>.”</p></li>
<li><p>Using MGFs to identify pdfs</p>
<p>MGFs uniquely identify probability density functions. If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two random variables where for all values of <span class="math inline">\(t\)</span>, <span class="math inline">\(M_X(t) = M_Y(t)\)</span>, then <span class="math inline">\(F_X(x) = F_Y(y)\)</span>.</p></li>
<li><p>Central Limit Theorem</p>
<p>The classical CLT states that for independent and identically distributed (iid) random variables <span class="math inline">\(X_1, \dots, X_n\)</span>, with expected value <span class="math inline">\(E[X_i] = \mu\)</span> and <span class="math inline">\(Var[X_i] = \sigma^2 &lt; \infty\)</span>, the sample average (centered and standardized) converges in distribution to a standard normal distribution at a root-<span class="math inline">\(n\)</span> rate. Notationally, this is written as</p>
<p><span class="math display">\[
\sqrt{n} (\bar{X} - \mu) \overset{d}{\to} N(0, \sigma^2)
\]</span></p>
<p><img src="images/chilipepper.png" width="20" height="16"> A fun aside: this is only <em>one</em> CLT, often referred to as the Levy CLT. There are other CLTs, such as the Lyapunov CLT and Lindeberg-Feller CLT!</p></li>
</ul>
<section id="transforming-continuous-random-variables" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="transforming-continuous-random-variables"><span class="header-section-number">1.4.1</span> Transforming Continuous Random Variables</h3>
<p>We will <em>often</em> take at face value previously proven <em>relationships</em> between random variables. What I mean by this, as an example, is that it is a nice (convenient) fact that a sum of two independent normal random variables is <em>still</em> normally distributed, with a nice form for the mean and variance. In particular, if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> and <span class="math inline">\(Y \sim N(\theta, \nu^2)\)</span>, then <span class="math inline">\(X + Y \sim N(\mu + \theta, \sigma^2 + \nu^2)\)</span>. Most frequently used examples of these sorts of relationships can be found in the ``Related Distributions” section of the Wikipedia page for a given probability distribution. Unless I explicitly ask you to derive/show how certain variables are related to each other, you can just state the known relationship, use it, and move on!</p>
<p>If I <em>do</em> ask you to derive/show these things, there a few different ways we can go about this. For this course, I only expect you to know the “CDF method” for <em>one function</em> of <em>one random variable</em>, as we’ll demonstrate below.</p>
<p><strong>Theorem</strong>. Let <span class="math inline">\(X\)</span> be a continuous random variable with pdf <span class="math inline">\(f_X(x)\)</span>. Define a new random variable <span class="math inline">\(Y = g(X)\)</span>, for nice* functions <span class="math inline">\(g\)</span>. Then <span class="math inline">\(f_Y(y) = f_X(g^{-1}(y)) \times \frac{1}{g'(g^{-1}(y))}\)</span>.</p>
<p><img src="images/chilipepper.png" width="20" height="16"> *By <em>nice</em> functions we mean functions that are strictly increasing and smooth <em>on the required range</em>. As an example, <span class="math inline">\(exp(x)\)</span> is a smooth, strictly increasing function; <span class="math inline">\(|x|\)</span> is not on the <em>whole real line</em>, but <em>is</em> from <span class="math inline">\((0, \infty)\)</span> (where a lot of useful pdfs are defined). For the purposes of this class, every function that you will need to do this for will be “nice.” Note that there are also considerations that need to be taken regarding the <em>range</em> of continuous random variables when considering transforming them. We will mostly ignore these considerations in this class, but a technically complete derivation (or proof) must consider them.</p>
<p><strong>Proof</strong>. We can write</p>
<p><span class="math display">\[\begin{align*}
    f_Y(y) &amp; = \frac{\partial}{\partial y} F_Y(y) \\
    &amp; = \frac{\partial}{\partial y} \Pr(Y \leq y) \\
    &amp; = \frac{\partial}{\partial y} \Pr(g(X) \leq y) \\
    &amp; = \frac{\partial}{\partial y} \Pr(X \leq g^{-1}(y)) \\
    &amp; = \frac{\partial}{\partial y} F_X(g^{-1}(y)) \\
    &amp; = f_X(g^{-1}(y)) \times \frac{\partial}{\partial y} g^{-1}(y)
\end{align*}\]</span> where to obtain the last equality we use chain rule! Now we require some statistical trickery to continue(note that this method is called the “CDF method” because we go <em>through</em> the CDF to derive the distribution for <span class="math inline">\(Y\)</span>)</p>
<p>You will see this in the Bayes chapter of our course notes, but it is often true that our lives are made easier as statisticians if we multiply things by one, or add zero. What exactly do I mean? Rearranging gross looking formulas into things we are familiar with (like pdfs, for example) often makes our lives easier and allows us to avoid dealing with such grossness. Here, the grossness is less obvious, but nonetheless relevant. Note that we can write</p>
<p><span class="math display">\[\begin{align*}
    y &amp; = y \\
    y &amp; = g(g^{-1}(y)) \\
    \frac{\partial}{\partial y} y &amp; = \frac{\partial}{\partial y} g(g^{-1}(y)) \\
    1 &amp; = g'(g^{-1}(y)) \frac{\partial}{\partial y} g^{-1}(y) \hspace{1cm} \text{(chain rule again!)} \\
    \frac{1}{g'(g^{-1}(y))} &amp; = \frac{\partial}{\partial y} g^{-1}(y)
\end{align*}\]</span> The right-hand side should look familiar: it is exactly what we needed to “deal with” in our proof! Returning to that proof, we have</p>
<p><span class="math display">\[\begin{align*}
    f_Y(y) &amp; = f_X(g^{-1}(y)) \times \frac{\partial}{\partial y} g^{-1}(y) \\
    &amp; = f_X(g^{-1}(y)) \times \frac{1}{g'(g^{-1}(y))}
\end{align*}\]</span> as desired.</p>
</section>
</section>
<section id="worked-examples" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="worked-examples"><span class="header-section-number">1.5</span> Worked Examples</h2>
<p><strong>Problem 1:</strong> Suppose <span class="math inline">\(X \sim Exponential(\lambda)\)</span>. Calculate <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(Var[X]\)</span>.</p>
<p>We know that <span class="math inline">\(f(x) = \lambda e^{-\lambda x}\)</span>. If we can calculate <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[X^2]\)</span>, then we’re basically done! We can write</p>
<p><span class="math display">\[
\begin{align*}    
E[X] &amp; = \int_0^\infty x \lambda e^{-\lambda x} dx \\    
&amp; = \lambda \int_0^\infty x e^{-\lambda x} dx
\end{align*}
\]</span></p>
<p>And now we need integration by parts! Set <span class="math inline">\(u = x\)</span>, <span class="math inline">\(dv = e^{-\lambda x} dx\)</span>. Then <span class="math inline">\(du = 1dx\)</span> and <span class="math inline">\(v = \frac{-1}{\lambda} e^{-\lambda x}\)</span>. Since <span class="math inline">\(\int u dv = uv - \int vdu\)</span>, we can continue</p>
<p><span class="math display">\[
\begin{align*}    
E[X] &amp; = \lambda \int_0^\infty x e^{-\lambda x} dx \\    
&amp; = \lambda \left( -\frac{x}{\lambda} e^{-\lambda x} \bigg|_0^\infty  - \int_0^\infty \frac{-1}{\lambda} e^{-\lambda x} dx \right) \\    
&amp; = \lambda \left( - \int_0^\infty \frac{-1}{\lambda} e^{-\lambda x} dx \right) \\    
&amp; = \lambda \left( \frac{-1}{\lambda^2} e^{-\lambda x}  \bigg|_0^\infty \right) \\    
&amp; = \frac{-1}{\lambda} e^{-\lambda x}  \bigg|_0^\infty \\    
&amp; = \frac{1}{\lambda} e^{-0} \\    
&amp; = \frac{1}{\lambda}
\end{align*}
\]</span></p>
<p>We can follow a similar process to get <span class="math inline">\(E[X^2]\)</span> (using the law of the unconscious statistician!). We can write</p>
<span class="math display">\[\begin{align*}
    E[X^2] &amp; = \int_0^\infty x^2 \lambda e^{-\lambda x} dx \\
    &amp; = \lambda \int_0^\infty x^2 e^{-\lambda x} dx
\end{align*}\]</span>
<p>And now we need integration by parts again! Set <span class="math inline">\(u = x^2\)</span>, <span class="math inline">\(dv = e^{-\lambda x} dx\)</span>. Then <span class="math inline">\(du = 2xdx\)</span> and <span class="math inline">\(v = \frac{-1}{\lambda} e^{-\lambda x}\)</span>. Since <span class="math inline">\(\int u dv = uv - \int vdu\)</span>, we can continue</p>
<span class="math display">\[\begin{align*}
    E[X] &amp; = \lambda \int_0^\infty x^2 e^{-\lambda x} dx \\
    &amp; = \lambda \left( -\frac{x^2}{\lambda} e^{-\lambda x} \bigg|_0^\infty  - \int_0^\infty \frac{-2}{\lambda} xe^{-\lambda x} dx \right) \\
    &amp; = \lambda \left( -\frac{x^2}{\lambda} e^{-\lambda x} \bigg|_0^\infty  + \frac{2}{\lambda} \int_0^\infty  xe^{-\lambda x} dx \right) \\
    &amp; = \lambda \left( -\frac{x^2}{\lambda} e^{-\lambda x} \bigg|_0^\infty  + \frac{2}{\lambda^3} \right)\\
    &amp; = \lambda \left( 0  + \frac{2}{\lambda^3} \right) \\
    &amp; = \frac{2}{\lambda^2}
\end{align*}\]</span>
<p>Now we can calculate <span class="math inline">\(Var[X] = E[X^2] - E[X]^2\)</span> as <span class="math display">\[
Var[X] = E[X^2] - E[X]^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}
\]</span> And so we have <span class="math inline">\(E[X] = \frac{1}{\lambda}\)</span> and <span class="math inline">\(Var[X] = \frac{1}{\lambda^2}\)</span>.</p>
<p><strong>Problem 2:</strong> Show that an exponentially distributed random variable is “memoryless”, i.e.&nbsp;show that <span class="math inline">\(\Pr(X &gt; s + x \mid X &gt; s) = \Pr(X &gt; x)\)</span>, <span class="math inline">\(\forall s\)</span>.</p>
<p>Recall that the CDF of an exponential distribution is given by <span class="math inline">\(F(x) = 1-e^{-\lambda x}\)</span>. Thanks to Bayes rule, we can write</p>
<p><span class="math display">\[\begin{align*}
    \Pr(X &gt; s + x \mid X &gt; s) &amp; = \frac{\Pr(X &gt; s + x , X &gt; s)}{\Pr(X &gt; s)} \\
    &amp; = \frac{\Pr(X &gt; s + x)}{\Pr(X &gt; s)} \\
    &amp; = \frac{1 - \Pr(X &lt; s + x)}{1 - \Pr(X &lt; s)} \\
    &amp; = \frac{1 - F(s + x)}{1 - F(s)}
\end{align*}\]</span> where the second equality is true because <span class="math inline">\(x &gt; 0\)</span>. Then we can write</p>
<span class="math display">\[\begin{align*}
    \Pr(X &gt; s + x \mid X &gt; s) &amp; = \frac{1 - F(s + x)}{1 - F(s)} \\
    &amp; = \frac{1 - \left(1 - e^{-\lambda(s + x)}\right)}{1 - \left(1 - e^{-\lambda s}\right)} \\
    &amp; = \frac{e^{-\lambda(s + x)}}{e^{-\lambda s}} \\
    &amp; = \frac{e^{-\lambda s - \lambda x}}{e^{-\lambda s}} \\
    &amp; = e^{-\lambda x} \\
    &amp; = 1 - F(x) \\
    &amp; = \Pr(X &gt; x)
\end{align*}\]</span>
<p>and we’re done!</p>
<p><strong>Problem 3:</strong> Suppose <span class="math inline">\(X \sim Exponential(1/\lambda)\)</span>, and <span class="math inline">\(Y \mid X \sim Poisson(X)\)</span>. Show that <span class="math inline">\(Y \sim Geometric(1/(1 + \lambda))\)</span>.</p>
<p>Note that we can write <span class="math inline">\(f(x, y) = f(y \mid x) f(x)\)</span>, and <span class="math inline">\(f(y) = \int f(x, y) dx\)</span>. Then</p>
<p><span class="math display">\[
f(x, y) = \left( \frac{1}{\lambda} e^{-x\lambda} \right) \left( \frac{x^y e^{-x}}{y!} \right)
\]</span> And so, <span class="math display">\[\begin{align*}
    f(y) &amp; = \int f(x, y) dx \\
    &amp; = \int \left( \frac{1}{\lambda} e^{-x\lambda} \right) \left( \frac{x^y e^{-x}}{y!} \right) dx \\
    &amp; = \frac{1}{\lambda y!} \int x^y e^{-x(1 + \lambda)/\lambda} dx
\end{align*}\]</span></p>
<p>And we can again use integration by parts! Let <span class="math inline">\(u = x^y\)</span> and <span class="math inline">\(dv = e^{-x(1 + \lambda)/\lambda} dx\)</span>. Then we have <span class="math inline">\(du = yx^{y-1} dx\)</span> and <span class="math inline">\(v = \frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda}\)</span>, and we can write</p>
<span class="math display">\[\begin{align*}
    f(y) &amp; = \frac{1}{\lambda y!} \int x^y e^{-x(1 + \lambda)/\lambda} dx \\
    &amp; = \frac{1}{\lambda y!} \left( x^y \frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda} \bigg|_{x = 0}^{x = \infty}  - \int \frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda} yx^{y-1} dx\right) \\
    &amp; = \frac{1}{\lambda y!} \left( - \int \frac{\lambda}{1 + \lambda}e^{-x(1 + \lambda)/\lambda} yx^{y-1} dx \right) \\
    &amp; = \frac{1}{\lambda y!} \left( \frac{\lambda }{1 + \lambda} \right) y \left( - \int e^{-x(1 + \lambda)/\lambda} x^{y-1} dx \right)
\end{align*}\]</span>
<p>This gross, but it’s actually not so bad. Note that, since <span class="math inline">\(Y\)</span> is Poisson, it can only take integer values beginning at 1! Then we can the process of integration by parts <span class="math inline">\(y\)</span> in order to get rid of <span class="math inline">\(x^{y\dots}\)</span> term on the inside of the integral. Specifically, each time we do this process we will pull out a <span class="math inline">\(\left( \frac{\lambda }{1 + \lambda} \right)\)</span>, and a <span class="math inline">\(y - i\)</span> for the <span class="math inline">\(i\)</span>th integration by parts step (try this one or two steps for yourself to see how it will simplify if you find this unintuitive!). We end up with,</p>
<span class="math display">\[\begin{align*}
    f(y) &amp; = \frac{1}{\lambda y!} \left( \frac{\lambda }{1 + \lambda} \right)^y y! \\
    &amp; = \frac{1}{\lambda} \left(\frac{\lambda}{1 + \lambda}\right)^y
\end{align*}\]</span>
<p>Now let <span class="math inline">\(p = \frac{1}{1 + \lambda}\)</span>. If we can show that <span class="math inline">\(f(y) \sim Geometric(p)\)</span> then we’re done. Note that <span class="math inline">\(1 - p = \lambda/(1 + \lambda)\)</span>. We have</p>
<p><span class="math display">\[\begin{align*}
    f(y) &amp; = \frac{1}{\lambda} (1 - p)^y \\
    &amp; = \frac{1}{\lambda} (1 - p)^{y-1} (1-p) \\
    &amp; = (1 - p)^{y-1} \frac{1}{\lambda} \left( \frac{\lambda}{1 + \lambda} \right) \\
    &amp; = (1 - p)^{y-1} \left( \frac{1}{1 + \lambda} \right) \\
    &amp; = (1 - p)^{y-1} p
\end{align*}\]</span> which is exactly the pdf of a geometric random variable with parameter <span class="math inline">\(p\)</span> and trials that begin at 1 (as opposed to 0), as makes sense with the Poisson distribution.</p>
<p><strong>Problem 4:</strong> Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, and let <span class="math inline">\(Y = \frac{X - \mu}{\sigma}\)</span>. Find the distribution of <span class="math inline">\(Y\)</span> (simplifying all of your math will be useful for this problem).</p>
<p>To solve this problem, we can use the theorem on transforming continuous random variables. We must first define our function <span class="math inline">\(g\)</span> that relates <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In this case, we have <span class="math inline">\(g(a) = \frac{a - \mu}{\sigma}\)</span>. Now all we need to do is collect the mathematical “pieces” we need to use theorem: <span class="math inline">\(g^{-1}(a)\)</span>, and <span class="math inline">\(g'(a)\)</span>, and finally, the pdf of a normal random variable. We have</p>
<span class="math display">\[\begin{align*}
    f_X(x) &amp; = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{1}{2\sigma^2} (x - \mu)^2) \\
    g^{-1}(a) &amp; = \sigma a + \mu \\
    g'(a) &amp; = \frac{\partial}{\partial a} \left(\frac{a - \mu}{\sigma}\right) = \frac{1}{\sigma}
\end{align*}\]</span>
<p>Putting it all together, we have</p>
<p><span class="math display">\[\begin{align*}
    f_Y(y) &amp; = f_X(g^{-1}(y)) \times \frac{1}{g'(g^{-1}(y))} \\
    &amp; = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{1}{2\sigma^2} (\sigma y + \mu - \mu)^2) \times \sigma \\
    &amp; = \frac{1}{\sqrt{2 \pi} \sigma} \exp(-\frac{1}{2\sigma^2} (\sigma y)^2) \times \sigma \\
    &amp; = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{1}{2\sigma^2} \sigma^2 y^2) \\
    &amp; = \frac{1}{\sqrt{2 \pi}} \exp(-\frac{1}{2} y^2)
\end{align*}\]</span> and note that this is the pdf of a normally distributed random variable with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>! Thus, we have shown that <span class="math inline">\(\frac{X - \mu}{\sigma} \sim N(0,1)\)</span>. If this random variable reminds you of a Z-score, it should!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome to Mathematical Statistics!</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mle.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>