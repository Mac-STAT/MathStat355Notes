<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MATH/STAT 455: Mathematical Statistics - 7&nbsp; Hypothesis Testing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bayes.html" rel="next">
<link href="./asymptotics.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./hypothesis.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH/STAT 455: Mathematical Statistics</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylorokonek/MathematicalStatistics/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./math-stat-455.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Mathematical Statistics!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./consistency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Computational Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">7.1</span> Learning Objectives</a></li>
  <li><a href="#reading-guide" id="toc-reading-guide" class="nav-link" data-scroll-target="#reading-guide"><span class="header-section-number">7.2</span> Reading Guide</a>
  <ul class="collapse">
  <li><a href="#reading-questions" id="toc-reading-questions" class="nav-link" data-scroll-target="#reading-questions"><span class="header-section-number">7.2.1</span> Reading Questions</a></li>
  </ul></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">7.3</span> Definitions</a></li>
  <li><a href="#theorems" id="toc-theorems" class="nav-link" data-scroll-target="#theorems"><span class="header-section-number">7.4</span> Theorems</a></li>
  <li><a href="#worked-examples" id="toc-worked-examples" class="nav-link" data-scroll-target="#worked-examples"><span class="header-section-number">7.5</span> Worked Examples</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The goal of hypothesis testing is to make a decision between two conflicting theories, or “hypotheses.” The process of hypothesis testing involves the following steps:</p>
<ol type="1">
<li><p>State the hypotheses: <span class="math inline">\(H_0\)</span> (null hypothesis) vs <span class="math inline">\(H_1\)</span> (alternative hypothesis)</p></li>
<li><p>Investigate: are data compatible with <span class="math inline">\(H_0\)</span>? assuming <span class="math inline">\(H_0\)</span> were true, are data extreme?</p></li>
<li><p>Make a decision: reject <span class="math inline">\(H_0\)</span> or fail to reject <span class="math inline">\(H_0\)</span></p></li>
</ol>
<p>The first step is relatively straightforward. For the purposes of this course, our null hypothesis will always be that some unknown parameter we are interested in <span class="math inline">\((\theta)\)</span> is equal to a fixed point <span class="math inline">\((\theta_0)\)</span>. We’ll consider two possible alternatives hypothesis:</p>
<ul>
<li><p><span class="math inline">\(H_1: \theta = \theta_1\)</span> (“simple” alternative)</p></li>
<li><p><span class="math inline">\(H_1: \theta \neq \theta_0\)</span> (two-sided alternative)</p></li>
</ul>
<p>The former is the simplest, non-trivial alternative hypothesis we can consider, and we can prove some nice things in this setting (and hence build intuition for hypothesis testing broadly). The latter is perhaps more relevant, particularly in linear regression.</p>
<p>If you recall from introductory statistics, the latter alternative provides the set-up we have when testing if the linear relationship between a predictor <span class="math inline">\(X\)</span> and outcome <span class="math inline">\(Y\)</span> are “statistically significantly” associated; we test the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span> against the alternative, <span class="math inline">\(H_1: \beta_1 \neq 0\)</span>, where <span class="math inline">\(E[Y \mid X] = \beta_0 + \beta_1 X\)</span>. In this example, we’d have the unknown parameter <span class="math inline">\(\beta_1\)</span>, and the fixed point of our null hypothesis as <span class="math inline">\(\theta_0 = 0\)</span>.</p>
<p>The second step of hypothesis testing is the investigation. In determining whether the data are compatible with the null hypothesis, we must first derive a <em>test statistic</em>. Test statistics are typically functions of (1) our estimators and (2) the distribution of our estimator <em>under the null hypothesis</em>. Intuitively, if we can determine the distribution of our estimator under the null hypothesis, we can then observe whether or not the data we actually have is “extreme” or not, given a certain threshold, <span class="math inline">\(\alpha\)</span>, for our hypothesis test. This threshold <span class="math inline">\(\alpha\)</span> is directly related to a <span class="math inline">\(100(1 - \alpha)\%\)</span> confidence interval, where anything observed outside the confidence interval bounds is considered to lie in the “rejection region” (where you would thus reject the null hypothesis).</p>
<p>There are three classical forms of test statistics that have varying finite-sample properties, and can be shown to be asymptotically equivalent: the Wald test, the likelihood ratio test (LRT), and the score test (also sometimes called the Lagrange multiplier test). Each of these is explained in further detail below.</p>
<section id="wald-tests" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="wald-tests">Wald Tests</h3>
<p>Suppose we are interested in testing the hypotheses <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs.&nbsp;<span class="math inline">\(H_1: \theta \neq \theta_0\)</span>. The Wald test is the hypothesis test that uses the Wald test statistic <span class="math inline">\(\lambda_{W}\)</span>, where</p>
<p><span class="math display">\[
\lambda_W = \left( \frac{\hat{\theta}_{MLE} - \theta_0}{se(\hat{\theta}_{MLE})}\right)^2.
\]</span></p>
<p>Intuitively, the Wald test measures the difference between the estimated value for <span class="math inline">\(\theta\)</span> and the null value for <span class="math inline">\(\theta\)</span>, standardized by the variation of your estimator. If this reminds you (once again) of a z-score, it should! In linear regression, with normally distributed standard errors, it turns out that <span class="math inline">\(\sqrt{W}\)</span> follows a <span class="math inline">\(t\)</span> distribution (we’ll show this on your problem set!).</p>
<p>Wald tests statistics are extremely straightforward to compute from the Central Limit Theorem. The CLT states that, for iid <span class="math inline">\(X_1, \dots, X_n\)</span> with expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span>,</p>
<p><span class="math display">\[
\sqrt{n} (\overline{X} - \mu) \overset{d}{\to} N(0, \sigma^2).
\]</span></p>
<p>Slutsky’s theorem allows us to write</p>
<p><span class="math display">\[
\left( \frac{\overline{X} - \mu}{\sigma / \sqrt{n}}\right) \overset{d}{\to} N(0,1),
\]</span></p>
<p>and note that the left-hand side is an estimator minus it’s expectation, divided by it’s standard error. When <span class="math inline">\(\overline{X}\)</span> is the MLE for <span class="math inline">\(\mu\)</span>, this is the square root of the Wald test statistic! The final thing to note is that the right-hand side tells us this quantity converges in distribution to a standard normal distribution. Think about what we’ve previously shown about standard normals “squared” to intuit the asymptotic distribution of a Wald test statistic: a <span class="math inline">\(\chi^2_\nu\)</span> random variable, where the degrees of freedom <span class="math inline">\(\nu\)</span> in this case is one! For a single parameter restriction (i.e.&nbsp;one hypothesis for one unknown parameter), the asymptotic distribution of a Wald test statistic will always be <span class="math inline">\(\chi^2_1\)</span>.</p>
<section id="wald-tests-for-multiple-hypotheses" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="wald-tests-for-multiple-hypotheses">Wald Tests for Multiple Hypotheses</h4>
<p>Note that there is also a multivariate version of the Wald test, used to jointly test <em>multiple</em> hypotheses on multiple parameters. In this case, we can write our null and alternative hypotheses using matrices and vectors.</p>
<p>As a simple example, consider a linear regression model where we have a single, categorical predictor with three categories. Our regression model looks something like this:</p>
<p><span class="math display">\[
E[Y \mid X] = \beta_0 + \beta_1 X_{Cat2} + \beta_2 X_{Cat3}
\]</span></p>
<p>If we want to test if there is a significant association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, we can’t look at <span class="math inline">\(\hat{\beta_1}\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> separately. Rather, we need to test the joint null hypothesis <span class="math inline">\(\beta_1 = \beta_2 = 0\)</span>, vs.&nbsp;the alternative where <em>at least one</em> of our coefficients is equal to zero. In introductory statistics, we did this using the <code>anova</code> function in R. In matrix form, we can write our null and alternative hypotheses as:</p>
<ul>
<li><p><span class="math inline">\(H_0: R \boldsymbol{\beta} = \textbf{r}\)</span></p></li>
<li><p><span class="math inline">\(H_1: R \boldsymbol{\beta} \neq \textbf{r}\)</span></p></li>
</ul>
<p>where <span class="math inline">\(R\)</span> in this case is the identity matrix, <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \beta_1)^\top\)</span>, and <span class="math inline">\(\textbf{r} = (0,0)^\top\)</span>. The Wald test statistic in this multi-hypothesis, multi-parameter case can then be written as</p>
<p><span class="math display">\[
(R\hat{\boldsymbol{\theta}} - \textbf{r})^\top [R (\hat{V}/n) R^\top]^{-1} (R\hat{\boldsymbol{\theta}} - \textbf{r})
\]</span></p>
<p>where <span class="math inline">\(\hat{V}\)</span> is an estimator of the covariance matrix for <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>. We won’t focus on multi-hypothesis, multi-parameter tests in this course, but I <em>do</em> want you to be able to draw connections between statistical theory and things you learned way back in your introductory statistics course, hence why this is included in the notes.</p>
</section>
</section>
<section id="likelihood-ratio-tests" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="likelihood-ratio-tests">Likelihood Ratio Tests</h3>
<p>Suppose we are interested in testing the hypotheses <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs.&nbsp;<span class="math inline">\(H_1: \theta \neq \theta_0\)</span>. The likelihood ratio test is the hypothesis test that uses the likelihood ratio test statistic <span class="math inline">\(\lambda_{LRT}\)</span>, where</p>
<p><span class="math display">\[
\lambda_{LRT} = -2 \log\left(\frac{\underset{\theta = \theta_0}{\text{sup}} \hspace{1mm} L(\theta)}{\underset{\theta \in \Theta}{\text{sup}} \hspace{1mm} L(\theta)} \right).
\]</span></p>
<p>Since the ratio of the likelihoods is bounded between 0 and 1 (since the denominator will always be at least as large as the numerator), the LRT statistic is always positive. When <span class="math inline">\(\lambda_{LRT}\)</span> is large, it suggests that the data are not compatible with <span class="math inline">\(H_0\)</span>, and values of <span class="math inline">\(\lambda_{LRT}\)</span> close to <span class="math inline">\(0\)</span> suggest the data <em>are</em> compatible with <span class="math inline">\(H_0\)</span>. Therefore, we’ll reject <span class="math inline">\(H_0\)</span> for large values of <span class="math inline">\(\lambda_{LRT}\)</span> and fail to reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\lambda_{LRT}\)</span> is small. The likelihood ratio test is the most “powerful” of all level <span class="math inline">\(\alpha\)</span> tests when we have a simple alternative hypothesis, and we can prove this using the Neyman-Pearson Lemma. For the simple null hypothesis on a single parameter that we consider, it can be shown that <span class="math inline">\(\lambda_{LRT} \overset{d}{\to} \chi^2_1\)</span>, just as with the Wald test statistic.</p>
</section>
<section id="score-tests" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="score-tests">Score Tests</h3>
<p>Suppose we are interested in testing the hypotheses <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs.&nbsp;<span class="math inline">\(H_1: \theta \neq \theta_0\)</span>. The score test is the hypothesis test that uses the score test statistic <span class="math inline">\(\lambda_S\)</span>,</p>
<p><span class="math display">\[
\lambda_S = \frac{\left( \frac{\partial}{\partial \theta_0} \log L(\theta_0 \mid x) \right)^2}{I(\theta_0)}
\]</span></p>
<p>as its test statistic. Note that the score test statistic depends <em>only</em> on the distribution of the estimator under the null hypothesis, rather than the maximum likelihood estimator. This is sometimes referred to as a test that requires only computation of a <em>restricted</em> estimator (where <span class="math inline">\(\theta_0\)</span> is “restricted” by the null distribution). The score test statistic is particularly useful when the MLE is on the boundary of the parameter space (think: order statistics).</p>
<p>Intuitively, if <span class="math inline">\(\theta_0\)</span> is near the estimator that maximizes the log likelihood function, the derivative of the log likelihood function should be close to <span class="math inline">\(0\)</span>. The score statistic “standardizes” this derivative by a measure of the variation of the estimator, contained in the information matrix. Values of <span class="math inline">\(\lambda_S\)</span> that are closer to zero are then more compatible with <span class="math inline">\(H_0\)</span>, since because it suggests <span class="math inline">\(\theta_0\)</span> is close to the estimator that maximizes the log likelihood function. We’ll reject <span class="math inline">\(H_0\)</span> for large values of <span class="math inline">\(\lambda_S\)</span>. For the simple null hypothesis on a single parameter that we consider, it can be shown that <span class="math inline">\(\lambda_{S} \overset{d}{\to} \chi^2_1\)</span>, just as with the Wald test statistic and LRT statistic.</p>
</section>
<section id="learning-objectives" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">7.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you should be able to…</p>
<ul>
<li>Derive and implement a hypothesis test using each of the three classical test statistics to distinguish between two conflicting hypotheses</li>
<li>Describe the differences and relationships between Type I Error, Type II Error, and power, as well as the factors that influence each of them</li>
<li>Calculate the power or Type II error for a given hypothesis test</li>
</ul>
</section>
<section id="reading-guide" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="reading-guide"><span class="header-section-number">7.2</span> Reading Guide</h2>
<p>Associated Readings: Chapter 6, Sections 6.1-6.5*</p>
<p>*Note: I (Taylor) don’t love the way that the textbook discusses hypothesis testing. This reading will not be required, but is included in the course notes as a reference if you’d like to read about hypothesis testing from a different perspective.</p>
<section id="reading-questions" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="reading-questions"><span class="header-section-number">7.2.1</span> Reading Questions</h3>
<ol type="1">
<li>What is the goal of hypothesis testing?</li>
<li>What are the typical steps to deriving a hypothesis test?</li>
<li>What is the difference between a one-sided and a two-sided alternative hypothesis? How does this impact our hypothesis testing procedure? How does this impact our p-value?</li>
<li>How are test statistics and p-values related?</li>
<li>How is type I error related to the choice of significance level?</li>
<li>What are the typical steps to calculating the probability of a type II error?</li>
<li>How is type II error related to the power of a hypothesis test?</li>
<li>What factors influence the power of a test? In practice, which of these factors can we control?</li>
</ol>
</section>
</section>
<section id="definitions" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">7.3</span> Definitions</h2>
<p><strong>Wald Test Statistic</strong></p>
<p>The Wald test statistic <span class="math inline">\(\lambda_W\)</span> for testing the hypothesis <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs.&nbsp;<span class="math inline">\(H_1: \theta \neq \theta_0\)</span> is given by</p>
<p><span class="math display">\[
\lambda_W = \left(\frac{\hat{\theta}_{MLE} - \theta_0}{se(\hat{\theta}_{MLE})}\right)^2,
\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is a maximum likelihood estimator.</p>
<p><strong>Likelihood Ratio Test (LRT) Statistic</strong></p>
<p>The likelihood ratio test statistic <span class="math inline">\(\lambda_{LRT}\)</span> for testing the hypothesis <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs.&nbsp;<span class="math inline">\(H_1: \theta \neq \theta_0\)</span> is given by</p>
<p><span class="math display">\[
\lambda_{LRT} = -2 \log\left(\frac{\underset{\theta = \theta_0}{\text{sup}} \hspace{1mm} L(\theta)}{\underset{\theta \in \Theta}{\text{sup}} \hspace{1mm} L(\theta)}\right),
\]</span></p>
<p>where we note that the denominator, <span class="math inline">\(\underset{\theta \in \Theta}{\text{sup}} \hspace{1mm} L(\theta)\)</span>, is the likelihood evaluated at the maximum likelihood estimator.</p>
<p><strong>Score Test Statistic</strong></p>
<p>The score test statistic <span class="math inline">\(\lambda_S\)</span> for testing the hypothesis <span class="math inline">\(H_0: \theta = \theta_0\)</span> vs.&nbsp;<span class="math inline">\(H_1: \theta \neq \theta_0\)</span> is given by</p>
<p><span class="math display">\[
\lambda_S = \frac{\left( \frac{\partial}{\partial \theta_0} \log L(\theta_0 \mid x) \right)^2}{I(\theta_0)}.
\]</span></p>
<p><strong>Power</strong></p>
<p>Power is the probability that we <em>correctly</em> reject the null hypothesis; aka, the probability that we reject the null hypothesis, when the null hypothesis is actually false. As a conditional probability statement: <span class="math inline">\(\Pr(\text{Reject }H_0 \mid H_0 \text{ False})\)</span>. Note that</p>
<p><span class="math display">\[
\text{Power} = 1 - \text{Type II Error}
\]</span></p>
<p><strong>Type I Error (“False positive”)</strong></p>
<p>Type I Error is the probability that the null hypothesis is rejected, when the null hypothesis is actually true. As a conditional probability statement: <span class="math inline">\(\Pr(\text{Reject }H_0 \mid H_0 \text{ True})\)</span></p>
<p><strong>Type II Error (“False negative”)</strong></p>
<p>Type II Error is the probability that we fail to reject the null hypothesis, given that the null hypothesis is actually false. As a conditional probability statement: <span class="math inline">\(\Pr(\text{Fail to reject }H_0 \mid H_0 \text{ False})\)</span></p>
<p><strong>Critical Region / Rejection Region</strong></p>
<p>The critical/rejection region is defined as the set of values for which the null hypothesis would be rejected. This set is often denoted with a capital <span class="math inline">\(R\)</span>.</p>
<p><strong>Critical Value</strong></p>
<p>The critical value is the point that separates the rejection region from the “acceptance” region (i.e., the value at which the decision for your hypothesis test would change). Acceptance is in quotes because we should never “accept” the null hypothesis… but we still call the “fail-to-reject” region the acceptance region for short.</p>
<p><strong>Significance Level</strong></p>
<p>The significance level, denoted <span class="math inline">\(\alpha\)</span>, is the probability that, under the null hypothesis, the test statistic lies in the critical/rejection region.</p>
<p><strong>P-value</strong></p>
<p>The p-value associated with a test statistic is the probability of obtaining a value <em>as or more extreme</em> than the observed test statistic, under the null hypothesis.</p>
<p><strong>Uniformly Most Powerful (UMP) Test</strong></p>
<p>A “most powerful” test is a hypothesis test that has the <em>greatest</em> power among all possible tests of a given significance threshold <span class="math inline">\(\alpha\)</span>. A <em>uniformly</em> most powerful (UMP) test is a test that is most powerful for all possible values of parameters in the restricted parameter space, <span class="math inline">\(\Theta_0\)</span>.</p>
<p>More formally, let the set <span class="math inline">\(R\)</span> denote the rejection region of a hypothesis test. Let</p>
<p><span class="math display">\[
\phi(x) = \begin{cases} 1 &amp; \quad \text{if } x \in R \\ 0 &amp; \quad \text{if } x \in R^c \end{cases}
\]</span></p>
<p>Then <span class="math inline">\(\phi(x)\)</span> is an indicator function. Recalling that expectations of indicator functions are probabilities, note that <span class="math inline">\(E[\phi(x)] = \Pr(\text{Reject } H_0)\)</span>. <span class="math inline">\(\phi(x)\)</span> then represents our hypothesis test. A hypothesis test <span class="math inline">\(\phi(x)\)</span> is UMP of size <span class="math inline">\(\alpha\)</span> if, for any other hypothesis test <span class="math inline">\(\phi'(x)\)</span> of size <span class="math inline">\(\alpha\)</span> we have,</p>
<p><span class="math display">\[
\underset{\theta \in \Theta_0}{\text{sup}} E[\phi'(X) \mid \theta] \leq \underset{\theta \in \Theta_0}{\text{sup}} E[\phi(X) \mid \theta]
\]</span></p>
<p>and <span class="math inline">\(\forall \theta \in \Theta_1\)</span>,</p>
<p><span class="math display">\[
E[\phi'(X) \mid \theta] \leq E[\phi(X) \mid \theta],
\]</span></p>
<p>where <span class="math inline">\(\Theta_0\)</span> is the set of all values for <span class="math inline">\(\theta\)</span> that align with the null hypothesis (sometimes just a single point, sometimes a region), and <span class="math inline">\(\Theta_1\)</span> is the set of all values for <span class="math inline">\(\theta\)</span> that align with the alternative hypothesis (sometimes just a single point, sometimes a region). <strong>Note:</strong> In general, UMP tests <em>do not exist</em> for two-sided alternative hypotheses. The Neyman-Pearson lemma tells us about UMP tests for simple null and alternative hypotheses, and the <a href="https://en.wikipedia.org/wiki/Uniformly_most_powerful_test">Karlin-Rubin theorem</a> extends this to one-sided null and alternative hypotheses.</p>
</section>
<section id="theorems" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="theorems"><span class="header-section-number">7.4</span> Theorems</h2>
<p><strong>Neyman-Pearson Lemma</strong></p>
<p>Consider a hypothesis test with <span class="math inline">\(H_0: \theta = \theta_0\)</span> and <span class="math inline">\(H_1: \theta = \theta_1\)</span>. Let <span class="math inline">\(\phi\)</span> be a <em>likelihood ratio test</em> of level <span class="math inline">\(\alpha\)</span>, where <span class="math inline">\(\alpha = E[\phi(X) \mid \theta_0]\)</span>. Then <span class="math inline">\(\phi\)</span> is a UMP level <span class="math inline">\(\alpha\)</span> test for thee hypotheses <span class="math inline">\(H_0: \theta = \theta_0\)</span> and <span class="math inline">\(H_1: \theta = \theta_1\)</span>.</p>
<p><strong>Proof.</strong></p>
<p>Let <span class="math inline">\(\alpha = E[\phi(X) \mid \theta_0]\)</span>. Note that the LRT statistic is simplified in the case of these simple hypotheses, and can be written just as <span class="math inline">\(\frac{f(x \mid \theta_1)}{f(x \mid \theta_0)}\)</span>. If the likelihood under the alternative is greater than some constant <span class="math inline">\(c\)</span> (which depends on <span class="math inline">\(\alpha\)</span>), then we reject the null in favor of the alternative, and vice versa. Then the hypothesis testing function <span class="math inline">\(\phi\)</span> can be written as</p>
<p><span class="math display">\[
\phi(x) = \begin{cases} 0 &amp; \quad \text{if } \lambda_{LRT} = \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} &lt; c\\
1 &amp; \quad \text{if } \lambda_{LRT} = \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} &gt; c\\
\text{Flip a coin} &amp; \quad \text{if } \lambda_{LRT} = \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} = c
\end{cases}
\]</span>Suppose <span class="math inline">\(\phi'\)</span> is any other test such that <span class="math inline">\(E[\phi'(X) \mid \theta_0] \leq \alpha\)</span> (another level <span class="math inline">\(\alpha\)</span> test). Then we must show that <span class="math inline">\(E[\phi'(X) \mid \theta_1] \leq E[\phi(X) \mid \theta_1]\)</span>.</p>
<p>By assumption, we have</p>
<span class="math display">\[\begin{align*}
    E[\phi(X) \mid \theta_0] &amp;= \int \phi(x) f_X(x \mid \theta_0) dx = \alpha \\
    E[\phi'(X) \mid \theta_0] &amp;= \int \phi'(x) f_X(x \mid \theta_0) dx \leq \alpha
\end{align*}\]</span>
<p>Therefore we can write</p>
<span class="math display">\[\begin{align*}
    E[\phi(X) &amp; \mid \theta_1] - E[\phi'(X) \mid \theta_1] \\
    &amp; = \int \phi(x) f_X(x \mid \theta_1) dx - \int \phi'(x) f_X(x \mid \theta_1) dx \\
    &amp; = \int [\phi(x) - \phi'(x)] f_X(x \mid \theta_1) dx \\
    &amp; = \int_{\left\{ \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} &gt; c \right\}} \underbrace{[\phi(x) - \phi'(x)]}_{\geq 0} f_X(x \mid \theta_1) dx + \int_{\left\{ \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} &lt; c \right\}} \underbrace{[\phi(x) - \phi'(x)]}_{\leq 0} f_X(x \mid \theta_1) dx + \int_{\left\{ \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} = c \right\}} [\phi(x) - \phi'(x)] f_X(x \mid \theta_1) dx \\
    &amp; \geq \int_{\left\{ \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} &gt; c \right\}} [\phi(x) - \phi'(x)] cf_X(x \mid \theta_0) dx + \int_{\left\{ \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} &lt; c \right\}} [\phi(x) - \phi'(x)] cf_X(x \mid \theta_0) dx + \int_{\left\{ \frac{f(x \mid \theta_1)}{f(x \mid \theta_0)} = c \right\}} [\phi(x) - \phi'(x)] cf_X(x \mid \theta_0) dx \\
    &amp; = c \int [\phi(x) - \phi'(x)] f_X(x \mid \theta_0) dx \\
    &amp; = c \int \phi(x) f_X(x \mid \theta_0) dx - c \int \phi'(x) f_X(x \mid \theta_0) dx \\
    &amp; \geq c(\alpha - \alpha) \\
    &amp; = 0
\end{align*}\]</span>
<p>And rearranging yields</p>
<span class="math display">\[\begin{align*}
E[\phi(X)  \mid \theta_1] - E[\phi'(X) \mid \theta_1] &amp; \geq 0 \\
E[\phi(X) \mid \theta_1] &amp; \geq E[\phi'(X) \mid \theta_1]
\end{align*}\]</span>
<p>as desired.</p>
</section>
<section id="worked-examples" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="worked-examples"><span class="header-section-number">7.5</span> Worked Examples</h2>
<p><strong>Problem 1:</strong> Let <span class="math inline">\(Y_i \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\sigma^2 = 25\)</span> is <em>known</em>. Suppose we want to test the hypotheses <span class="math inline">\(H_0: \mu = 8\)</span> vs.&nbsp;<span class="math inline">\(H_1: \mu &gt; 8\)</span> and we observe <span class="math inline">\(\overline{Y} = 10\)</span> across <span class="math inline">\(n = 64\)</span> observations. Can we reject <span class="math inline">\(H_0\)</span>, with a significance threshold of <span class="math inline">\(\alpha = 0.05\)</span>? (Use a Wald test statistic)</p>
<p>Our hypotheses are already stated in the problem set-up. The next thing we should do is derive a Wald test statistic. We know that the MLE for <span class="math inline">\(\mu\)</span> is given by <span class="math inline">\(\hat{\mu}_{MLE} = \overline{Y}\)</span> (we have shown this is previous problem sets/worked examples). Then the Wald test statistic can be written as</p>
<p><span class="math display">\[
\lambda_W = \left( \frac{\hat{\mu}_{MLE} - \mu_0}{\sigma^2/\sqrt{n}}\right)^2 = \left( \frac{10 - 8}{25/\sqrt{64}}\right)^2 = 6.5536
\]</span></p>
<p>We can compare this test statistic to the critical value from a <span class="math inline">\(\chi^2_1\)</span> distribution since, by properties of normal distributions and recalling that standard normal distributions squared are <span class="math inline">\(\chi^2_1\)</span>,</p>
<span class="math display">\[\begin{align*}
    \overline{Y} &amp; \sim N(\mu, \sigma^2/n) \\
    \frac{\overline{Y} - \mu}{\sigma/\sqrt{n}} &amp; \sim N(0,1) \\
    \left( \frac{\overline{Y} - \mu}{\sigma/\sqrt{n}}  \right)^2 &amp; \sim \chi^2_1
\end{align*}\]</span>
<p>To calculate the critical value when <span class="math inline">\(\alpha = 0.05\)</span>, we turn to R.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The quantile function for a given distribution gives us the value at which</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a given percentage of the distributions lies ahead of that value, which</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># is exactly what we want in this case!</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.05</span>, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.841459</code></pre>
</div>
</div>
<p>Finally, noting that our test statistic is greater than the critical value, we reject <span class="math inline">\(H_0\)</span>.</p>
<p><strong>Problem 2:</strong> Suppose we wanted to use a different significance level <span class="math inline">\(\alpha\)</span>. How would the procedure in Problem 1 change if we let <span class="math inline">\(\alpha = 0.01\)</span>? How would the procedure in Problem 1 change if we let <span class="math inline">\(\alpha = 0.1\)</span>?</p>
<p>Changing the significance level changes the <em>critical value</em>, and may change whether or not we reject <span class="math inline">\(H_0\)</span>, depending on the difference between our critical value and the test statistic. We can calculate what the critical value would be if we let <span class="math inline">\(\alpha = 0.01\)</span> and <span class="math inline">\(\alpha = 0.1\)</span> again in R:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># alpha = 0.01</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.01</span>, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 6.634897</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># alpha = 0.1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.1</span>, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.705543</code></pre>
</div>
</div>
<p>Note that when <span class="math inline">\(\alpha = 0.1\)</span>, we still reject <span class="math inline">\(H_0\)</span>. This should make intuitive sense, since increasing <span class="math inline">\(\alpha\)</span> only can only increase our rejection region. However, when <span class="math inline">\(\alpha = 0.01\)</span>, we would <em>fail to reject</em> <span class="math inline">\(H_0\)</span>, as our test statistic is not “more extreme” (greater) than the critical value.</p>
<p><strong>Problem 3:</strong> Suppose we have a random sample <span class="math inline">\(X_1, \dots, X_n \sim Bernoulli(p)\)</span>, and we want to test the hypotheses <span class="math inline">\(H_0:p = 0.5\)</span>, <span class="math inline">\(H_1:p \neq 0.5\)</span>. Suppose we calculate an estimator for <span class="math inline">\(p\)</span> as <span class="math inline">\(\hat{p} = \frac{1}{n} \sum_{i = 1}^n X_i\)</span>. Derive a Wald test statistic for this hypothesis testing scenario (simplifying as much as you can).</p>
<p>Recall that <span class="math inline">\(\hat{p}\)</span> as defined in the problem set-up is the MLE for <span class="math inline">\(p\)</span>. Then the Wald test statistic can be written as</p>
<p><span class="math display">\[
\lambda_W = \left( \frac{\hat{p} - p_0}{se(\hat{p})}\right)^2.
\]</span></p>
<p>We can simplify a little further by calculating <span class="math inline">\(se(\hat{p})\)</span> and plugging in <span class="math inline">\(p_0\)</span>. Recall from the CLT (and Slutsky) that we have</p>
<p><span class="math display">\[
\left( \frac{\hat{p} - p_0}{\sqrt{\hat{p}(1 - \hat{p})/n}} \right) \overset{d}{\to} N(0,1)
\]</span></p>
<p>Then the standard error of <span class="math inline">\(\hat{p}\)</span> is given by <span class="math inline">\(\sqrt{\hat{p}(1 - \hat{p})/n}\)</span>, and our Walt test statistic simplifies to</p>
<p><span class="math display">\[
\lambda_W = \left( \frac{\hat{p} - 0.5}{\sqrt{\hat{p}(1 - \hat{p})/n}}\right)^2.
\]</span></p>
<p>(Note that this is as “simplified” as we can get without knowing <span class="math inline">\(\hat{p}\)</span> or <span class="math inline">\(n\)</span>)</p>
<p><strong>Problem 4:</strong> Derive a LRT statistic for the hypothesis testing scenario described in Problem 3 (simplifying as much as you can).</p>
<p>The LRT statistic is given by</p>
<p><span class="math display">\[
\lambda_{LRT} = -2 \log\left(\frac{\underset{p = p_0}{\text{sup}} \hspace{1mm} L(p)}{\underset{p \in \Theta}{\text{sup}} \hspace{1mm} L(p)} \right)= -2 \log\left(\frac{ L(0.5)} {L(\hat{p}_{MLE})} \right)
\]</span></p>
<p>The likelihood for our observations can be written as</p>
<p><span class="math display">\[
L(p) = \prod_{i = 1}^n p^{x_i} (1 - p)^{(1 - x_i)}
\]</span></p>
<p>And so our LRT statistic simplifies to</p>
<span class="math display">\[\begin{align*}
\lambda_{LRT} &amp; = -2 \log\left(\frac{ L(0.5)} {L(\hat{p})} \right) \\
&amp; = -2 \left[\log L(0.5) - \log L(\hat{p}) \right] \\
&amp; = -2 \left[ \log(0.5) \sum_{i = 1}^n X_i + \log(1 - 0.5)\sum_{i = 1}^n(1 - X_i) - \log(\hat{p}) \sum_{i = 1}^n X_i - \log(1 - \hat{p})\sum_{i = 1}^n(1 - X_i)\right] \\
&amp; = -2 \left[ \log(0.5) \left( \sum_{i = 1}^n X_i + \sum_{i = 1}^n(1 - X_i)\right) - \log(\hat{p}) \sum_{i = 1}^n X_i - \log(1 - \hat{p})\sum_{i = 1}^n(1 - X_i)\right] \\
&amp; = -2 \left[ n\log(0.5) - \log(\hat{p}) \sum_{i = 1}^n X_i - \log(1 - \hat{p})\sum_{i = 1}^n(1 - X_i)\right] \\
&amp; = -2 \left[ n\log(0.5) - \log(\hat{p}) n \overline{X} - \log(1 - \hat{p}) (n - n \overline{X})\right] \\
&amp; = -2 n \left[ \log(0.5) - \log(\hat{p}) \hat{p} - \log(1 - \hat{p}) (1 -  \hat{p})\right]
\end{align*}\]</span>
<p>(Note that this is as “simplified” as we can get without knowing <span class="math inline">\(\hat{p}\)</span> or <span class="math inline">\(n\)</span>)</p>
<p><strong>Problem 5:</strong> Derive a score test statistic for the hypothesis testing scenario described in Problem 3 (simplifying as much as you can).</p>
<p>The score test statistic is given by</p>
<p><span class="math display">\[
\lambda_S = \frac{\left( \frac{\partial}{\partial p_0} \log L(p_0 \mid x) \right)^2}{I(p_0)}.
\]</span></p>
<p>We can simplify by deriving the score and information matrix, and then plugging in <span class="math inline">\(p_0 = 0.5\)</span>. We have,</p>
<span class="math display">\[\begin{align*}
\frac{\partial}{\partial p_0} \log L(p_0 \mid x) &amp; = \frac{\partial}{\partial p_0} \left[ \log(p_0) \sum_{i = 1}^n X_i + \log(1 - p_0) \sum_{i = 1}^n (1 - X_i) \right] \\
&amp; = \frac{\sum_{i = 1}^n X_i}{p_0} - \frac{n - \sum_{i = 1}^n  X_i}{1 - p_0} \\
\left( \frac{\partial}{\partial p_0} \log L(p_0 \mid x) \right)^2 &amp; = \left(\frac{\sum_{i = 1}^n X_i}{p_0} - \frac{n - \sum_{i = 1}^n  X_i}{1 - p_0} \right)^2
\end{align*}\]</span>
<p>and plugging in <span class="math inline">\(p_0 = 0.5\)</span>, we have,</p>
<p><span class="math display">\[
\left( \frac{\partial}{\partial p_0} \log L(p_0 \mid x)\right)^2 = \left(\frac{\sum_{i = 1}^n X_i}{0.5} - \frac{n - \sum_{i = 1}^n  X_i}{1 - 0.5} \right)^2 = \left( \frac{-n + 2\sum_{i = 1}^n X_i}{0.5}\right)^2 = \left( -2n + 4\sum_{i = 1}^n X_i\right)^2.
\]</span></p>
<p>The information matrix is given by <span class="math inline">\(-E\left[ \frac{\partial^2}{\partial p_0^2} \log L(p_0 \mid x)\right]\)</span>. Piecing this together,</p>
<span class="math display">\[\begin{align*}
\frac{\partial^2}{\partial p_0^2} \log L(p_0 \mid x)
&amp; =  \frac{\partial^2}{\partial p_0^2} \left[ \frac{\sum_{i = 1}^n X_i}{p_0} - \frac{n - \sum_{i = 1}^n  X_i}{1 - p_0} \right] \\
&amp; = \frac{-\sum_{i = 1}^n X_i}{p_0^2} - \frac{n - \sum_{i = 1}^n  X_i}{(1 - p_0)^2}
\end{align*}\]</span>
<p>And to get <span class="math inline">\(I(p_0)\)</span>, we take the negative expectation of the above quantity under the null hypothesis (that is, where <span class="math inline">\(E[X] = p_0\)</span>) to obtain</p>
<span class="math display">\[\begin{align*}
    I(p_0) &amp; = -E \left[ \frac{-\sum_{i = 1}^n X_i}{p_0^2} - \frac{n - \sum_{i = 1}^n  X_i}{(1 - p_0)^2} \right] \\
    &amp; = \frac{1}{p_0^2} \sum_{i = 1}^n E[X_i] + \frac{1}{(1 - p_0)^2} \left( n - \sum_{i = 1}^n E[X_i]\right) \\
    &amp; = \frac{1}{p_0^2} \sum_{i = 1}^n p_0 + \frac{1}{(1 - p_0)^2} \left( n - \sum_{i = 1}^n p_0\right) \\
    &amp; = \frac{n}{p_0}  + \frac{n}{(1 - p_0)^2} \left( 1 - p_0\right) \\
    &amp; = \frac{n}{p_0}  + \frac{n}{(1 - p_0)}
\end{align*}\]</span>
<p>And plugging in <span class="math inline">\(p_0 = 0.5\)</span> we have</p>
<p><span class="math display">\[
I(0.5) = \frac{n}{0.5}  + \frac{n}{(1 - 0.5)} = 2n + 2n = 4n.
\]</span></p>
<p>Then, finally, the score test statistic (simplified as much as possible) is given by</p>
<span class="math display">\[\begin{align*}
    \lambda_S &amp; = \frac{\left( \frac{\partial}{\partial p_0} \log L(p_0 \mid x) \right)^2}{I(p_0)} \\
    &amp; = \frac{\left( -2n + 4\sum_{i = 1}^n X_i\right)^2}{4n} \\
    &amp; = \frac{\left( -2n + 4n \hat{p}\right)^2}{4n} \\
    &amp; = \frac{4n^2\left( -1 + 2 \hat{p}\right)^2}{4n} \\
    &amp; = n\left( -1 + 2 \hat{p}\right)^2
\end{align*}\]</span>
<p><strong>Problem 6:</strong> For each of Problems 3, 4, and 5, calculate the p-values from each test when <span class="math inline">\(\hat{p} = 0.4\)</span> and <span class="math inline">\(n = 300\)</span>.</p>
<p>We’ll again use R to obtain the critical values for these hypothesis tests, noting that in each case, the test statistic follows a <span class="math inline">\(\chi^2_1\)</span> distribution asymptotically:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test statistic</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>lambda_w <span class="ot">&lt;-</span> ((p_hat <span class="sc">-</span> <span class="fl">0.5</span>)<span class="sc">/</span>(<span class="fu">sqrt</span>(p_hat <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p_hat) <span class="sc">/</span> n)))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># LRT statistic</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>lambda_lrt <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> n <span class="sc">*</span> (<span class="fu">log</span>(<span class="fl">0.5</span>) <span class="sc">-</span> <span class="fu">log</span>(p_hat) <span class="sc">*</span> p_hat <span class="sc">-</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> p_hat) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p_hat))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Score test statistic</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>lambda_s <span class="ot">&lt;-</span> n <span class="sc">*</span> (<span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> p_hat)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare statistics</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>lambda_w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 12.5</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>lambda_lrt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 12.08131</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>lambda_s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 12</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate p-values</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Recall: probability that we observe something *as or more extreme*</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(lambda_w, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.000406952</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(lambda_lrt, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0005092985</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(lambda_s, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0005320055</code></pre>
</div>
</div>
<p>Things to note:</p>
<ul>
<li><p>When <span class="math inline">\(n\)</span> is large (300, in this case), each of the three classical test statistics are approximately equal! This makes sense, as they all converge in distribution to the same random variable, asymptotically.</p></li>
<li><p>P-values are the probability that we would observe something <em>as or more extreme</em> than what we actually did observe, under the null hypothesis. In R, we can use the <code>p</code> function (for a given pdf) to calculate this.</p></li>
</ul>
<p><strong>Problem 7:</strong> Repeat Problem 6 but with <span class="math inline">\(\hat{p} = 0.4\)</span> and <span class="math inline">\(n = 95\)</span>. If your significance threshold were <span class="math inline">\(\alpha = 0.05\)</span>, would your conclusion to the hypothesis test be the same regardless of which test statistic you chose?</p>
<p>To answer this question, we can again calculate p-values, and compare them to 0.05 (note that we could have also calculated a critical value, and compared our test statistics to the critical value, as these are equivalent).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">95</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald test statistic</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>lambda_w <span class="ot">&lt;-</span> ((p_hat <span class="sc">-</span> <span class="fl">0.5</span>)<span class="sc">/</span>(<span class="fu">sqrt</span>(p_hat <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p_hat) <span class="sc">/</span> n)))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># LRT statistic</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>lambda_lrt <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> n <span class="sc">*</span> (<span class="fu">log</span>(<span class="fl">0.5</span>) <span class="sc">-</span> <span class="fu">log</span>(p_hat) <span class="sc">*</span> p_hat <span class="sc">-</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> p_hat) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> p_hat))</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Score test statistic</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>lambda_s <span class="ot">&lt;-</span> n <span class="sc">*</span> (<span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> p_hat)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare statistics</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>lambda_w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.958333</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>lambda_lrt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.825748</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>lambda_s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate p-values</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Recall: probability that we observe something *as or more extreme*</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(lambda_w, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.04663986</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(lambda_lrt, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05047083</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(lambda_s, <span class="at">df =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05125258</code></pre>
</div>
</div>
<p>In this case, we would reject <span class="math inline">\(H_0\)</span> using the Wald test statistic, but <em>fail to reject</em> using the LRT statistic and score test statistic, since the only p-value that was below our significance threshold was the one calculated from the Wald test statistic. Finite-sample distributions of the three classical test statistics are generally unknown; only asymptotically have they been shown to be equivalent, and therefore, can provide <em>different</em> answers to hypothesis tests when sample sizes are relatively small.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./asymptotics.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bayes.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>