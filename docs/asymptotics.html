<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Asymptotics &amp; the Central Limit Theorem – MATH/STAT 355: Statistical Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./hypothesis.html" rel="next">
<link href="./consistency.html" rel="prev">
<link href="./favicon-32x32.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-12acdecf0bfd8bf8a7d30417284c8ed4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./asymptotics.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH/STAT 355: Statistical Theory</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylorokonek/MathematicalStatistics/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./math-stat-355.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Statistical Theory!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./consistency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Computational Optimization</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">6.1</span> Learning Objectives</a></li>
  <li><a href="#concept-questions" id="toc-concept-questions" class="nav-link" data-scroll-target="#concept-questions"><span class="header-section-number">6.2</span> Concept Questions</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">6.3</span> Definitions</a></li>
  <li><a href="#theorems" id="toc-theorems" class="nav-link" data-scroll-target="#theorems"><span class="header-section-number">6.4</span> Theorems</a></li>
  <li><a href="#worked-examples" id="toc-worked-examples" class="nav-link" data-scroll-target="#worked-examples"><span class="header-section-number">6.5</span> Worked Examples</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Asymptotic unbiasedness and consistency allow us to assess the behavior of estimators when sample sizes get large. Thus far, however, we’ve only discussed what happens to <em>point estimates</em> as <span class="math inline">\(n\)</span> goes to infinity. Point estimates are great, but they don’t tell the whole story. In order to truly quantify uncertainty (which is arguably one of the main goals of statistics, if not <em>the</em> main goal), we need to be able to estimate a range of plausible values for our estimators: we need to be able to construct confidence intervals. This is made possible primarily by asymptotic normality, the Central Limit Theorem, and properties of the normal distribution.</p>
<section id="confidence-intervals" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="confidence-intervals">Confidence Intervals</h3>
<p>Confidence intervals are one of the most difficult concepts for a budding statistician to grasp, because they don’t have the intuitive, probabilistic definition we often want them to have (aka the probability that the truth lies within the interval). As with Frequentist statistics more generally, the definition of a confidence interval relies on the concept of <em>repeated sampling</em> from a population.</p>
<p>A confidence interval either contains the true parameter, or it does not. There is no probability involved in that statement. Probability comes into play when considering that, under repeated sampling, if we construct confidence intervals each time we take a new sample and construct an estimator, a given percentage <em>of those intervals</em> will contain the true parameter.</p>
<section id="confidence-intervals---the-clt" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="confidence-intervals---the-clt">Confidence Intervals - The CLT</h4>
<p>The primary way that we construct confidence intervals is by “rearranging” the CLT so that a quantity’s asymptotic distribution does not depend on the data nor the parameter of interest. This process is sometimes called establishing an <em>approximate pivotal quantity</em>.</p>
<p>As an example, consider an iid random sample <span class="math inline">\(X_1, \dots, X_n\)</span> where <span class="math inline">\(E[X_i] = \mu\)</span> and <span class="math inline">\(Var[X_i] = \sigma^2\)</span>, where <span class="math inline">\(\sigma^2\)</span> is known. The CLT tells us that</p>
<p><span class="math display">\[
\sqrt{n}(\bar{X} - \mu) \overset{d}{\to} N(0, \sigma^2).
\]</span></p>
<p>We can use this to construct a confidence interval for <span class="math inline">\(\mu\)</span>. By Slutsky’s theorem we can write</p>
<p><span class="math display">\[
\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \overset{d}{\to} N(0, 1).
\]</span></p>
<p>and note now that we <em>know</em> the asymptotic distribution for the (pivotal) quantity on the left, and can therefore use the <em>quantiles</em> of this distribution to construct confidence intervals. For a standard normal distribution (as is the case here) we can note that 95% of the distribution is contained within 1.96 standard deviations of the mean, and therefore</p>
<p><span class="math display">\[
\Pr\left(-1.96 \leq \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \leq 1.96\right) = 0.95
\]</span></p>
<p>We can rearrange the probability statement on the left hand side to get</p>
<p><span class="math display">\[
\Pr\left(\bar{X}-1.96 \sigma/\sqrt{n} \leq \mu \leq \bar{X} + 1.96\sigma/\sqrt{n}\right) = 0.95
\]</span></p>
<p>and therefore, our 95% confidence interval for <span class="math inline">\(\mu\)</span> is given by <span class="math inline">\(\left( \bar{X}-1.96 \sigma/\sqrt{n}, \bar{X}+1.96 \sigma/\sqrt{n} \right)\)</span>.</p>
</section>
<section id="confidence-intervals---exact" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="confidence-intervals---exact">Confidence Intervals - “Exact”</h4>
<p>A second way that we construct confidence intervals is through a concrete distributional assumption, and known quantiles of those distributions. Note that the confidence interval constructed above involves the Central Limit Theorem, and <em>no</em> <em>finite sample</em> distributional assumption. All we assume are that the data are iid observations with finite means and variances. The “distribution” only comes into play as our sample size gets large.</p>
<p>When sample sizes <em>aren’t</em> large, applying the CLT might not make a whole lot of sense. In these scenarios, it can be useful to use an alternative confidence interval construction, aided by assuming a specific distribution for our random variables. In some scenarios these assumptions may make more sense than others: in short, we’re <em>always</em> making some sort of assumption, regardless of what we do. It’s part of our job as statisticians to ensure that the assumptions we make, make sense for the application we’re working with!</p>
<p>One example of a commonly used “exact” confidence interval is the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Clopper%E2%80%93Pearson_interval">Clopper-Pearson</a> interval for a binomial proportion. Consider an iid random sample <span class="math inline">\(X_1, \dots, X_n\)</span>, where <span class="math inline">\(\sum_{i = 1}^n X_i \sim Binomial(n, p)\)</span>. Intuitively, the interval is constructed by the following steps:</p>
<ol type="1">
<li><p>Find the <em>largest</em> p such that <span class="math inline">\(\Pr(X \leq k) \geq \alpha/2\)</span>, where <span class="math inline">\(k\)</span> is the observed number of successes. Call this largest value <span class="math inline">\(p_U\)</span>.</p></li>
<li><p>Find the <em>smallest</em> p such that <span class="math inline">\(\Pr(X \geq k) \geq \alpha/2\)</span>, where <span class="math inline">\(k\)</span> is again the observed number of successes. Call this smallest value <span class="math inline">\(p_L\)</span>.</p></li>
<li><p>Define the <span class="math inline">\(100(1 -\alpha)\)</span>% confidence interval for <span class="math inline">\(p\)</span> to be <span class="math inline">\((p_U, p_L)\)</span>.</p></li>
</ol>
<p>This construction process allows us to determine all possible values of <span class="math inline">\(p\)</span> that are compatible with our observed number of successes (which is exactly what a confidence interval should do).</p>
<p>In more math-y terms, We can show that if <span class="math inline">\(\sum_{i = 1}^n X_i \sim Binomial(n, p)\)</span>, then <span class="math inline">\(\Pr(\sum_{i = 1}^n X_i \geq x) = \Pr(Y \leq p)\)</span>, where <span class="math inline">\(Y \sim Beta(\sum_{i = 1}^n x_i, n - \sum_{i = 1}^n x_i + 1)\)</span>. The point of doing this is that we can rewrite our probability statements (involved in our confidence interval construction) in terms of a random variable that <em>does not depend</em> on <span class="math inline">\(p\)</span>. We can then compute the Clopper-Pearson interval for <span class="math inline">\(p\)</span> as</p>
<p><span class="math display">\[
\Phi_{\frac{\alpha}{2}; x, n - x + 1} &lt; p &lt; \Phi_{1 - \frac{\alpha}{2}; x + 1, n - x}
\]</span></p>
<p>where <span class="math inline">\(\Phi_{a; v, w}\)</span> is the <span class="math inline">\(a\)</span>th quantile from a Beta distribution with shape parameters <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span>. Alternatively, you can even write the Clopper-Pearson in terms of quantiles of the <span class="math inline">\(F\)</span>-distribution, but the Beta format is enough to emphasize the main point: If we can determine the distribution of some function of our data and unknown parameter, and manipulate that distribution enough so that it depends on neither the data nor unknown parameter, we can use quantiles and probability statements to construct confidence intervals.</p>
</section>
</section>
<section id="convergence" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="convergence">Convergence</h3>
<p>If an estimator <span class="math inline">\(\hat{\theta}_n\)</span> is a consistent estimator for <span class="math inline">\(\theta\)</span>, we also say that <span class="math inline">\(\hat{\theta}_n\)</span> converges in probability to <span class="math inline">\(\theta\)</span> (i.e., <span class="math inline">\(\hat{\theta}_n \overset{p}{\to} \theta\)</span>). There are three different types of convergence: almost sure convergence, convergence in probability, and convergence in distribution (in order from “strongest” to “weakest”). The main two that we’ll care about for this course are convergence in probability and convergence in distribution. For our purposes, it will <em>mostly</em> suffice to know that (1) convergence in probability <em>implies</em> convergence in distribution, (2) the Central Limit Theorem, delta-method, and Slutsky’s Theorem are tools we can use to determine (and manipulate) asymptotic distributions of random variables, and (3) the Continuous Mapping Theorem (defined below). That being said, an attempt at an intuitive understand of these three types of convergence is provided below, in the context of sequences of random variables.</p>
<section id="convergence-almost-surely" class="level4" data-number="6.0.0.1">
<h4 data-number="6.0.0.1" class="anchored" data-anchor-id="convergence-almost-surely"><span class="header-section-number">6.0.0.1</span> Convergence Almost Surely</h4>
<p>Convergence almost surely is the strongest of the three types of convergence. A sequence of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> convergences almost surely (also called convergence almost everywhere) if</p>
<p><span class="math display">\[
\Pr(\lim_{n \to \infty} X_n = X) = 1
\]</span></p>
<p>In my opinion, one of the most intuitive explanations for convergence almost surely (and convergence in probability, comparatively) is given in the top answer to <a href="https://stats.stackexchange.com/questions/2230/convergence-in-probability-vs-almost-sure-convergence">this</a> StackExchange post. Convergence almost surely says that <span class="math inline">\(X_n\)</span> <em>will</em> be equal to <span class="math inline">\(X\)</span> at some point, for finite <span class="math inline">\(n\)</span>.</p>
</section>
<section id="convergence-in-probability" class="level4" data-number="6.0.0.2">
<h4 data-number="6.0.0.2" class="anchored" data-anchor-id="convergence-in-probability"><span class="header-section-number">6.0.0.2</span> Convergence in Probability</h4>
<p>Surprise! You already know this one, if you made it through the Consistency chapter. The concept of convergence in probability applies to sequences of random variables in addition to estimators. A sequence of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> converges in probability towards the random variable <span class="math inline">\(X\)</span> if for all <span class="math inline">\(\epsilon &gt; 0\)</span></p>
<p><span class="math display">\[
\lim_{n \to \infty} \Pr(|X_n - X | &gt; \epsilon) = 0
\]</span></p>
<p>Note that this equivalent to saying that <span class="math inline">\(\lim_{n \to \infty} \Pr(|X_n - X | \leq \epsilon) = 1\)</span>, which looks a bit more like what we saw last chapter. Convergence in probability says that <span class="math inline">\(X_n\)</span> will equal <span class="math inline">\(X\)</span> <em>with a very high probability</em> as <span class="math inline">\(n\)</span> gets large, though there will always be some small (shrinking with <span class="math inline">\(n\)</span>) chance that they aren’t equal.</p>
</section>
<section id="convergence-in-distribution" class="level4" data-number="6.0.0.3">
<h4 data-number="6.0.0.3" class="anchored" data-anchor-id="convergence-in-distribution"><span class="header-section-number">6.0.0.3</span> Convergence in Distribution</h4>
<p>A sequence of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> with CDFs <span class="math inline">\(F_1, \dots, F_n\)</span> converges in distribution (also called “weak” convergence, or convergence in “law”) if</p>
<p><span class="math display">\[
\lim_{n \to \infty} F_n(x) = F(x)
\]</span>for every number <span class="math inline">\(x\)</span> at which <span class="math inline">\(F\)</span> is continuous. Note that <em>unlike</em> convergence almost surely and convergence in probability, our sequence of variables isn’t “shrinking” in quite the same way, with this type of convergence. Rather, our sequence is better and better approximating an <em>entire distribution</em>, rather than focusing in on a single point. A visual representation of convergence in distribution is provided below, where <span class="math inline">\(F_n(x)\)</span> is the black sequence of lines, and the red line is <span class="math inline">\(F(x)\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="convergence_in_distribution.gif" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
</section>
<section id="asymptotic-properties-of-mles" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="asymptotic-properties-of-mles">Asymptotic Properties of MLEs</h3>
<p>In addition to the nice intuition behind maximum likelihood estimation (finding the parameters that make our data the most likely to have occurred), most* MLEs also have incredibly convenient asymptotic properties, including:</p>
<ul>
<li><p>Asymptotic unbiasedness</p></li>
<li><p>Consistency</p></li>
<li><p>Asymptotic normality</p></li>
<li><p>Asymptotic efficiency</p></li>
</ul>
<p>The definitions of the latter two properties are included below (and the former in the previous chapters).</p>
<p>*the MLEs that do not have all of these properties are the ones that don’t have certain “regularity conditions.” For the MLEs we consider in this class, these are the MLEs that are on the boundary of the support of the pdf (such as the maximum or minimum order statistic).</p>
</section>
<section id="learning-objectives" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">6.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you should be able to…</p>
<ul>
<li><p>Explain the usefulness of the Central Limit Theorem for Frequentist statistical theory</p></li>
<li><p>Manipulate asymptotic distributions to remove their dependence on unknown parameters using the delta-method and Slutsky’s theorem</p>
<ul>
<li>…and explain why such manipulation is important for confidence interval construction</li>
</ul></li>
<li><p>Derive confidence intervals for unknown parameters based on asymptotic or exact distributions</p></li>
</ul>
</section>
<section id="concept-questions" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="concept-questions"><span class="header-section-number">6.2</span> Concept Questions</h2>
<ol type="1">
<li><p>What feature of a confidence interval tells us about the precision of our estimator?</p></li>
<li><p>Why is “removing” unknown parameters from the asymptotic distribution of our estimators important when constructing confidence intervals?</p></li>
</ol>
</section>
<section id="definitions" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">6.3</span> Definitions</h2>
<p><strong>Asymptotic Normality</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta}_n\)</span> is asymptotically normal if <span class="math inline">\(\hat{\theta}_n\)</span> converges in distribution to a normally distributed random variable.</p>
<p><strong>Asymptotic Efficiency</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta}_n\)</span> is asymptotically efficient if it’s asymptotic variance attains the C-R Lower Bound. Note that this is the C-R Lower Bound for a <em>single</em> observation, and therefore the asymptotic distribution of an MLE looks something like this:</p>
<p><span class="math display">\[
\sqrt{n} (\hat{\theta}_n - \theta) \overset{d}{\to} N\left( 0, \frac{1}{I_1(\theta)}\right)
\]</span></p>
<p><strong>Confidence Interval</strong></p>
<p>A 100(1 - <span class="math inline">\(\alpha\)</span>)% confidence interval for a parameter <span class="math inline">\(\theta\)</span> is given by <span class="math inline">\((a, b)\)</span>, where <span class="math inline">\(\Pr(a \leq \theta \leq b) = 1 - \alpha\)</span>.</p>
</section>
<section id="theorems" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="theorems"><span class="header-section-number">6.4</span> Theorems</h2>
<p><strong>Central Limit Theorem (CLT)</strong></p>
<p>For iid random variables <span class="math inline">\(X_1, \dots, X_n\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>,</p>
<p><span class="math display">\[
\sqrt{n} (\bar{X_n} - \mu) \overset{d}{\to} N(0, \sigma^2)
\]</span></p>
<p>where “<span class="math inline">\(\overset{d}{\to}\)</span>” denotes convergence in distribution.</p>
<details>
<summary>
Proof.
</summary>
<p>Note that this proof is not completely rigorous, in that we will use the following theorem (without proof) in order to prove the CLT:</p>
<p>Theorem: Let <span class="math inline">\(W_1, \dots, W_n\)</span> be a sequence of random variables with MGF of the sequence <span class="math inline">\(W_n\)</span> given by <span class="math inline">\(M_{W_n}(t)\)</span>. Also, let <span class="math inline">\(V\)</span> denote another random variable with MGF <span class="math inline">\(M_V(t)\)</span>. Then if <span class="math inline">\(\underset{n \to \infty}{\text{lim}} M_{W_n}(t) = M_V(t)\)</span>, for all values of <span class="math inline">\(t\)</span> in some interval around <span class="math inline">\(t = 0\)</span>, then the sequence <span class="math inline">\(W_1, \dots, W_n\)</span> converges in distribution to <span class="math inline">\(V\)</span>.</p>
<p>Suppose <span class="math inline">\(X_1, \dots, X_n\)</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and let <span class="math inline">\(Y_i = (X_i - \mu)/\sigma\)</span>. Then <span class="math inline">\(E[Y_i] = 0\)</span>, and <span class="math inline">\(Var[Y_i] = 1\)</span> since</p>
<p><span class="math display">\[
E[Y_i] = E \left[ (X_i - \mu)/\sigma\right] = \frac{1}{\sigma} (E[X_i] - \mu) = \frac{1}{\sigma} (\mu - \mu) = 0
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Var[Y_i] = Var\left[ (X_i - \mu)/\sigma\right] = \frac{1}{\sigma^2} Var[X_i - \mu] = \frac{1}{\sigma^2} Var[X_i] = \frac{\sigma^2}{\sigma^2} = 1
\]</span></p>
<p>Further, let</p>
<p><span class="math display">\[
Z_n = \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} = \frac{1}{\sqrt{n}} \sum_{i = 1}^n Y_i
\]</span></p>
<p>where the last two terms are equal since</p>
<p><span class="math display">\[\begin{align*}
    \frac{1}{\sqrt{n}} \sum_{i = 1}^n Y_i &amp; = \frac{1}{\sqrt{n}} \sum_{i = 1}^n \left( \frac{X_i - \mu}{\sigma}\right)\\
    &amp; = \frac{1}{\sigma\sqrt{n}} \sum_{i = 1}^n (X_i - \mu) \\
    &amp; = \frac{1}{\sigma\sqrt{n}}  (n\bar{X} - n\mu) \\
    &amp; = \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma}
\end{align*}\]</span></p>
<p>We’ll show that <span class="math inline">\(Z_n \overset{d}{\to} N(0,1)\)</span> by showing that the MGF of <span class="math inline">\(Z_n\)</span> converges to the MGF of a standard normal distribution. Let <span class="math inline">\(M_Y(t)\)</span> denote the MGF of each <span class="math inline">\(Y_i\)</span>. Then the MGF of <span class="math inline">\(\sum_{i = 1}^n Y_i\)</span> is given by</p>
<p><span class="math display">\[
E[e^{t\sum_{i = 1}^n Y_i}] = E[e^{tY_1}e^{tY_2} \dots e^{tY_n}] = E[e^{tY_1}]E[e^{tY_2}] \dots E[e^{tY_n}] = M_Y(t)^n
\]</span></p>
<p>and the MGF of <span class="math inline">\(Z_n\)</span> is</p>
<p><span class="math display">\[
M_{Z_n}(t) = E[e^{tZ_n}] = E[e^{t\frac{1}{\sqrt{n}}\sum_{i = 1}^n Y_i}] = M_Y\left(\frac{t}{\sqrt{n}}\right)^n
\]</span></p>
<p>Now note that the Taylor expansion of the function <span class="math inline">\(e^{tY}\)</span> about <span class="math inline">\(0\)</span> is given by</p>
<p><span class="math display">\[
e^{tY} = 1 + tY + \frac{t^2Y^2}{2!} + \frac{t^3Y^3}{3!} + \dots
\]</span></p>
<p>Taking the expectation of both sides, we obtain</p>
<p><span class="math display">\[
E[e^{tY}] = 1 + tE[Y] + \frac{t^2E[Y^2]}{2!} + \frac{t^3E[Y^3]}{3!} + \dots
\]</span></p>
<p>and note now that the left hand side is the MGF for <span class="math inline">\(Y\)</span>. Recalling that <span class="math inline">\(E[Y] = 0\)</span> and <span class="math inline">\(Var[Y] = 1\)</span>, we have</p>
<p><span class="math display">\[
E[e^{tY}] = 1  + \frac{t^2}{2!} + \frac{t^3E[Y^3]}{3!} + \dots
\]</span></p>
<p>And therefore</p>
<p><span class="math display">\[
E[e^{tZ_n}] = \left[1  + \frac{t^2}{2n} + \frac{t^3E[Y^3]}{3!n^{3/2}} + \dots \right]^n
\]</span></p>
<p>We’ll now make use of a theorem regarding sequences of real numbers (without proof): Let <span class="math inline">\(a_n\)</span> and <span class="math inline">\(c_n\)</span> be sequences of real numbers such that <span class="math inline">\(a_n \overset{n \to \infty}{\to} 0\)</span> and <span class="math inline">\(c_na_n^2 \overset{n \to \infty}{\to} 0\)</span>. Then if <span class="math inline">\(a_nc_n \overset{n \to \infty}{\to} b\)</span>, <span class="math inline">\((1 + a_n)^{c_n} \overset{n \to \infty}{\to} e^b\)</span>.</p>
<p>Let <span class="math inline">\(a_n = \frac{t^2}{2n} + \frac{t^3E[Y^3]}{3!n^{3/2}} + \dots\)</span> and <span class="math inline">\(c_n = n\)</span>. Note that both <span class="math inline">\(a_n \overset{n \to \infty}{\to} 0\)</span> and <span class="math inline">\(c_na_n^2 \overset{n \to \infty}{\to} 0\)</span>. Then</p>
<p><span class="math display">\[
\underset{n \to \infty}{\text{lim}} a_n c_n = \underset{n \to \infty}{\text{lim}} \left[ \frac{t^2}{2} + \frac{t^3E[Y^3]}{3!n^{1/2}} + \dots\right] = \frac{t^2}{2}
\]</span></p>
<p>and therefore</p>
<p><span class="math display">\[
M_{Z_n}(t) = (1 + a_n)^{c_n} \overset{n \to \infty}{\to} e^{t^2/2}
\]</span></p>
<p>where we note that the right hand side is the MGF of a standard normal distribution. Then finally, we have proved that</p>
<p><span class="math display">\[
\sqrt{n}(\bar{X} - \mu) \overset{d}{\to} N(0, \sigma^2)
\]</span></p>
<p>as desired.</p>
</details>
<p><strong>Continuous Mapping Theorem</strong></p>
<p>If <span class="math inline">\(X_n \overset{p}{\to} X\)</span>, and <span class="math inline">\(g\)</span> is a continuous function, then <span class="math inline">\(g(X_n) \overset{p}{\to} g(X)\)</span>. Similarly for convergence almost surely and convergence in distribution.</p>
<details>
<summary>
Proof.
</summary>
<p>Left to the reader, but also on <a href="https://en.wikipedia.org/wiki/Continuous_mapping_theorem#Proof">Wikipedia</a>.</p>
</details>
<p><strong>Slutsky’s Theorem</strong></p>
<p>If <span class="math inline">\(g(X, Y)\)</span> is a jointly continuous function at every point <span class="math inline">\((X, a)\)</span> for some fixed <span class="math inline">\(a\)</span>, and if <span class="math inline">\(X_n \overset{d}{\to} X\)</span> and <span class="math inline">\(Y_n \overset{p}{\to} a\)</span>, then <span class="math inline">\(g(X_n, Y_n) \overset{d}{\to} g(X, a)\)</span>.</p>
<details>
<summary>
Proof.
</summary>
<p>Beyond the scope of the course, unfortunately, but here’s a <a href="https://en.wikipedia.org/wiki/Slutsky%27s_theorem#Proof">link</a> to the Wikipedia page if you want to go down that rabbit hole in your spare time.</p>
</details>
<p><strong>Delta-method</strong></p>
<p>Let <span class="math inline">\(\sqrt{n} (Y - \mu) \overset{d}{\to} N(0, \sigma^2)\)</span>. If <span class="math inline">\(g(Y)\)</span> is differentiable at <span class="math inline">\(\mu\)</span> and <span class="math inline">\(g'(\mu) \neq 0\)</span>, then</p>
<p><span class="math display">\[
\sqrt{n} \left( g(Y) - g(\mu)\right) \overset{d}{\to} N(0, [g'(\mu)]^2 \sigma^2)
\]</span></p>
<details>
<summary>
Proof.
</summary>
<p>Since <span class="math inline">\(g\)</span> is differentiable at <span class="math inline">\(\mu\)</span>, it’s first-order Taylor expansion is given by</p>
<p><span class="math display">\[
g(Y) = g(\mu) + (Y - \mu)g'(\mu) + O(| Y - \mu |^2)
\]</span></p>
<p>where <span class="math inline">\(O(f(x))\)</span>, referred to as “Big O,” describes the limiting behavior of the function <span class="math inline">\(f(x)\)</span>. In this case, we use it to note that every term in the Taylor expansion after the first derivative evaluated at <span class="math inline">\(\mu\)</span> is growing no faster than <span class="math inline">\(|Y - \mu|^2\)</span> as <span class="math inline">\(n \to \infty\)</span>.</p>
<p>Rearranging, note that</p>
<p><span class="math display">\[
g(Y) - g(\mu) =  (Y - \mu)g'(\mu) + O(| Y - \mu |^2)
\]</span></p>
<p>and so</p>
<p><span class="math display">\[\begin{align*}
    \sqrt{n}\left( g(Y) - g(\mu) \right) = \sqrt{n}(Y - \mu) g'(\mu) + O(\sqrt{n} |Y - \mu|^2)
\end{align*}\]</span></p>
<p>Then note that <span class="math inline">\(\sqrt{n}(Y - \mu) \overset{d}{\to} N(0, \sigma^2)\)</span>, <span class="math inline">\(g'(\mu) \overset{p}{\to} g'(\mu)\)</span> since it’s just a constant, and <span class="math inline">\(O(\sqrt{n} |Y - \mu|^2) \overset{p}{\to} 0\)</span> (due to the <span class="math inline">\(\sqrt{n}\)</span> term). Then using two applications of Slutsky’s theorem, we can write that</p>
<p><span class="math display">\[
\sqrt{n}(Y - \mu)g'(\mu) \overset{d}{\to} N(0, \sigma^2)g'(\mu) = \overset{d}{\to} N(0, [g'(\mu)]^2\sigma^2)
\]</span> and</p>
<p><span class="math display">\[\begin{align*}
    \sqrt{n}(Y - \mu)g'(\mu) + O(\sqrt{n} |Y - \mu|^2) &amp; \overset{d}{\to} N(0, \sigma^2)g'(\mu) + 0\\
    &amp; \overset{d}{=} N(0, [g'(\mu)]^2\sigma^2)
\end{align*}\]</span></p>
<p>and so finally we have shown that <span class="math display">\[
\sqrt{n}\left( g(Y) - g(\mu) \right) \overset{d}{\to}  N(0, [g'(\mu)]^2\sigma^2)
\]</span> as desired.</p>
</details>
</section>
<section id="worked-examples" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="worked-examples"><span class="header-section-number">6.5</span> Worked Examples</h2>
<p><strong>Problem 1:</strong> Suppose <span class="math inline">\(\sqrt{n}(Y_n - \mu) \overset{d}{\to} N(0, \sigma^2)\)</span>. Find the asymptotic distribution of <span class="math inline">\(\sqrt{n}(Y_n^2 - \mu^2)\)</span> when <span class="math inline">\(\mu \neq 0\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>We can apply the delta-method with the function <span class="math inline">\(g(x) = x^2\)</span>. Note that <span class="math inline">\(g'(x) = 2x\)</span>, and therefore we can write</p>
<p><span class="math display">\[\begin{align*}
    \sqrt{n}(Y_n - \mu) &amp; \overset{d}{\to} N(0, \sigma^2) \\
    \sqrt{n}(g(Y_n) - g(\mu)) &amp; \overset{d}{\to} N(0, [g'(\mu)]^2\sigma^2) \\
    \sqrt{n}(Y_n^2 - \mu^2) &amp; \overset{d}{\to} N(0, [2\mu]^2\sigma^2) \\
    \sqrt{n}(Y_n^2 - \mu^2) &amp; \overset{d}{\to} N(0, 4 \mu^2\sigma^2)
\end{align*}\]</span></p>
</details>
<p><strong>Problem 2:</strong> Suppose <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} Bernoulli(p)\)</span>, and recall that the MLE for <span class="math inline">\(p\)</span> is given by <span class="math inline">\(\hat{p}_{MLE} = \frac{1}{n} \sum_{i = 1}^n X_i\)</span>. Find the asymptotic distribution of <span class="math inline">\(\hat{p}_{MLE}\)</span> using the CLT and known properties of the Bernoulli distribution (expectation and variance, for example), and construct a 95% confidence interval for <span class="math inline">\(p\)</span> based on this asymptotic distribution.</p>
<details>
<summary>
Solution:
</summary>
<p>We know that <span class="math inline">\(E[X_i] = p\)</span> and <span class="math inline">\(Var[X_i] = p(1-p)\)</span>. Then the CLT tell us that</p>
<p><span class="math display">\[
\sqrt{n}(\hat{p}_{MLE} - p) \overset{d}{\to} N(0, p(1-p))
\]</span></p>
<p>The WLLN gives us that <span class="math inline">\(\hat{p}_{MLE} \overset{p}{\to} p\)</span>, since <span class="math inline">\(\hat{p}_{MLE}\)</span> is a sample mean. We can then use the continuous mapping theorem to show that <span class="math inline">\(\frac{1}{\sqrt{\hat{p}_{MLE}(1-\hat{p}_{MLE})}} \overset{p}{\to} \frac{1}{\sqrt{p(1 - p)}}\)</span>. Applying Slutsky’s theorem, we then have</p>
<p><span class="math display">\[
\sqrt{n}\left(\frac{\hat{p}_{MLE} - p}{\sqrt{\hat{p}_{MLE}(1-\hat{p}_{MLE})}}\right) \overset{d}{\to} N(0, 1)
\]</span></p>
<p>and finally, (letting <span class="math inline">\(\hat{p} = \hat{p}_{MLE}\)</span> for ease of notation)</p>
<p><span class="math display">\[\begin{align*}
    0.95 &amp; = \Pr\left(-1.96 &lt; \frac{\hat{p} - p}{\sqrt{\hat{p}(1-\hat{p})/n}}  &lt; 1.96\right)  \\
    &amp; = \Pr\left(-1.96\sqrt{\hat{p}(1-\hat{p})/n} &lt; \hat{p} - p  &lt; 1.96\sqrt{\hat{p}(1-\hat{p})/n}\right) \\
    &amp; = \Pr\left(\hat{p} -1.96\sqrt{\hat{p}(1-\hat{p})/n} &lt;  p  &lt; \hat{p} + 1.96\sqrt{\hat{p}(1-\hat{p})/n}\right)
\end{align*}\]</span></p>
</details>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/math-stat-355\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./consistency.html" class="pagination-link" aria-label="Consistency">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./hypothesis.html" class="pagination-link" aria-label="Hypothesis Testing">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>