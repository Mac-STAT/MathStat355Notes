<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Maximum Likelihood Estimation – MATH/STAT 355: Statistical Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./mom.html" rel="next">
<link href="./probability.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-12acdecf0bfd8bf8a7d30417284c8ed4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./mle.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH/STAT 355: Statistical Theory</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylorokonek/MathematicalStatistics/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./math-stat-355.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Statistical Theory!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./consistency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Computational Optimization</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">2.1</span> Learning Objectives</a></li>
  <li><a href="#concept-questions" id="toc-concept-questions" class="nav-link" data-scroll-target="#concept-questions"><span class="header-section-number">2.2</span> Concept Questions</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">2.3</span> Definitions</a></li>
  <li><a href="#theorems" id="toc-theorems" class="nav-link" data-scroll-target="#theorems"><span class="header-section-number">2.4</span> Theorems</a></li>
  <li><a href="#worked-examples" id="toc-worked-examples" class="nav-link" data-scroll-target="#worked-examples"><span class="header-section-number">2.5</span> Worked Examples</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In <em>Probability</em>, you calculated probabilities of events by assuming a probability model for data and then <em>assuming you knew the value of the parameters</em> in that model. In <em>Mathematical Statistics</em>, we will similarly write down a probability model but then we will use observed data to <em>estimate the value of the parameters</em> in that model.</p>
<p>There is more than one technique that you can use to estimate the value of an unknown parameter. You’re already familiar with one technique—<strong>least squares estimation</strong>—from <em>STAT 155</em>. We’ll review the ideas behind that approach later in the course. To start, we’ll explore two other widely used estimation techniques: <strong>maximum likelihood estimation</strong> (this chapter) and the <strong>method of moments</strong> (next chapter).</p>
<section id="introduction-to-mle" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="introduction-to-mle">Introduction to MLE</h3>
<p>To understand maximum likelihood estimation, we can first break down each individual word in that phrase: (1) maximum, (2) likelihood, (3) estimation. We’ll start in reverse order.</p>
<p>Recall from your introductory statistics course that we are (often) interested in estimating <em>true, unknown <strong>parameters</strong></em> in statistics, using some data. Our best guess at the truth, based on the data we observe / sample that we have, is an <strong><em>estimate</em></strong> of the truth (given some modeling assumptions). This is all the “estimation” piece is getting at here. We’re going to be learning about a method that produces estimates!</p>
<p>The likelihood piece may be less familiar to you. A likelihood is essentially a fancy form of a function (see the Definitions section for an <em>exact</em> definition), that combines an assumed probability distribution for your data, with some unknown parameters.* The key here is that a likelihood is a <em>function</em>. It may <em>look</em> more complicated than a function like <span class="math inline">\(y = mx + b\)</span>, but we can often manipulate them in a similar fashion, which comes in handy when trying to find the…</p>
<p>Maximum! We’ve maximized functions before, and we can do it again! There are ways to maximize functions numerically (using certain algorithms, such as Newton-Raphson for example, which we’ll cover in a later chapter), but we will primarily focus on maximizing likelihoods <em>analytically</em> in this course to help us build intuition.</p>
<p>Recall from calculus: To maximize a function we…</p>
<ol type="1">
<li><p>Take the derivative of the function</p></li>
<li><p>Set the derivative equal to zero</p></li>
<li><p>Solve!</p></li>
<li><p>(double check that the second derivative is negative, so that it’s actually a maximum as opposed to a minimum)</p></li>
<li><p>(also check the endpoints)</p></li>
</ol>
<p>The last two steps we’ll often skip in this class, since things have a tendency to work out nicely with most likelihood functions. If we are trying to maximize a likelihood with <em>multiple</em> parameters, there a few different ways we can go about this. One way (which is nice for distributions like the multivariate normal) is to place all of the parameters in a vector, write the distribution in terms of matrices and vectors, and then use matrix algebra to obtain all of the MLEs for each parameter at once! An alternative way is to take <em>partial</em> derivatives of the likelihood function with respect to each parameter, and solve a <em>system</em> of equations to obtain MLEs for each parameter. We’ll see an example of this in Problem Set 1 as well as Worked Example 2!</p>
<p>One final thing to note (before checking out worked examples and making sure you have a grasp on definitions and theorems) is that it is often <em>easier</em> to maximize the <em>log-likelihood</em> as opposed to the the likelihood… un-logged. This is for a variety of reasons, one of which is that many common probability density functions contain some sort of <span class="math inline">\(e^x\)</span> term, and logging (<em>natural</em> logging) simplifies that for us. Another one is that log rules sometimes make taking derivatives easier. The value of a parameter that maximizes the log-likelihood is the same value that maximizes the likelihood, un-logged (since log is a monotone, increasing function). This is truly just a convenience thing!</p>
</section>
<section id="when-maximizing-the-usual-way-doesnt-work" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="when-maximizing-the-usual-way-doesnt-work">When maximizing the “usual” way doesn’t work…</h3>
<p>To maximize a function what I’m calling the “usual” way involves the five steps listed above. Unfortunately, sometimes this doesn’t work. We typically recognize that the process won’t work once we get to step 3, and realize that “solving” ends up giving us an MLE that doesn’t depend at all on our data. When this happens, it’s usually because the MLE is an <em>order statistic</em> (see Definitions section of this chapter), and usually because the distribution of our random variable has a range that depends on our unknown parameter. An example of this (that will appear on your homework) occurs when <span class="math inline">\(X_1, \dots, X_n\)</span> <span class="math inline">\(\sim\)</span> Uniform(0, <span class="math inline">\(\theta\)</span>). In this case, the range of <span class="math inline">\(X_i\)</span> depends directly on <span class="math inline">\(\theta\)</span>, since it cannot be any <em>greater than</em> <span class="math inline">\(\theta\)</span>.</p>
<p>In these cases, the process of finding the MLE for our unknown parameter usually involves plotting the likelihood as a function of the unknown parameter. We then look at where that function achieves its maximum (usually at one of the endpoints), and determine which observation (again, typically the minimum or maximum) will maximize our likelihood. An example of this can be found in Example 5.2.4 in our course textbook.</p>
</section>
<section id="maximum-likelihood-does-it-make-sense-is-it-even-good" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="maximum-likelihood-does-it-make-sense-is-it-even-good">Maximum Likelihood: Does it make sense? Is it even “good”?</h3>
<p>Let’s think for a minute about why maximum likelihood, as a procedure for producing estimates of parameters, might make sense. Given a distributional assumption* (a probability density function) for <em>independent</em> random variables, we define a “likelihood” as a product of their densities. We can think of this intuitively as just the “likelihood” or “chance” that our data occurs, given a specific distribution. Maximum likelihood estimators then tell us, given that assumed likelihood, <strong>what parameter values make our observed data <em>most likely</em></strong> to have occurred.</p>
<p>So. Does it make sense? I would argue, intuitively, yes! Yes, it does. Is it good? That’s perhaps a different question with a more complicated answer. It’s a good baseline, certainly, and foundational to <em>much</em> of statistical theory. We’ll see in a later chapter that maximum likelihood estimates have good properties related to having minimal variance among a larger class of estimators (yay!), but the maximum likelihood estimators we will consider in this course rely on <em>parametric</em> assumptions (i.e.&nbsp;we assume that the data follows a specific probability distribution in order to calculate MLEs). There are ways around these assumptions, but they are outside the scope of our course.</p>
<p>*<img src="images/chilipepper.png" width="20" height="16"> Note that distributions are only involved in <em>parametric</em> methods, as opposed to non-parametric and semi-parametric methods, the latter of which are for independent study or a graduate course in statistics!</p>
</section>
<section id="relation-to-least-squares" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="relation-to-least-squares">Relation to Least-Squares</h3>
<p>Recall that we typically write a simple linear regression model in one of two ways. For <span class="math inline">\(n\)</span> observations <span class="math inline">\(X_1, \dots, X_n\)</span> with outcomes <span class="math inline">\(Y_1, \dots, Y_n\)</span>, we can write</p>
<p><span class="math display">\[
E[Y_i \mid X_i] = \beta_0 + \beta_1 X_i
\]</span></p>
<p><em>or</em> we can write</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]</span></p>
<p>where <span class="math inline">\(E[\epsilon_i] = 0\)</span>. The latter equation makes it more clear where residuals come into play (they are just given by <span class="math inline">\(\epsilon_i\)</span>), and the former perhaps makes it more clear why the word “average” usually finds its way into our interpretations of regression coefficients. The second form, however, allows us to make it more clear how we would write up a “least-squares” equation.</p>
<p>Recall that the least-squares line (or, line of “best” fit) is the line that minimizes the <em>sum of squared residuals</em>. Parsing these words out, note that our residuals can be written as</p>
<p><span class="math display">\[
\epsilon_i = Y_i - \beta_0 - \beta_1 X_i.
\]</span></p>
<p><em>Squared</em> residuals are then written as</p>
<p><span class="math display">\[
\epsilon_i^2 = (Y_i - \beta_0 - \beta_1 X_i)^2,
\]</span></p>
<p>and finally, the <em>sum</em> of squared residuals is given by</p>
<p><span class="math display">\[
\sum_{i = 1}^n \epsilon_i^2 = \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 X_i)^2
\]</span></p>
<p>We can find what values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> minimize this sum by taking partial derivatives, setting equations equal to zero, and solving. It turns out that if let <span class="math inline">\(\epsilon_i \overset{iid}{\sim} N(0, \sigma^2)\)</span> where <span class="math inline">\(\sigma^2\)</span> is <em>known</em>, then the MLE for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are equivalent to the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the sum of squared residuals!</p>
</section>
<section id="learning-objectives" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">2.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you should be able to…</p>
<ul>
<li><p>Derive maximum likelihood estimators for parameters of common probability density functions</p></li>
<li><p>Calculate maximum likelihood estimators “by hand” for common probability density functions</p></li>
<li><p>Explain (in plain English) why maximum likelihood estimation is an intuitive approach to estimating unknown parameters using a combination of (1) observed data, and (2) a distributional assumption</p></li>
</ul>
</section>
<section id="concept-questions" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="concept-questions"><span class="header-section-number">2.2</span> Concept Questions</h2>
<ol type="1">
<li><p>What is the intuition behind the maximum likelihood estimation (MLE) approach?</p></li>
<li><p>What are the typical steps to find a MLE? (see Ex 5.2.1, 5.2.2, and Case Study 5.2.1; work through at least one of these examples in detail, filling in any steps that the textbook left out)</p></li>
<li><p>Are there ever situations when the typical steps to finding a MLE don’t work? If so, what can we do instead to find the MLE? (see Ex 5.2.3, 5.2.4)</p></li>
<li><p>How do the steps to finding a MLE change when we have more than one unknown parameter? (see Ex 5.2.5)</p></li>
</ol>
</section>
<section id="definitions" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">2.3</span> Definitions</h2>
<p>You are expected to know the following definitions:</p>
<p><strong>Parameter</strong></p>
<p>In a frequentist* framework, a parameter is a <em>fixed</em>, unknown truth (very philosophical). By fixed, I mean “not random”. We assume that there is some true unknown value, governing the generation of all possible random observations of all possible people and things <em>in the whole world</em>. We sometimes call this unknown governing process the “superpopulation” (think: all who ever have been, all who are, and all who ever will be).</p>
<p>Practically speaking, parameters are things that we want to estimate, and we will estimate them using observed data!</p>
<p>*Two main schools of thought in statistics are: (1) Frequentist (everything you’ve ever learned so far in statistics, realistically), and (2) Bayesian. We’ll cover the latter, and differences between the two, in a later chapter. There’s also technically Fiducial inference as a third school of thought, but that one’s never been widely accepted.</p>
<p><strong>Statistic/Estimator</strong></p>
<p>A statistic (or “estimator”) is a function of your data, used to “estimate” an unknown parameter. Often, statistics/estimators will be functions of <em>means</em> or averages, as we’ll see in the worked examples for this chapter!</p>
<p><strong>Likelihood Function</strong></p>
<p>Let <span class="math inline">\(x_1, \dots, x_n\)</span> be a sample of size <span class="math inline">\(n\)</span> of independent observations from the probability density function <span class="math inline">\(f_X(x \mid \boldsymbol{\theta})\)</span>, where <span class="math inline">\(\boldsymbol{\theta}\)</span> is a set of unknown parameters that define the pdf. Then the likelihood function <span class="math inline">\(L(\boldsymbol{\theta})\)</span> is the product of the pdf evaluated at each <span class="math inline">\(x_i\)</span>,</p>
<p><span class="math display">\[
L(\boldsymbol{\theta}) = \prod_{i = 1}^n f_X(x_i \mid \boldsymbol{\theta}).
\]</span></p>
<p>Note that this <em>looks</em> exactly like the joint pdf for <span class="math inline">\(n\)</span> independent random variables, but it is <em>interpreted</em> differently. A likelihood is a function of <em>parameters</em>, given a set of observations (random variables). A joint pdf is a function of random variables.</p>
<p>Note: The likelihood function is one of the reasons why we like independent observations so much! If observations aren’t independent, we can’t simply multiply all of their pdfs together to get a likelihood function.</p>
<p><strong>Maximum Likelihood Estimate (MLE)</strong></p>
<p>Let <span class="math inline">\(L(\boldsymbol{\theta}) = \prod_{i = 1}^n f_X(x_i \mid \boldsymbol{\theta})\)</span> be the likelihood function corresponding to a random sample of observations <span class="math inline">\(x_1, \dots, x_n\)</span>. If <span class="math inline">\(\boldsymbol{\theta}_e\)</span> is such that <span class="math inline">\(L(\boldsymbol{\theta}_e) \geq L(\boldsymbol{\theta})\)</span> for all possible values <span class="math inline">\(\boldsymbol{\theta}\)</span>, then <span class="math inline">\(\boldsymbol{\theta}_e\)</span> is called a <em>maximum likelihood estimate</em> for <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p><strong>Log-likelihood</strong></p>
<p>In statistics, when we say “log,” we essentially always mean “ln” (or, natural log). The log-likelihood is then, hopefully unsurprisingly, given by <span class="math inline">\(\log(L(\boldsymbol{\theta}))\)</span>. One thing that’s useful to note (and will come in handy when calculating MLEs, is that the log of a product is equal to a sum of logs. For likelihoods, that means</p>
<p><span class="math display">\[
\log(L(\boldsymbol{\theta})) = \log \left(\prod_{i = 1}^n f_X(x_i \mid \boldsymbol{\theta})\right) = \sum_{i = 1}^n \log(f_X(x_i \mid \boldsymbol{\theta}))
\]</span></p>
<p>This will end up making it <em>much</em> easier to take derivatives than needing to deal with products!</p>
<p><strong>Order Statistic</strong></p>
<p>The <span class="math inline">\(k\)</span>th order statistic is equal to a sample’s <span class="math inline">\(k\)</span>th smallest value. Practically speaking, there are essentially three order statistics we typically care about: the minimum, the median, and the maximum. We denote the minimum (or, first order statistic) in a sample of random variables <span class="math inline">\(X_1, \dots, X_n\)</span> as <span class="math inline">\(X_{(1)}\)</span> , the maximum as <span class="math inline">\(X_{(n)}\)</span>, and the median <span class="math inline">\(X_{(m+1)}\)</span> where <span class="math inline">\(n = 2m + 1\)</span> <em>when</em> <span class="math inline">\(n\)</span> <em>is odd</em>. Note that median is in fact not an order statistic if <span class="math inline">\(n\)</span> is even (since the median is an average of two values, <span class="math inline">\(X_{(m)}\)</span> and <span class="math inline">\(X_{(m+1)}\)</span>, in this case.</p>
<p>See Example 5.2.4 in the Textbook for an example of where order statistics occasionally come into play when calculating maximum likelihood estimates.</p>
</section>
<section id="theorems" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="theorems"><span class="header-section-number">2.4</span> Theorems</h2>
<p>None for this chapter!</p>
</section>
<section id="worked-examples" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="worked-examples"><span class="header-section-number">2.5</span> Worked Examples</h2>
<p><strong>Problem 1:</strong> Suppose we observe <span class="math inline">\(n\)</span> independent observations <span class="math inline">\(X_1, \dots, X_n \sim Bernoulli(p)\)</span>, where <span class="math inline">\(f_X(x) = p^x(1-p)^{1-x}\)</span>. Find the MLE of <span class="math inline">\(p\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>We can write the likelihood function as</p>
<p><span class="math display">\[
L(p) = \prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i}
\]</span></p>
<p>Then the log-likelihood is given by</p>
<p><span class="math display">\[\begin{align*}
\log(L(p)) &amp; = \log \left[ \prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \right] \\
&amp; = \sum_{i = 1}^n \log \left[p^{x_i} (1-p)^{1 - x_i} \right] \\
&amp; = \sum_{i = 1}^n \left[ \log(p^{x_i}) + \log((1-p)^{1-x_i}) \right] \\
&amp; = \sum_{i = 1}^n \left[ x_i \log(p) + (1 - x_i) \log(1-p) \right] \\
&amp; = \log(p)\sum_{i = 1}^n x_i  + \log(1-p) \sum_{i = 1}^n (1 - x_i)  \\
&amp; = \log(p)\sum_{i = 1}^n x_i  + \log(1-p)  (n - \sum_{i = 1}^n x_i)
\end{align*}\]</span></p>
<p>We can take the derivative of the log-likelihood with respect to <span class="math inline">\(p\)</span>, and set it equal to zero…</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial}{\partial p} \log(L(p)) &amp; = \frac{\partial}{\partial p} \left[ \log(p)\sum_{i = 1}^n x_i  + \log(1-p)  (n - \sum_{i = 1}^n x_i) \right] \\
&amp; = \frac{\sum_{i = 1}^n x_i }{p} - \frac{n - \sum_{i = 1}^n x_i}{1-p} \\
0 &amp; \equiv \frac{\sum_{i = 1}^n x_i }{p} - \frac{n - \sum_{i = 1}^n x_i}{1-p} \\
\frac{\sum_{i = 1}^n x_i }{p}  &amp; = \frac{n - \sum_{i = 1}^n x_i}{1-p} \\
(1-p) \sum_{i = 1}^n x_i &amp; = p (n - \sum_{i = 1}^n x_i) \\
\sum_{i = 1}^n x_i - p\sum_{i = 1}^n x_i &amp; = pn - p \sum_{i = 1}^n x_i \\
\sum_{i = 1}^n x_i &amp; = pn \\
\frac{1}{n} \sum_{i = 1}^n x_i &amp; = p
\end{align*}\]</span></p>
<p>and by solving for <span class="math inline">\(p\)</span>, we get that the MLE of <span class="math inline">\(p\)</span> is equal to <span class="math inline">\(\frac{1}{n}\sum_{i = 1}^n x_i\)</span>. We will <em>often</em> see that the MLEs of parameters are functions of sample averages (in this case, just the identity function!).</p>
</details>
<p><strong>Problem 2:</strong> Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are a random sample from the Normal pdf with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
f_X(x ; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2},
\]</span></p>
<p>for <span class="math inline">\(-\infty &lt; x &lt; \infty, \ -\infty &lt; \mu &lt; \infty,\)</span> and <span class="math inline">\(\sigma^2 &gt; 0\)</span>. Find the MLEs of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. (Note that this is Question 5 on the MLE section of Problem Set 1! For your HW, try your best to do this problem from scratch, without looking at the course notes!)</p>
<details>
<summary>
Solution:
</summary>
<p>Since we are dealing with a likelihood with two parameters, we’ll need to solve a <em>system</em> of equations to obtain the MLEs for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p><span class="math display">\[\begin{align*}
    \log(L(\mu, \sigma^2)) &amp; = \log( \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2} (x_i - \mu)^2) ) \\
    &amp; = \sum_{i = 1}^n \left[ \log(\frac{1}{\sqrt{2\pi \sigma^2}})  - \frac{1}{2\sigma^2} (x_i - \mu)^2 \right] \\
    &amp; = \sum_{i = 1}^n \left[ -\frac{1}{2} \log(2 \pi \sigma^2) - \frac{1}{2\sigma^2} (x_i - \mu)^2 \right] \\
    &amp; = \frac{-n}{2} \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i = 1}^n (x_i - \mu)^2
\end{align*}\]</span></p>
<p>Now we need to find <span class="math inline">\(\frac{\partial}{\partial \sigma^2}\log(L(\mu, \sigma^2))\)</span> and <span class="math inline">\(\frac{\partial}{\partial \mu}\log(L(\mu, \sigma^2))\)</span>. Let’s make our lives a little bit easier by setting <span class="math inline">\(\sigma^2 \equiv \theta\)</span> (so we don’t trip ourselves up with the exponent). We get</p>
<p><span class="math display">\[\begin{align*}
    \frac{\partial}{\partial \theta}\log(L(\mu, \theta)) &amp; = \frac{\partial}{\partial \theta} \left(\frac{-n}{2} \log(2\pi \theta) - \frac{1}{2\theta} \sum_{i = 1}^n (x_i - \mu)^2 \right)\\
    &amp; = \frac{-2\pi n}{4 \pi \theta} + \frac{\sum_{i = 1}^n (x_i - \mu)^2 }{2 \theta^2} \\
    &amp; = \frac{-n}{2 \theta} + \frac{\sum_{i = 1}^n (x_i - \mu)^2 }{2 \theta^2}
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
    \frac{\partial}{\partial \mu}\log(L(\mu, \theta)) &amp; = \frac{\partial}{\partial \mu} \left(\frac{-n}{2} \log(2\pi \theta) - \frac{1}{2\theta} \sum_{i = 1}^n (x_i - \mu)^2 \right)\\
    &amp; = \frac{\partial}{\partial \mu} \left( -\frac{1}{2\theta} \sum_{i = 1}^n (x_i^2 - 2 \mu x_i + \mu^2)\right) \\
    &amp; = \frac{\partial}{\partial \mu} \left( -\frac{1}{2\theta} ( \sum_{i = 1}^n x_i^2 - 2 \mu \sum_{i = 1}^n x_i + n\mu^2 )\right) \\
    &amp; = \frac{\partial}{\partial \mu} \left( -\frac{1}{2\theta} (- 2 \mu \sum_{i = 1}^n x_i + n\mu^2 ) \right) \\
    &amp; = \frac{\partial}{\partial \mu} \left(   \frac{\sum_{i = 1}^n x_i}{\theta} \mu - \frac{n}{2\theta}\mu^2  \right) \\
    &amp; = \frac{\sum_{i = 1}^n x_i}{\theta} - \frac{n}{\theta} \mu
\end{align*}\]</span></p>
<p>We now have the following system of equations to solve:</p>
<p><span class="math display">\[\begin{align*}
    0 &amp; \equiv \frac{-n}{2 \theta} + \frac{\sum_{i = 1}^n (x_i - \mu)^2 }{2 \theta^2} \\
    0 &amp; \equiv \frac{\sum_{i = 1}^n x_i}{\theta} - \frac{n}{\theta} \mu
\end{align*}\]</span></p>
<p>Typically, we solve one of the equations for <em>one</em> of the parameters, plug that into the other equation, and then go from there. We’ll start by solving the second equation for <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[\begin{align*}
    0 &amp; = \frac{\sum_{i = 1}^n x_i}{\theta} - \frac{n}{\theta} \mu \\
    \frac{n}{\theta} \mu &amp; = \frac{\sum_{i = 1}^n x_i}{\theta} \\
    \mu &amp; = \frac{1}{n} \sum_{i = 1}^n x_i
\end{align*}\]</span></p>
<p>Well that’s convenient! We already have the MLE for <span class="math inline">\(\mu\)</span> as being just the sample average. Plugging this into the first equation in our system we obtain</p>
<p><span class="math display">\[\begin{align*}
    0 &amp; = \frac{-n}{2 \theta} + \frac{\sum_{i = 1}^n (x_i - \mu)^2 }{2 \theta^2} \\
    0 &amp; = \frac{-n}{2 \theta} + \frac{\sum_{i = 1}^n (x_i - \frac{1}{n} \sum_{i = 1}^n x_i )^2 }{2 \theta^2} \\
    \frac{n}{2 \theta} &amp; = \frac{\sum_{i = 1}^n (x_i - \frac{1}{n} \sum_{i = 1}^n x_i )^2 }{2 \theta^2} \\
    n &amp; = \frac{\sum_{i = 1}^n (x_i - \frac{1}{n} \sum_{i = 1}^n x_i )^2 }{\theta} \\
    \theta &amp; = \frac{1}{n} \sum_{i = 1}^n (x_i - \frac{1}{n} \sum_{i = 1}^n x_i )^2] \\
    \theta &amp; = \frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x} )^2
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i\)</span>. And so finally, we have that the MLE for <span class="math inline">\(\sigma^2\)</span> is given by <span class="math inline">\(\frac{1}{n} \sum_{i = 1}^n (x_i - \bar{x} )^2\)</span>, and the MLE for <span class="math inline">\(\mu\)</span> is given by <span class="math inline">\(\bar{x}\)</span>!</p>
</details>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/math-stat-355\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./probability.html" class="pagination-link" aria-label="Probability: A Brief Review">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mom.html" class="pagination-link" aria-label="Method of Moments">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>