<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MATH/STAT 455: Mathematical Statistics - 4&nbsp; Properties of Estimators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./consistency.html" rel="next">
<link href="./mom.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./properties.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH/STAT 455: Mathematical Statistics</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylorokonek/MathematicalStatistics/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./math-stat-455.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Mathematical Statistics!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./consistency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Computational Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">4.1</span> Learning Objectives</a></li>
  <li><a href="#reading-guide" id="toc-reading-guide" class="nav-link" data-scroll-target="#reading-guide"><span class="header-section-number">4.2</span> Reading Guide</a>
  <ul class="collapse">
  <li><a href="#reading-questions" id="toc-reading-questions" class="nav-link" data-scroll-target="#reading-questions"><span class="header-section-number">4.2.1</span> Reading Questions</a></li>
  </ul></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">4.3</span> Definitions</a></li>
  <li><a href="#theorems" id="toc-theorems" class="nav-link" data-scroll-target="#theorems"><span class="header-section-number">4.4</span> Theorems</a></li>
  <li><a href="#worked-examples" id="toc-worked-examples" class="nav-link" data-scroll-target="#worked-examples"><span class="header-section-number">4.5</span> Worked Examples</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Now that we’ve developed the tools for deriving estimators of unknown parameters, we can start thinking about different metrics for determining how “good” our estimators actually are. In general, we like our estimators to be:</p>
<ul>
<li><p><strong>Unbiased</strong>: Our estimate should be estimating <em>what it’s supposed to be estimating</em>, for lack of a better phrase. Bias (or, unbiased-ness, in this case) is related to accuracy. In introductory statistics, you likely discussed sample bias (or, whether or not the data you collect is representative of the population you are trying to make inference on) and information bias (or, whether the values of the data you collect are representative of the people who report them). If you have a biased sample or biased information, your estimates (think, regression coefficients) are likely going to misrepresent true relationships in the population. Bias of <em>estimates</em> has a very specific definition in statistical theory that is <em>distinct</em> from sample bias and information bias. Questions of sample bias and information bias are important to consider when collecting and analyzing data, and questions of whether or not our estimates are biased are important to consider <em>prior</em> to analyzing data.</p></li>
<li><p><strong>Precise:</strong> In short, if our estimates are wildly uncertain (think, gigantic confidence intervals), they’ll essentially be of no use to us from a practical perspective. As an extreme example, consider how you would feel if a new cancer drug was released with <em>very</em> severe side-effects, but scientists noted that the drug would increase cancer patients expected survival time by somewhere between 1 and 700 days. Are we really certain enough, in this case, that the benefits of the drug outweigh the potential costs? What if instead, the expected survival time would increase between 650 and 700 days? Would that change your answer? These types of questions are precisely (ha!) why precision is important. Again, you’ve likely discussed precision (colloquially) in an introductory statistics course. In statistical theory, precision (similar to bias) has a very specific definition. So long as our estimates are unbiased, we want to minimize variance (and therefore increase precision) as much as we possibly can. Even at the same sample size, some estimates are more precise than others, which we’ll explore in this chapter.</p></li>
</ul>
<section id="the-bias-variance-trade-off" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="the-bias-variance-trade-off">The Bias-Variance Trade-off</h3>
<p>If you are familiar with machine learning techniques or models for prediction purposes more generally (as opposed to inference), you may have stumbled upon the phrase “bias-variance trade-off.” In scenarios where we want to make good predictions for new observations using a statistical model, one way to measure how “well” our model is predicting new observations is through minimizing <strong>mean squared error</strong>. Intuitively, this is something we should <em>want</em> to minimize: “errors” (the difference between a predicted value and an observed value) are bad, we square them because the direction of the error (above or below) shouldn’t matter too much, and average over them because we need a summary measure of all our errors combined, and an average seems reasonable. In statistical terms, mean squared error has a very specific definition (see below) as the expected value of what is sometimes called a <em>loss function</em> (where in this case, loss is defined as squared error loss). We’ll return to this in the decision theory chapter of our course notes.</p>
<p>It just so happens that we can decompose mean squared error into a sum of two terms: the variance of our estimator + the bias of our estimator (squared). What this means for us is that two estimators may have the <em>exact same</em> MSE, but <em>very</em> different variances or biases (potentially). In general, if we hold MSE constant and imagine <em>increasing</em> the variance of our estimator, the bias would need to decrease accordingly to maintain the same MSE. This is where the “trade-off” comes from. MSE is an <em>incredibly</em> commonly used metric for assessing prediction models, but as we will see, doesn’t necessarily paint a full picture in terms of how “good” an estimator is. Smaller MSE does not automatically imply “better estimator,” just as smaller bias (in some cases) does not automatically imply “better estimator.”</p>
</section>
<section id="learning-objectives" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">4.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you should be able to…</p>
<ul>
<li><p>Calculate bias and variance of various estimators for unknown parameters</p></li>
<li><p>Explain the distinction between bias and variance colloquially in terms of precision and accuracy, and why these properties are important</p></li>
<li><p>Compare estimators in terms of their relative efficiency</p></li>
<li><p>Justify why there exists a bias-variance trade-off, and explain what consequences this may have when comparing estimators</p></li>
</ul>
</section>
<section id="reading-guide" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="reading-guide"><span class="header-section-number">4.2</span> Reading Guide</h2>
<p>Associated Readings: Chapter 5, Section 5.4 (“Properties of Estimators”) &amp; 5.5 (“Minimum-Variance Estimators: The Cramér-Rao Lower Bound”)</p>
<section id="reading-questions" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="reading-questions"><span class="header-section-number">4.2.1</span> Reading Questions</h3>
<ol type="1">
<li><p>Intuitively, what is the difference between bias and precision?</p></li>
<li><p>What are the typical steps to checking if an estimator is unbiased? (see Examples 5.4.2, 5.4.3, and 5.4.4 in the textbook)</p></li>
<li><p>How can we construct unbiased estimators? (see comment in Example 5.4.2 and 5.4.4)</p></li>
<li><p>If an estimator is unbiased, is it also <em>asymptotically</em> unbiased? If an estimator is asymptotically unbiased, is it necessarily unbiased?</p></li>
<li><p>When comparing estimators, how can we determine which estimator is more efficient? (see Examples 5.4.5 and 5.4.6)</p></li>
<li><p>Describe, in your own words, what the Cramér-Rao inequality tells us.</p></li>
<li><p>What is the difference between a UMVUE and an efficient estimator? Does one imply the other? (see the Comment below Definition 5.5.2)</p></li>
</ol>
</section>
</section>
<section id="definitions" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">4.3</span> Definitions</h2>
<p>You are expected to know the following definitions:</p>
<p><strong>Unbiased</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta} = g(X_1, \dots, X_n)\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(E[\hat{\theta}] = \theta\)</span>, for all <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Asymptotically Unbiased</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta} = g(X_1, \dots, X_n)\)</span> is an <em>asymptotically</em> unbiased estimator for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\underset{n \to \infty}{\text{lim}} E[\hat{\theta}] = \theta\)</span>.</p>
<p><strong>Precision</strong></p>
<p>The precision of a random variable <span class="math inline">\(X\)</span> is given by <span class="math inline">\(\frac{1}{Var(X)}\)</span>.</p>
<p><strong>Mean Squared Error (MSE)</strong></p>
<p>The mean squared error of an estimator is given by</p>
<p><span class="math display">\[
MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = Var(\hat{\theta}) + Bias(\hat{\theta})^2
\]</span></p>
<p><strong>Relative Efficiency</strong></p>
<p>The relative efficiency of an estimator <span class="math inline">\(\hat{\theta}_1\)</span> with respect to an estimator <span class="math inline">\(\hat{\theta}_2\)</span> is the ratio <span class="math inline">\(Var(\hat{\theta}_2)/Var(\hat{\theta}_1)\)</span>.</p>
<p><strong>Uniformly Minimum-Variance Unbiased Estimator (UMVUE)</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta}^*\)</span> is the UMVUE if, for all estimators <span class="math inline">\(\hat{\theta}\)</span> in the class of unbiased estimators <span class="math inline">\(\Theta\)</span>,</p>
<p><span class="math display">\[
Var(\hat{\theta}^*) \leq Var(\theta)
\]</span></p>
<p><strong>Score</strong></p>
<p>The score is defined as the first partial derivative with respect to <span class="math inline">\(\theta\)</span> of the log-likelihood function, given by</p>
<p><span class="math display">\[
\frac{\partial}{\partial \theta} \log L(\theta \mid x)
\]</span></p>
<p><strong>Information Matrix</strong></p>
<p>The information matrix* <span class="math inline">\(I(\theta)\)</span> for a collection of iid random variables <span class="math inline">\(X_1, \dots, X_n\)</span> is the variance of the score, given by</p>
<p><span class="math display">\[
I(\theta) = E \left[ \left( \frac{\partial}{\partial \theta} \log L(\theta \mid x) \right)^2\right]
\]</span></p>
<p>Note that the above formula <em>is</em> in fact the variance of the score, since we can show that the <em>expectation</em> of the score is 0 (under some regularity conditions). This is shown as part of the proof of the C-R lower bound in the Theorems section of this chapter.</p>
<p>The information matrix is sometimes written in terms of a pdf for a single random variable as opposed to a likelihood (this is what our textbook does, for example). In this case, we have <span class="math inline">\(I(\theta) = n I_1(\theta)\)</span>, where the <span class="math inline">\(I_1(\theta)\)</span> on the right-hand side is defined as <span class="math inline">\(E \left[ \left( \frac{\partial}{\partial \theta} \log f_X(x \mid \theta) \right)^2\right]\)</span>. Sometimes (as in the textbook) <span class="math inline">\(I_1(\theta)\)</span> is written without the subscript <span class="math inline">\(1\)</span> which is a slight abuse of notation that is endlessly confusing (to me, at least). For this set of course notes, we’ll always specify the information matrix in terms of a pdf for a single random variable with the subscript <span class="math inline">\(1\)</span>, for clarity.</p>
<p>*The information matrix is often referred to as the Fisher Information matrix, as it was developed by Sir Ronald Fisher. Fisher developed <em>much</em> of the core, statistical theory that we use today. He was also the founding chairman of the University of Cambridge Eugenics Society, and contributed to a large body of scientific work and public policy that promoted racist and classist ideals.</p>
</section>
<section id="theorems" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="theorems"><span class="header-section-number">4.4</span> Theorems</h2>
<p><strong>Covariance Inequality</strong> (based on the Cauchy-Schwarz inequality)</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables. Then,</p>
<p><span class="math display">\[
Var(X) \geq \frac{Cov(X, Y)^2}{Var(Y)}
\]</span></p>
<p>The proof is quite clear on <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Wikipedia</a>.</p>
<p><strong>Cramér-Rao Lower Bound</strong></p>
<p>Let <span class="math inline">\(f_Y(y \mid \theta)\)</span> be a pdf with nice* conditions, and let <span class="math inline">\(Y_1, \dots, Y_n\)</span> be a random sample from <span class="math inline">\(f_Y(y \mid \theta)\)</span>. Let <span class="math inline">\(\hat{\theta}\)</span> be <em>any</em> unbiased estimator of <span class="math inline">\(\theta\)</span>. Then</p>
<span class="math display">\[\begin{align*}
Var(\hat{\theta}) &amp; \geq \left\{ E\left[ \left( \frac{\partial \log( L(\theta \mid y))}{\partial \theta}\right)^2\right]\right\}^{-1} \\
&amp; = \left\{ E\left[ \left( \frac{\partial^2 \log( L(\theta \mid y))}{\partial \theta^2}\right)^2\right]\right\}^{-1} \\
&amp; = \frac{1}{I(\theta)}
\end{align*}\]</span>
<p>*our nice conditions that we need are that <span class="math inline">\(f_Y(y \mid \theta)\)</span> has continuous first- and second-order derivatives, which would quickly discover we need by looking at the form for the C-R lower bound, and that the set of values <span class="math inline">\(y\)</span> where <span class="math inline">\(f_Y(y \mid \theta) \neq 0\)</span> does not depend on <span class="math inline">\(\theta\)</span>. If you are familiar with the concept of the “support” of a function, that is where this second condition comes from. The key here is that this condition allows to interchange derivatives and integrals, in particular, <span class="math inline">\(\frac{\partial}{\partial \theta} \int f(x) dx = \int \frac{\partial}{\partial \theta} f(x)dx\)</span>, which we’ll need to complete the proof.</p>
<p><strong>Proof.</strong></p>
<p>Let <span class="math inline">\(X = \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta}\)</span>. By the Covariance Inequality,</p>
<p><span class="math display">\[
Var(\hat{\theta}) \geq \frac{Cov(\hat{\theta},X)^2}{Var(X)}
\]</span></p>
<p>and so if we can show</p>
<span class="math display">\[\begin{align*}
\frac{Cov(\hat{\theta},X)^2}{Var(X)} &amp; = \left\{ E\left[ \left( \frac{\partial \log( L(\theta \mid \textbf{y}))}{\partial \theta}\right)^2\right]\right\}^{-1}  \\
&amp; = \frac{1}{I(\theta)}
\end{align*}\]</span>
<p>then we’re done, as this is the C-R lower bound. Note first that</p>
<span class="math display">\[\begin{align*}
E[X] &amp; = \int x f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right)  f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \left( \frac{\partial \log f_Y(\textbf{y} \mid \theta)}{\partial \theta} \right)  f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \frac{\frac{\partial}{\partial \theta} f_Y(\textbf{y} \mid \theta)}{ f_Y(\textbf{y} \mid \theta)} f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \frac{\partial}{\partial \theta} f_Y (\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \frac{\partial}{\partial \theta} \int f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \frac{\partial}{\partial \theta} 1 \\
&amp; = 0
\end{align*}\]</span>
<p>This means that</p>
<span class="math display">\[\begin{align*}
    Var[X] &amp; = E[X^2] - E[X]^2 \\
    &amp; = E[X^2] \\
    &amp; = E \left[ \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right)^2\right ]
\end{align*}\]</span>
<p>and</p>
<span class="math display">\[\begin{align*}
    Cov(\hat{\theta}, X) &amp; = E[\hat{\theta} X] - E[\hat{\theta}] E[X] \\
    &amp; = E[\hat{\theta}X] \\
    &amp; = \int \hat{\theta} x f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right) f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \left( \frac{\partial \log f_Y(\textbf{y} \mid \theta)}{\partial \theta} \right) f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \frac{\frac{\partial}{\partial \theta} f_Y(\textbf{y} \mid \theta)}{ f_Y(\textbf{y} \mid \theta)} f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \frac{\partial}{\partial \theta} f_Y(\textbf{y} \mid \theta) d\textbf{y}  \\
    &amp; = \frac{\partial}{\partial \theta} \int \hat{\theta} f_Y(\textbf{y} \mid \theta) d\textbf{y}  \\
    &amp; =  \frac{\partial}{\partial \theta} E[\hat{\theta}] \\
    &amp; = \frac{\partial}{\partial \theta} \theta \\
    &amp; = 1
\end{align*}\]</span>
<p>where <span class="math inline">\(E[\hat{\theta}] = \theta\)</span> since our estimator is unbiased. Putting this all together, we have</p>
<span class="math display">\[\begin{align*}
    Var[\hat{\theta}] &amp; \geq \frac{Cov(\hat{\theta},X)^2}{Var(X)} \\
    &amp; = \frac{1^2}{E \left[ \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right)^2\right ]} \\
    &amp; = \frac{1}{I(\theta)}
\end{align*}\]</span>
<p>as desired.</p>
<p><strong>Comment:</strong> Note that what the Cramér-Rao lower bound tells us is that, if the variance of an unbiased estimator is <em>equal</em> to the Cramér-Rao lower bound, then that estimator has the <em>minimum possible variance</em> among all unbiased estimators there could possibly be. This allows us to <em>prove</em>, for example, whether or not an unbiased estimator is the UMVUE: If an unbiased estimator’s variance achieves the C-R lower bound, then it is <em>optimal</em> according to the UMVUE criterion.</p>
</section>
<section id="worked-examples" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="worked-examples"><span class="header-section-number">4.5</span> Worked Examples</h2>
<p><strong>Problem 1:</strong> Suppose <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} Exponential(1/\theta)\)</span>. Compute the MLE of <span class="math inline">\(\theta\)</span>, and show that it is an unbiased estimator of <span class="math inline">\(\theta\)</span>.</p>
<p>Note that we can write</p>
<span class="math display">\[\begin{align*}
    L(\theta) &amp; = \prod_{i = 1}^n \frac{1}{\theta} e^{-x_i / \theta} \\
    \log L(\theta) &amp; = \sum_{i = 1}^n \log(\frac{1}{\theta} e^{-x_i / \theta}) \\
    &amp; = \sum_{i = 1}^n  \log(\frac{1}{\theta}) - \sum_{i = 1}^n x_i / \theta \\
    &amp; = -n \log(\theta) - \frac{1}{\theta} \sum_{i = 1}^n x_i \\
    \frac{\partial}{\partial \theta} \log L(\theta) &amp; = \frac{\partial}{\partial \theta}  \left( -n \log(\theta) - \frac{1}{\theta} \sum_{i = 1}^n x_i \right) \\
    &amp; = -\frac{n}{\theta} + \frac{\sum_{i = 1}^n x_i }{\theta^2}
\end{align*}\]</span>
<p>Setting this equal to <span class="math inline">\(0\)</span> and solving for <span class="math inline">\(\theta\)</span> we obtain</p>
<span class="math display">\[\begin{align*}
    0 &amp; \equiv -\frac{n}{\theta} + \frac{\sum_{i = 1}^n x_i }{\theta^2}  \\
    \frac{n}{\theta} &amp; = \frac{\sum_{i = 1}^n x_i }{\theta^2} \\
    n &amp; = \frac{\sum_{i = 1}^n x_i }{\theta} \\
    \theta &amp; = \frac{1}{n} \sum_{i = 1}^n x_i
\end{align*}\]</span>
<p>and so the MLE for <span class="math inline">\(\theta\)</span> is the sample mean. To show that the MLE is unbiased, we note that</p>
<span class="math display">\[\begin{align*}
    E \left[ \frac{1}{n} \sum_{i = 1}^n X_i \right] &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i] = \frac{1}{n} \sum_{i = 1}^n \theta  = \theta
\end{align*}\]</span>
<p>as desired.</p>
<p><strong>Problem 2:</strong> Suppose again that <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} Exponential(1/\theta)\)</span>. Let <span class="math inline">\(\hat{\theta}_2 = Y_1\)</span>, and <span class="math inline">\(\hat{\theta}_3 = nY_{(1)}\)</span>. Show that <span class="math inline">\(\hat{\theta}_2\)</span> and <span class="math inline">\(\hat{\theta}_3\)</span> are unbiased estimators of <span class="math inline">\(\theta\)</span>. Hint: use the fact that <span class="math inline">\(Y_{(1)} \sim Exponential(n/\theta)\)</span></p>
<p>Note that the mean of a random variable <span class="math inline">\(Y \sim Exponential(\lambda)\)</span> is given by <span class="math inline">\(1/\lambda\)</span>. Then we can write</p>
<p><span class="math display">\[
E[\hat{\theta}_2] = E[Y_1] = \frac{1}{1/\theta} = \theta
\]</span></p>
<p>and</p>
<p><span class="math display">\[
E[\hat{\theta}_3] = E[nY_{(1)}] = \frac{n}{n/\theta} = \theta
\]</span> as desired.</p>
<p><strong>Problem 3:</strong> Compare the variance of the estimators from Problems 1 and 2. Which is most efficient?</p>
<p>Recall that the variance of a random variable <span class="math inline">\(Y \sim Exponential(\lambda)\)</span> is given by <span class="math inline">\(1/\lambda^2\)</span>. Let the MLE from Problem 1 be denoted <span class="math inline">\(\hat{\theta}_1 = \bar{X}\)</span>. Then we can write</p>
<p><span class="math display">\[
Var\left[\hat{\theta}_1\right] = Var\left[\frac{1}{n} \sum_{i = 1}^n X_i\right] = \frac{1}{n^2} \sum_{i = 1}^n Var[X_i] = \frac{1}{n^2} \left( \frac{n}{(1/\theta)^2} \right) = \frac{\theta^2}{n}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Var\left[\hat{\theta}_2\right] = Var[Y_1] = \frac{1}{(1/\theta)^2} = \theta^2
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Var\left[\hat{\theta}_2\right] = Var[nY_{(1)}] = n^2 Var[Y_{(1)}] = \frac{n^2}{(n/\theta)^2} = \theta^2
\]</span></p>
<p>Thus, the variance of the MLE, <span class="math inline">\(\hat{\theta}_1\)</span>, is most efficient, and is <span class="math inline">\(n\)</span> times smaller than the variance of both <span class="math inline">\(\hat{\theta}_2\)</span> and <span class="math inline">\(\hat{\theta}_3\)</span>.</p>
<p><strong>Problem 4:</strong> Suppose <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>. Show that the estimator <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum_{i = 1}^n X_i\)</span> <em>and</em> the estimator <span class="math inline">\(\hat{\mu}_w = \sum_{i = 1}^n w_i X_i\)</span> are both unbiased estimators of <span class="math inline">\(\mu\)</span>, where <span class="math inline">\(\sum_{i = 1}^n w_i = 1\)</span>.</p>
<p>We can write</p>
<p><span class="math display">\[
E[\hat{\mu}] = E\left[ \frac{1}{n} \sum_{i = 1}^n X_i \right] = \frac{1}{n}\sum_{i = 1}^n E[X_i] = \frac{1}{n}\sum_{i = 1}^n \mu = \mu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
E[\hat{\mu}_w] = E \left[ \sum_{i = 1}^n w_i X_i \right] = \sum_{i = 1}^n w_i E \left[ X_i \right] = \sum_{i = 1}^n w_i \mu = \mu \sum_{i = 1}^n w_i = \mu
\]</span></p>
<p>as desired.</p>
<p><strong>Problem 5:</strong> Determine whether the estimator <span class="math inline">\(\hat{\mu}\)</span> or <span class="math inline">\(\hat{\mu}_w\)</span> is more efficient, in Problem 5, if we additionally impose the constraint <span class="math inline">\(w_i \geq 0\)</span> <span class="math inline">\(\forall i\)</span>. (Note that this is a more “general” example based on Example 5.4.5 in the course textbook) (Hint: use the Cauchy-Schwarz inequality)</p>
<p>To determine relative efficiency, we must compute the variance of each estimator. We have</p>
<p><span class="math display">\[
Var[\hat{\mu}] = Var \left[ \frac{1}{n} \sum_{i = 1}^n X_i \right] = \frac{1}{n^2} \sum_{i = 1}^n Var[X_i] = \frac{1}{n^2} \sum_{i = 1}^n \sigma^2 = \sigma^2 / n
\]</span></p>
<p>and</p>
<span class="math display">\[\begin{align*}
    Var[\hat{\mu}_w] &amp; =  Var \left[ \sum_{i = 1}^n w_i X_i \right] \\
    &amp; = \sum_{i = 1}^n Var[w_i X_i] \\
    &amp; = \sum_{i = 1}^n w_i^2 Var[X_i] \\
    &amp; = \sum_{i = 1}^n w_i^2  \sigma^2 \\
    &amp; = \sigma^2 \sum_{i = 1}^n w_i^2
\end{align*}\]</span>
<p>And so to determine which estimator is more efficient, we need to determine if <span class="math inline">\(\frac{1}{n}\)</span> is less than <span class="math inline">\(\sum_{i = 1}^n w_i^2\)</span> (or not). The Cauchy-Schwarz inequality tells us that</p>
<span class="math display">\[\begin{align*}
    \left( \sum_{i = 1}^n w_i \cdot 1\right)^2 &amp; \leq \left( \sum_{i = 1}^n w_i^2 \right) \left( \sum_{i = 1}^n 1^2 \right) \\
    \left( \sum_{i = 1}^n w_i \right)^2 &amp; \leq \left( \sum_{i = 1}^n w_i^2 \right) n \\
    1 &amp; \leq \left( \sum_{i = 1}^n w_i^2 \right) n  \\
    \frac{1}{n} &amp; \leq \sum_{i = 1}^n w_i^2
\end{align*}\]</span>
<p>and therefore, <span class="math inline">\(\hat{\mu}\)</span> is more efficient than <span class="math inline">\(\hat{\mu}_w\)</span>.</p>
<p><strong>Problem 6:</strong> Suppose <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>. Show that the MLE for <span class="math inline">\(\sigma^2\)</span> is <em>biased</em>, and suggest a modified variance estimator for <span class="math inline">\(\sigma^2\)</span> that is <em>unbiased</em>. (Note that this is example 5.4.4 in our course textbook)</p>
<p>Recall that the MLE for <span class="math inline">\(\sigma^2\)</span> is given by <span class="math inline">\(\frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\)</span>. Then</p>
<span class="math display">\[\begin{align*}
    E\left[ \frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\right] &amp; = \frac{1}{n} \sum_{i = 1}^n E\left[ (X_i - \bar{X})^2\right] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E\left[ X_i^2 - 2X_i \bar{X} + \bar{X}^2\right] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - 2 E\left[ \frac{1}{n} \sum_{i = 1}^n X_i \bar{X} \right] + E[\bar{X}^2] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - 2 E\left[ \bar{X} \frac{1}{n} \sum_{i = 1}^n X_i  \right] + E[\bar{X}^2] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - 2 E\left[ \bar{X}^2  \right] + E[\bar{X}^2] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - E\left[ \bar{X}^2  \right]
\end{align*}\]</span>
<p>Recall that since <span class="math inline">\(X_i \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>, <span class="math inline">\(\bar{X} \sim N(\mu, \sigma^2/n)\)</span>, and that we can write <span class="math inline">\(Var[Y] + E[Y]^2 = E[Y^2]\)</span> (definition of variance). Then we can write</p>
<span class="math display">\[\begin{align*}
    E\left[ \frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2 \right] &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - E\left[ \bar{X}^2  \right] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n \left( \sigma^2 + \mu^2 \right) - \left( \frac{\sigma^2}{n} + \mu^2 \right) \\
    &amp; = \sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2  \\
    &amp; = \sigma^2 - \frac{\sigma^2}{n} \\
    &amp; = \sigma^2 \left( 1 - \frac{1}{n} \right) \\
    &amp; = \sigma^2  \left( \frac{n-1}{n} \right)
\end{align*}\]</span>
<p>Therefore, since <span class="math inline">\(E[\hat{\sigma}^2_{MLE}] \neq \sigma^2\)</span>, the MLE is unbiased. Note that</p>
<span class="math display">\[\begin{align*}
    E\left[ \left( \frac{n}{n-1} \right)\frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\right] &amp; = \left( \frac{n}{n-1} \right) \left( \frac{n-1}{n} \right) \sigma^2   \\
    &amp; = \sigma^2
\end{align*}\]</span>
<p>and so the estimator <span class="math inline">\(\frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X})^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>. This estimator is often called the “sample variance”, and is denoted by <span class="math inline">\(S^2\)</span>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./mom.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./consistency.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>