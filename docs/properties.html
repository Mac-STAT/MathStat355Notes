<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Properties of Estimators – MATH/STAT 355: Statistical Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./consistency.html" rel="next">
<link href="./mom.html" rel="prev">
<link href="./favicon-32x32.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-12acdecf0bfd8bf8a7d30417284c8ed4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./properties.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH/STAT 355: Statistical Theory</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/taylorokonek/MathematicalStatistics/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./math-stat-355.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome to Statistical Theory!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability: A Brief Review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./properties.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./consistency.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Asymptotics &amp; the Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Decision Theory</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./computation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Computational Optimization</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">4.1</span> Learning Objectives</a></li>
  <li><a href="#concept-questions" id="toc-concept-questions" class="nav-link" data-scroll-target="#concept-questions"><span class="header-section-number">4.2</span> Concept Questions</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions"><span class="header-section-number">4.3</span> Definitions</a></li>
  <li><a href="#theorems" id="toc-theorems" class="nav-link" data-scroll-target="#theorems"><span class="header-section-number">4.4</span> Theorems</a></li>
  <li><a href="#worked-examples" id="toc-worked-examples" class="nav-link" data-scroll-target="#worked-examples"><span class="header-section-number">4.5</span> Worked Examples</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Properties of Estimators</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Now that we’ve developed the tools for deriving estimators of unknown parameters, we can start thinking about different metrics for determining how “good” our estimators actually are. In general, we like our estimators to be:</p>
<ul>
<li><p><strong>Unbiased</strong>: Our estimate should be estimating <em>what it’s supposed to be estimating</em>, for lack of a better phrase. Bias (or, unbiased-ness, in this case) is related to accuracy. In introductory statistics, you likely discussed sample bias (or, whether or not the data you collect is representative of the population you are trying to make inference on) and information bias (or, whether the values of the data you collect are representative of the people who report them). If you have a biased sample or biased information, your estimates (think, regression coefficients) are likely going to misrepresent true relationships in the population.</p>
<p>Bias of <em>estimates</em> has a very specific definition in statistical theory that is <em>distinct</em> from sample bias and information bias. Questions of sample bias and information bias are important to consider when collecting and analyzing data, and questions of whether or not our estimates are biased are important to consider <em>prior</em> to analyzing data.</p></li>
<li><p><strong>Precise:</strong> In short, if our estimates are wildly uncertain (think, gigantic confidence intervals), they’ll essentially be of no use to us from a practical perspective. As an extreme example, consider how you would feel if a new cancer drug was released with <em>very</em> severe side-effects, but scientists noted that the drug would increase cancer patients expected survival time by somewhere between 1 and 700 days. Are we really certain enough, in this case, that the benefits of the drug outweigh the potential costs? What if instead, the expected survival time would increase between 650 and 700 days? Would that change your answer?</p>
<p>These types of questions are precisely (ha!) why precision is important. Again, you’ve likely discussed precision (colloquially) in an introductory statistics course. In statistical theory, precision (similar to bias) has a very specific definition. So long as our estimates are unbiased, we want to minimize variance (and therefore increase precision) as much as we possibly can. Even at the same sample size, some estimates are more precise than others, which we’ll explore in this chapter.</p></li>
</ul>
<section id="the-bias-variance-trade-off" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="the-bias-variance-trade-off">The Bias-Variance Trade-off</h3>
<p>If you are familiar with machine learning techniques or models for prediction purposes more generally (as opposed to inference), you may have stumbled upon the phrase “bias-variance trade-off.” In scenarios where we want to make good predictions for new observations using a statistical model, one way to measure how “well” our model is predicting new observations is through minimizing <strong>mean squared error</strong>. Intuitively, this is something we should <em>want</em> to minimize: “errors” (the difference between a predicted value and an observed value) are bad, we square them because the direction of the error (above or below) shouldn’t matter too much, and average over them because we need a summary measure of all our errors combined, and an average seems reasonable. In statistical terms, mean squared error has a very specific definition (see below) as the expected value of what is sometimes called a <em>loss function</em> (where in this case, loss is defined as squared error loss). We’ll return to this in the decision theory chapter of our course notes.</p>
<p>It just so happens that we can decompose mean squared error into a sum of two terms: the variance of our estimator + the bias of our estimator (squared). What this means for us is that two estimators may have the <em>exact same</em> MSE, but <em>very</em> different variances or biases (potentially). In general, if we hold MSE constant and imagine <em>increasing</em> the variance of our estimator, the bias would need to decrease accordingly to maintain the same MSE. This is where the “trade-off” comes from. MSE is an <em>incredibly</em> commonly used metric for assessing prediction models, but as we will see, doesn’t necessarily paint a full picture in terms of how “good” an estimator is. Smaller MSE does not automatically imply “better estimator,” just as smaller bias (in some cases) does not automatically imply “better estimator.”</p>
</section>
<section id="sufficiency" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="sufficiency">Sufficiency</h3>
<p>Another property we like to have in an estimator (sometimes) is called <em>sufficiency</em>. I like to think about sufficiency in terms of minimizing the amount of information we need to retain in order to get a “complete picture” of what’s going on. Suppose, for example, someone is allergic to tomatoes. Rather than listing <em>every food</em> that contains tomatoes and saying that they’re allergic to each of them individually, they could just say that they’re allergic to tomatoes and call it a day. Stating “tomatoes” is <strong>sufficient</strong> information in this case for us to get the whole picture of their allergies!</p>
<p>A similar concept applies to estimators. Recall from the MLE chapter of the notes that we previously showed that the MLE of a sample proportion is given by <span class="math inline">\(\bar{X}\)</span>. If I want someone to be able to obtain the MLE for a sample proportion, I then have a few options. I could give them:</p>
<ul>
<li>Every observation I know, <span class="math inline">\(x_1, \dots,x_n\)</span></li>
<li>Just one number, the sample mean, <span class="math inline">\(\frac{1}{n}\sum_{i = 1}^n x_i\)</span></li>
<li>All my observations plus some extra information, just for fun!</li>
</ul>
<p>It should hopefully be obvious that you don’t need extra information for fun, but we <em>also</em> don’t need to know the value of each individual observation. The sample mean is sufficient! Formal definitions and a relevant theorem to follow.</p>
</section>
<section id="learning-objectives" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">4.1</span> Learning Objectives</h2>
<p>By the end of this chapter, you should be able to…</p>
<ul>
<li><p>Calculate bias and variance of various estimators for unknown parameters</p></li>
<li><p>Explain the distinction between bias and variance colloquially in terms of precision and accuracy, and why these properties are important</p></li>
<li><p>Compare estimators in terms of their relative efficiency</p></li>
<li><p>Justify why there exists a bias-variance trade-off, and explain what consequences this may have when comparing estimators</p></li>
</ul>
</section>
<section id="concept-questions" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="concept-questions"><span class="header-section-number">4.2</span> Concept Questions</h2>
<ol type="1">
<li><p>Intuitively, what is the difference between bias and precision?</p></li>
<li><p>What are the typical steps to checking if an estimator is unbiased?</p></li>
<li><p>How can we construct unbiased estimators?</p></li>
<li><p>If an estimator is unbiased, is it also <em>asymptotically</em> unbiased? If an estimator is asymptotically unbiased, is it necessarily unbiased?</p></li>
<li><p>When comparing estimators, how can we determine which estimator is more efficient?</p></li>
<li><p>Why might we care about sufficiency, particularly when thinking about the variance of unbiased estimators?</p></li>
<li><p>Describe, in your own words, what the Cramér-Rao inequality tells us.</p></li>
<li><p>What is the difference between a UMVUE and an efficient estimator? Does one imply the other?</p></li>
</ol>
</section>
<section id="definitions" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="definitions"><span class="header-section-number">4.3</span> Definitions</h2>
<p>You are expected to know the following definitions:</p>
<p><strong>Unbiased</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta} = g(X_1, \dots, X_n)\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(E[\hat{\theta}] = \theta\)</span>, for all <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Asymptotically Unbiased</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta} = g(X_1, \dots, X_n)\)</span> is an <em>asymptotically</em> unbiased estimator for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\underset{n \to \infty}{\text{lim}} E[\hat{\theta}] = \theta\)</span>.</p>
<p><strong>Precision</strong></p>
<p>The precision of a random variable <span class="math inline">\(X\)</span> is given by <span class="math inline">\(\frac{1}{Var(X)}\)</span>.</p>
<p><strong>Mean Squared Error (MSE)</strong></p>
<p>The mean squared error of an estimator is given by</p>
<p><span class="math display">\[
MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = Var(\hat{\theta}) + Bias(\hat{\theta})^2
\]</span></p>
<p><strong>Sufficient</strong></p>
<p>For some function <span class="math inline">\(T\)</span>, <span class="math inline">\(T(X)\)</span> is a sufficient statistic for an unknown parameter <span class="math inline">\(\theta\)</span> if the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(T(X)\)</span> does not depend on <span class="math inline">\(\theta\)</span>. A “looser” definition is that the distribution of <span class="math inline">\(X\)</span> must depend on <span class="math inline">\(\theta\)</span> <em>only through</em> <span class="math inline">\(T(X)\)</span>.</p>
<p><strong>Minimal Sufficiency</strong></p>
<p>For some function <span class="math inline">\(T^*\)</span>, <span class="math inline">\(T^*(X)\)</span> is a minimal sufficient statistic for an unknown parameter <span class="math inline">\(\theta\)</span> if <span class="math inline">\(T^*(X)\)</span> is sufficient, <em>and</em> for every other sufficient statistic <span class="math inline">\(T(X)\)</span>. <span class="math inline">\(T^*(X) = f(T(X))\)</span> for some function <span class="math inline">\(f\)</span>.</p>
<p><strong>Complete</strong></p>
<p>A statistic <span class="math inline">\(T(X)\)</span> is <em>complete</em> for an unknown parameter <span class="math inline">\(\theta\)</span> if <span class="math display">\[
E[g(T(x))] \text{ is } \theta-\text{free} \implies g(T(x)) \text{ is constant, almost everywhere}
\]</span> for a nice function <span class="math inline">\(g\)</span>.</p>
<p>Importantly, it is <em>equivalent</em> to say that <span class="math inline">\(T(X)\)</span> is <em>complete</em> for an unknown parameter <span class="math inline">\(\theta\)</span> if</p>
<p><span class="math display">\[
E[g(T(x))] = 0 \implies g(T(x)) = 0 \quad\text{ almost everywhere}
\]</span></p>
<p><strong>Relative Efficiency</strong></p>
<p>The relative efficiency of an estimator <span class="math inline">\(\hat{\theta}_1\)</span> with respect to an estimator <span class="math inline">\(\hat{\theta}_2\)</span> is the ratio <span class="math inline">\(Var(\hat{\theta}_2)/Var(\hat{\theta}_1)\)</span>.</p>
<p><strong>Uniformly Minimum-Variance Unbiased Estimator (UMVUE)</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta}^*\)</span> is the UMVUE if, for all estimators <span class="math inline">\(\hat{\theta}\)</span> in the class of unbiased estimators <span class="math inline">\(\Theta\)</span>,</p>
<p><span class="math display">\[
Var(\hat{\theta}^*) \leq Var(\hat{\theta})
\]</span></p>
<p><strong>Score</strong></p>
<p>The score is defined as the first partial derivative with respect to <span class="math inline">\(\theta\)</span> of the log-likelihood function, given by</p>
<p><span class="math display">\[
\frac{\partial}{\partial \theta} \log L(\theta \mid x)
\]</span></p>
<p><strong>Information Matrix</strong></p>
<p>The information matrix* <span class="math inline">\(I(\theta)\)</span> for a collection of iid random variables <span class="math inline">\(X_1, \dots, X_n\)</span> is the variance of the score, given by</p>
<p><span class="math display">\[
I(\theta) = E \left[ \left( \frac{\partial}{\partial \theta} \log L(\theta \mid x) \right)^2\right] = -E\left[ \frac{\partial^2}{\partial \theta^2} \log L(\theta \mid x)\right]
\]</span></p>
<p>Note that the above formula <em>is</em> in fact the variance of the score, since we can show that the <em>expectation</em> of the score is 0 (under some regularity conditions). This is shown as part of the proof of the C-R lower bound in the Theorems section of this chapter.</p>
<p>The information matrix is sometimes written in terms of a pdf for a single random variable as opposed to a likelihood. In this case, we have <span class="math inline">\(I(\theta) = n I_1(\theta)\)</span>, where the <span class="math inline">\(I_1(\theta)\)</span> on the right-hand side is defined as <span class="math inline">\(E \left[ \left( \frac{\partial}{\partial \theta} \log f_X(x \mid \theta) \right)^2\right]\)</span>. Sometimes <span class="math inline">\(I_1(\theta)\)</span> is written without the subscript <span class="math inline">\(1\)</span> which is a slight abuse of notation that is endlessly confusing (to me, at least). For this set of course notes, we’ll always specify the information matrix in terms of a pdf for a single random variable with the subscript <span class="math inline">\(1\)</span>, for clarity.</p>
<p>*The information matrix is often referred to as the Fisher Information matrix, as it was developed by Sir Ronald Fisher. Fisher developed <em>much</em> of the core, statistical theory that we use today. He was also the founding chairman of the University of Cambridge Eugenics Society, and contributed to a large body of scientific work and public policy that promoted racist and classist ideals.</p>
</section>
<section id="theorems" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="theorems"><span class="header-section-number">4.4</span> Theorems</h2>
<p><strong>Covariance Inequality</strong> (based on the Cauchy-Schwarz inequality)</p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables. Then,</p>
<p><span class="math display">\[
Var(X) \geq \frac{Cov(X, Y)^2}{Var(Y)}
\]</span></p>
<p>The proof is quite clear on <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Wikipedia</a>.</p>
<p><strong>The Factorization Criterion for sufficiency</strong></p>
<p>Consider a pdf for a random variable <span class="math inline">\(X\)</span> that depends on an unknown parameter <span class="math inline">\(\theta\)</span>, given by <span class="math inline">\(\pi(x \mid \theta)\)</span>. The statistic <span class="math inline">\(T(x)\)</span> is sufficient for <span class="math inline">\(\theta\)</span> if and only if <span class="math inline">\(\pi(x \mid \theta)\)</span> factors as:</p>
<p><span class="math display">\[
\pi(x \mid \theta) = g(T(x) \mid \theta) h(x)
\]</span> where <span class="math inline">\(g(T(x) \mid \theta)\)</span> depends on <span class="math inline">\(x\)</span> only through <span class="math inline">\(T(x)\)</span>, and <span class="math inline">\(h(x)\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>Note that in the statistics literature this criterion is sometimes referred to as the Fisher-Neyman Factorization Criterion.</p>
<p>Two proofs available on <a href="https://en.wikipedia.org/wiki/Sufficient_statistic#Proof">Wikipedia</a>. The one for the discrete-only case is more intuitive, if you’d like to look through one of them.</p>
<p><strong>Lehmann-Scheffe Theorem</strong></p>
<p>Suppose that a random variable <span class="math inline">\(X\)</span> has pdf given by <span class="math inline">\(f(x \mid \theta)\)</span>, and that <span class="math inline">\(T^*(X)\)</span> is such that for every* pair of points <span class="math inline">\((x,y)\)</span>, the ratio of pdfs</p>
<p><span class="math display">\[
\frac{f(y \mid \theta)}{f(x \mid \theta)}
\]</span> does not depend on <span class="math inline">\(\theta\)</span> <strong>if and only if</strong> <span class="math inline">\(T^*(x) = T^*(y)\)</span>. Then <span class="math inline">\(T^*(X)\)</span> is a minimal sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>*every pair of points that have the same support as <span class="math inline">\(X\)</span>.</p>
<details>
<summary>
Proof.
</summary>
<p>We’ll utilize something called a likelihood ratio (literally a ratio of likelihoods) to prove this theorem. We’ll also come back to likelihood ratios later in the Hypothesis Testing chapter!</p>
<p>Let <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> be two possible values of our unknown parameter <span class="math inline">\(\theta\)</span>. Then a likelihood ratio comparing densities evaluated at these two values is defined as</p>
<p><span class="math display">\[
L_{\theta_1, \theta_2}(x) \equiv \frac{f(x \mid \theta_2)}{f(x \mid \theta_1)}
\]</span> Our proof will proceed as follows:</p>
<ol type="1">
<li><p>We’ll show that if <span class="math inline">\(T(X)\)</span> is sufficient, then <span class="math inline">\(L_{\theta_1, \theta_2}(X)\)</span> is a function of <span class="math inline">\(T(X)\)</span> <span class="math inline">\(\forall\)</span> <span class="math inline">\(\theta_1, \theta_2\)</span>.</p></li>
<li><p>We’ll show the converse: If <span class="math inline">\(L_{\theta_1, \theta_2}(X)\)</span> is a function of <span class="math inline">\(T(X)\)</span> <span class="math inline">\(\forall\)</span> <span class="math inline">\(\theta_1, \theta_2\)</span>, then <span class="math inline">\(T(X)\)</span> is sufficient. This combined with (1) will show that <span class="math inline">\(L_{\theta_1, \theta_2}(X)\)</span> is a minimal sufficient statistic.</p></li>
<li><p>We’ll use the above two statements to prove the theorem!</p></li>
</ol>
<p>First, suppose that <span class="math inline">\(T(X)\)</span> is sufficient for <span class="math inline">\(\theta\)</span>. Then, by definition we can write</p>
<p><span class="math display">\[
L_{\theta_1, \theta_2}(x) = \frac{f(x \mid \theta_2)}{f(x \mid \theta_1)} = \frac{g(T(x) \mid \theta_1)h(x)}{g(T(x) \mid \theta_2)h(x)} = \frac{g(T(x) \mid \theta_1)}{g(T(x) \mid \theta_2)}
\]</span> and so <span class="math inline">\(L_{\theta_1, \theta_2}(X)\)</span> is a function of <span class="math inline">\(T(x)\)</span> <span class="math inline">\(\forall\)</span> <span class="math inline">\(\theta_1, \theta_2\)</span>.</p>
<p>Second, assume WLOG that <span class="math inline">\(\theta_1\)</span> is fixed, and denote our unknown parameter <span class="math inline">\(\theta_2 = \theta\)</span>. We can rearrange our likelihood ratio as</p>
<p><span class="math display">\[\begin{align*}
    L_{\theta_1, \theta}(x) &amp; = \frac{f(x \mid \theta)}{f(x \mid \theta_1)} \\
    f(x \mid \theta) &amp; = L_{\theta_1, \theta}(x) f(x \mid \theta_1)
\end{align*}\]</span></p>
<p>and note that <span class="math inline">\(L_{\theta_1, \theta}(x)\)</span> is a function of <span class="math inline">\(T(X)\)</span> by assumption, and <span class="math inline">\(f(x \mid \theta_1)\)</span> is a function of <span class="math inline">\(x\)</span> that does not depend on our unknown parameter <span class="math inline">\(\theta\)</span>. Then <span class="math inline">\(T(X)\)</span> satisfies the factorization criterion, and is therefore sufficient.</p>
<p>Let <span class="math inline">\(T^{**}(X) \equiv L_{\theta_1, \theta_2}(X)\)</span>. Then the first two statements we have shown give us that</p>
<p><span class="math display">\[
T(X) \text{ is sufficient } \iff T^{**}(X) \text{ is a function of } T(X)
\]</span></p>
<p>and therefore <span class="math inline">\(T^{**}(X)\)</span> is a minimal sufficient statistic, by definition.</p>
<p>We’ll now (officially) prove our theorem. By hypothesis of the theorem,</p>
<p><span class="math display">\[\begin{align*}
    T^*(x) = T^*(y) &amp; \iff \frac{f(y \mid \theta)}{f(x \mid \theta)} \text{ is } \theta-free \\
    &amp; \iff \frac{f(y \mid \theta_1)}{f(x \mid \theta_1)} = \frac{f(y \mid \theta_2)}{f(x \mid \theta_2)} \quad \forall \theta_1, \theta_2 \\
    &amp; \iff \frac{f(y \mid \theta_2)}{f(y \mid \theta_1)} = \frac{f(x \mid \theta_2)}{f(x \mid \theta_1)} \quad \forall \theta_1, \theta_2 \\
    &amp; \iff L_{\theta_1, \theta_2}(y) = L_{\theta_1, \theta_2} (x) \quad \forall \theta_1, \theta_2 \\
    &amp; \iff T^{**}(y) = T^{**}(x)
\end{align*}\]</span></p>
<p>Therefore <span class="math inline">\(T^*(X)\)</span> and <span class="math inline">\(T^{**}(X)\)</span> are equivalent. Since <span class="math inline">\(T^{**}(X)\)</span> is a minimal sufficient statistic, <span class="math inline">\(T^*(X)\)</span> is therefore also minimal sufficient.</p>
</details>
<p><strong>Complete, Sufficient, Minimal</strong></p>
<p>If <span class="math inline">\(T(X)\)</span> is complete and sufficient, then <span class="math inline">\(T(X)\)</span> is minimal sufficient.</p>
<details>
<summary>
Proof.
</summary>
<p>Just kidding! Prove it on your own and show it to me, if you want bonus points in my heart :)</p>
</details>
<p><strong>Rao-Blackwell-Lehmann-Scheffe (RBLS)</strong></p>
<p>Let <span class="math inline">\(T(X)\)</span> be a complete and sufficient statistic for unknown parameter <span class="math inline">\(\theta\)</span>, and let <span class="math inline">\(\tau(\theta)\)</span> be some function of <span class="math inline">\(\theta\)</span>. If there exists at least one unbiased estimator <span class="math inline">\(\tilde{\tau}(X)\)</span> for <span class="math inline">\(\tau(\theta)\)</span>, then there exists a <em>unique</em> UMVUE <span class="math inline">\(\hat{\tau}(T(X))\)</span> for <span class="math inline">\(\tau(\theta)\)</span> given by</p>
<p><span class="math display">\[
\hat{\tau}(T(X)) = E[\tilde{\tau}(X) \mid T(X)]
\]</span></p>
<p><em>Why do we care?</em> An important consequence of the RBLS Theorem is that if <span class="math inline">\(T(X)\)</span> is a complete and sufficient statistic for <span class="math inline">\(\theta\)</span>, then any function <span class="math inline">\(\phi(T(X))\)</span> is the UMVUE of its expectation <span class="math inline">\(E[\phi(T(X))]\)</span> (so long as the expectation is finite for all <span class="math inline">\(\theta\)</span>). This Theorem is therefore a <em>very</em> convenient way to find UMVUEs: (1) Find a complete and sufficient statistic for an unknown parameter, and (2) functions of that statistic are then the UMVUE for their expectation!</p>
<details>
<summary>
Proof.
</summary>
<p>To prove RBLS, we first must prove an Improvement Lemma and a Uniqueness Lemma.</p>
<p><em>Improvement Lemma.</em> Suppose that <span class="math inline">\(T(X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(\tilde{\tau}(X)\)</span> is an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, then <span class="math inline">\(E[\tilde{\tau}(X) \mid T(X)]\)</span> does not depend on <span class="math inline">\(\theta\)</span> (by sufficiency) and is also an estimator of <span class="math inline">\(\tau(\theta)\)</span>, which (importantly) has smaller variance than <span class="math inline">\(\tilde{\tau}(X)\)</span>.</p>
<p><em>Proof of Lemma.</em> First, note that <span class="math inline">\(E[\tilde{\tau}(X) \mid T(X)]\)</span> is an unbiased estimator for <span class="math inline">\(\tau(\theta)\)</span>, since <span class="math display">\[\begin{align*}
    E[E[\tilde{\tau}(X) \mid T(X)]] &amp; = E[\tilde{\tau}(X)] \quad \quad \text{(Law of Iterated Expectation)} \\
    &amp; = \tau(\theta) \quad \quad (\tilde{\tau}(X) \text{ is unbiased})
\end{align*}\]</span> Then, <span class="math display">\[\begin{align*}
    Var(\tilde{\tau}(X)) &amp; = E[Var(\tilde{\tau}(X) \mid T(X))] + Var(E[\tilde{\tau}(X) \mid T(X)]) \\
    &amp; \geq Var(E[\tilde{\tau}(X) \mid T(X)])
\end{align*}\]</span> and we’re done! <span class="math inline">\(E[\tilde{\tau}(X) \mid T(X)]\)</span> has a smaller variance than <span class="math inline">\(\tilde{\tau(X)}\)</span>. Since both are unbiased, this is considered an “improvement” (hence the name of the Lemma).</p>
<p><em>Uniqueness Lemma.</em> If <span class="math inline">\(T(X)\)</span> is complete, then for some unknown parameter <span class="math inline">\(\theta\)</span> and function of it <span class="math inline">\(\tau(\theta)\)</span>, <span class="math inline">\(\tau(\theta)\)</span> has at most <em>one</em> unbiased estimator <span class="math inline">\(\hat{\tau}(T(X))\)</span> <em>that depends on</em> <span class="math inline">\(T(X)\)</span>.</p>
<p><em>Proof of Lemma.</em> Suppose, toward contradiction, that <span class="math inline">\(\tau(\theta)\)</span> has more than one unbiased estimator that depends on <span class="math inline">\(T(X)\)</span>, given by <span class="math inline">\(\tilde{\tau}(T(X))\)</span> and <span class="math inline">\(\hat{\tau}(T(X))\)</span>, <span class="math inline">\(\tilde{\tau}(T(X)) \neq \hat{\tau}(T(X))\)</span>. Then</p>
<p><span class="math display">\[
E[\tilde{\tau}(T(X)) - \hat{\tau}(T(X))] = \tau(\theta) - \tau(\theta) = 0 \quad \forall \theta
\]</span> Let <span class="math inline">\(g(T(X)) = \tilde{\tau}(T(X)) - \hat{\tau}(T(X))\)</span>. Since <span class="math inline">\(T(X)\)</span> is complete, and <span class="math inline">\(E[g(T(X))] = 0\)</span>, this implies <span class="math inline">\(\tilde{\tau}(T(X)) - \hat{\tau}(T(X)) = 0\)</span>, which means <span class="math inline">\(\tilde{\tau}(T(X)) = \hat{\tau}(T(X))\)</span>. Contradiction.</p>
<p><em>Back to the proof of RBLS</em>.</p>
<p>We’ve shown previously that <span class="math inline">\(\hat{\tau}(T(X))\)</span> is an unbiased estimator for <span class="math inline">\(\tau{\theta}\)</span> (law of iterated expectation). Let <span class="math inline">\(\tau_1(X)\)</span> be any other unbiased estimator for <span class="math inline">\(\tau(\theta)\)</span>, and let <span class="math inline">\(\tau_2(T(X)) = E[\tau_1(X) \mid T(X)]\)</span>. Then <span class="math inline">\(\tau_2(T(X))\)</span> is also unbiased for <span class="math inline">\(\tau(\theta)\)</span> (again, iterated expectation), and by the Uniqueness Lemma (since <span class="math inline">\(T\)</span> is complete by supposition), <span class="math inline">\(\hat{\tau}(T(X)) = \tau_2(T(X))\)</span>. But,</p>
<p><span class="math display">\[\begin{align*}
    Var(\hat{\tau}(T(X))) &amp; = Var(\tau_2(T(X))) \quad \quad (\hat{\tau} = \tau_2) \\
    &amp; \leq Var(\tau_1(T(X))) \quad \quad \text{(Improvement Lemma)}
\end{align*}\]</span> so <span class="math inline">\(\hat{\tau}(T(X))\)</span> is the UMVUE for <span class="math inline">\(\tau(\theta)\)</span>, as desired.</p>
</details>
<p><strong>Cramér-Rao Lower Bound</strong></p>
<p>Let <span class="math inline">\(f_Y(y \mid \theta)\)</span> be a pdf with nice* conditions, and let <span class="math inline">\(Y_1, \dots, Y_n\)</span> be a random sample from <span class="math inline">\(f_Y(y \mid \theta)\)</span>. Let <span class="math inline">\(\hat{\theta}\)</span> be <em>any</em> unbiased estimator of <span class="math inline">\(\theta\)</span>. Then</p>
<p><span class="math display">\[\begin{align*}
Var(\hat{\theta}) &amp; \geq \left\{ E\left[ \left( \frac{\partial \log( L(\theta \mid y))}{\partial \theta}\right)^2\right]\right\}^{-1} \\
&amp; = -\left\{ E\left[ \frac{\partial^2 \log( L(\theta \mid y))}{\partial \theta^2} \right] \right\}^{-1} \\
&amp; = \frac{1}{I(\theta)}
\end{align*}\]</span></p>
<p>*our nice conditions that we need are that <span class="math inline">\(f_Y(y \mid \theta)\)</span> has continuous first- and second-order derivatives, which would quickly discover we need by looking at the form for the C-R lower bound, and that the set of values <span class="math inline">\(y\)</span> where <span class="math inline">\(f_Y(y \mid \theta) \neq 0\)</span> does not depend on <span class="math inline">\(\theta\)</span>. If you are familiar with the concept of the “support” of a function, that is where this second condition comes from. The key here is that this condition allows to interchange derivatives and integrals, in particular, <span class="math inline">\(\frac{\partial}{\partial \theta} \int f(x) dx = \int \frac{\partial}{\partial \theta} f(x)dx\)</span>, which we’ll need to complete the proof.</p>
<details>
<summary>
Proof.
</summary>
<p>Let <span class="math inline">\(X = \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta}\)</span>. By the Covariance Inequality,</p>
<p><span class="math display">\[
Var(\hat{\theta}) \geq \frac{Cov(\hat{\theta},X)^2}{Var(X)}
\]</span></p>
<p>and so if we can show</p>
<p><span class="math display">\[\begin{align*}
\frac{Cov(\hat{\theta},X)^2}{Var(X)} &amp; = \left\{ E\left[ \left( \frac{\partial \log( L(\theta \mid \textbf{y}))}{\partial \theta}\right)^2\right]\right\}^{-1}  \\
&amp; = \frac{1}{I(\theta)}
\end{align*}\]</span></p>
<p>then we’re done, as this is the C-R lower bound. Note first that</p>
<p><span class="math display">\[\begin{align*}
E[X] &amp; = \int x f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right)  f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \left( \frac{\partial \log f_Y(\textbf{y} \mid \theta)}{\partial \theta} \right)  f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \frac{\frac{\partial}{\partial \theta} f_Y(\textbf{y} \mid \theta)}{ f_Y(\textbf{y} \mid \theta)} f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \int \frac{\partial}{\partial \theta} f_Y (\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \frac{\partial}{\partial \theta} \int f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
&amp; = \frac{\partial}{\partial \theta} 1 \\
&amp; = 0
\end{align*}\]</span></p>
<p>This means that</p>
<p><span class="math display">\[\begin{align*}
    Var[X] &amp; = E[X^2] - E[X]^2 \\
    &amp; = E[X^2] \\
    &amp; = E \left[ \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right)^2\right ]
\end{align*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
    Cov(\hat{\theta}, X) &amp; = E[\hat{\theta} X] - E[\hat{\theta}] E[X] \\
    &amp; = E[\hat{\theta}X] \\
    &amp; = \int \hat{\theta} x f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right) f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \left( \frac{\partial \log f_Y(\textbf{y} \mid \theta)}{\partial \theta} \right) f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \frac{\frac{\partial}{\partial \theta} f_Y(\textbf{y} \mid \theta)}{ f_Y(\textbf{y} \mid \theta)} f_Y(\textbf{y} \mid \theta) d\textbf{y} \\
    &amp; = \int \hat{\theta} \frac{\partial}{\partial \theta} f_Y(\textbf{y} \mid \theta) d\textbf{y}  \\
    &amp; = \frac{\partial}{\partial \theta} \int \hat{\theta} f_Y(\textbf{y} \mid \theta) d\textbf{y}  \\
    &amp; =  \frac{\partial}{\partial \theta} E[\hat{\theta}] \\
    &amp; = \frac{\partial}{\partial \theta} \theta \\
    &amp; = 1
\end{align*}\]</span></p>
<p>where <span class="math inline">\(E[\hat{\theta}] = \theta\)</span> since our estimator is unbiased. Putting this all together, we have</p>
<p><span class="math display">\[\begin{align*}
    Var[\hat{\theta}] &amp; \geq \frac{Cov(\hat{\theta},X)^2}{Var(X)} \\
    &amp; = \frac{1^2}{E \left[ \left( \frac{\partial \log L(\theta \mid \textbf{y})}{\partial \theta} \right)^2\right ]} \\
    &amp; = \frac{1}{I(\theta)}
\end{align*}\]</span></p>
<p>as desired.</p>
</details>
<p><strong>Comment:</strong> Note that what the Cramér-Rao lower bound tells us is that, if the variance of an unbiased estimator is <em>equal</em> to the Cramér-Rao lower bound, then that estimator has the <em>minimum possible variance</em> among all unbiased estimators there could possibly be. This allows us to <em>prove</em>, for example, whether or not an unbiased estimator is the UMVUE: If an unbiased estimator’s variance achieves the C-R lower bound, then it is <em>optimal</em> according to the UMVUE criterion.</p>
</section>
<section id="worked-examples" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="worked-examples"><span class="header-section-number">4.5</span> Worked Examples</h2>
<p><strong>Problem 1:</strong> Suppose <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} Exponential(1/\theta)\)</span>. Compute the MLE of <span class="math inline">\(\theta\)</span>, and show that it is an unbiased estimator of <span class="math inline">\(\theta\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>Note that we can write</p>
<p><span class="math display">\[\begin{align*}
    L(\theta) &amp; = \prod_{i = 1}^n \frac{1}{\theta} e^{-x_i / \theta} \\
    \log L(\theta) &amp; = \sum_{i = 1}^n \log(\frac{1}{\theta} e^{-x_i / \theta}) \\
    &amp; = \sum_{i = 1}^n  \log(\frac{1}{\theta}) - \sum_{i = 1}^n x_i / \theta \\
    &amp; = -n \log(\theta) - \frac{1}{\theta} \sum_{i = 1}^n x_i \\
    \frac{\partial}{\partial \theta} \log L(\theta) &amp; = \frac{\partial}{\partial \theta}  \left( -n \log(\theta) - \frac{1}{\theta} \sum_{i = 1}^n x_i \right) \\
    &amp; = -\frac{n}{\theta} + \frac{\sum_{i = 1}^n x_i }{\theta^2}
\end{align*}\]</span></p>
<p>Setting this equal to <span class="math inline">\(0\)</span> and solving for <span class="math inline">\(\theta\)</span> we obtain</p>
<p><span class="math display">\[\begin{align*}
    0 &amp; \equiv -\frac{n}{\theta} + \frac{\sum_{i = 1}^n x_i }{\theta^2}  \\
    \frac{n}{\theta} &amp; = \frac{\sum_{i = 1}^n x_i }{\theta^2} \\
    n &amp; = \frac{\sum_{i = 1}^n x_i }{\theta} \\
    \theta &amp; = \frac{1}{n} \sum_{i = 1}^n x_i
\end{align*}\]</span></p>
<p>and so the MLE for <span class="math inline">\(\theta\)</span> is the sample mean. To show that the MLE is unbiased, we note that</p>
<p><span class="math display">\[\begin{align*}
    E \left[ \frac{1}{n} \sum_{i = 1}^n X_i \right] &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i] = \frac{1}{n} \sum_{i = 1}^n \theta  = \theta
\end{align*}\]</span></p>
<p>as desired.</p>
</details>
<p><strong>Problem 2:</strong> Suppose again that <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} Exponential(1/\theta)\)</span>. Let <span class="math inline">\(\hat{\theta}_2 = Y_1\)</span>, and <span class="math inline">\(\hat{\theta}_3 = nY_{(1)}\)</span>. Show that <span class="math inline">\(\hat{\theta}_2\)</span> and <span class="math inline">\(\hat{\theta}_3\)</span> are unbiased estimators of <span class="math inline">\(\theta\)</span>. Hint: use the fact that <span class="math inline">\(Y_{(1)} \sim Exponential(n/\theta)\)</span></p>
<details>
<summary>
Solution:
</summary>
<p>Note that the mean of a random variable <span class="math inline">\(Y \sim Exponential(\lambda)\)</span> is given by <span class="math inline">\(1/\lambda\)</span>. Then we can write</p>
<p><span class="math display">\[
E[\hat{\theta}_2] = E[Y_1] = \frac{1}{1/\theta} = \theta
\]</span></p>
<p>and</p>
<p><span class="math display">\[
E[\hat{\theta}_3] = E[nY_{(1)}] = \frac{n}{n/\theta} = \theta
\]</span> as desired.</p>
</details>
<p><strong>Problem 3:</strong> Compare the variance of the estimators from Problems 1 and 2. Which is most efficient?</p>
<details>
<summary>
Solution:
</summary>
<p>Recall that the variance of a random variable <span class="math inline">\(Y \sim Exponential(\lambda)\)</span> is given by <span class="math inline">\(1/\lambda^2\)</span>. Let the MLE from Problem 1 be denoted <span class="math inline">\(\hat{\theta}_1 = \bar{X}\)</span>. Then we can write</p>
<p><span class="math display">\[
Var\left[\hat{\theta}_1\right] = Var\left[\frac{1}{n} \sum_{i = 1}^n X_i\right] = \frac{1}{n^2} \sum_{i = 1}^n Var[X_i] = \frac{1}{n^2} \left( \frac{n}{(1/\theta)^2} \right) = \frac{\theta^2}{n}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Var\left[\hat{\theta}_2\right] = Var[Y_1] = \frac{1}{(1/\theta)^2} = \theta^2
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Var\left[\hat{\theta}_3\right] = Var[nY_{(1)}] = n^2 Var[Y_{(1)}] = \frac{n^2}{(n/\theta)^2} = \theta^2
\]</span></p>
<p>Thus, the variance of the MLE, <span class="math inline">\(\hat{\theta}_1\)</span>, is most efficient, and is <span class="math inline">\(n\)</span> times smaller than the variance of both <span class="math inline">\(\hat{\theta}_2\)</span> and <span class="math inline">\(\hat{\theta}_3\)</span>.</p>
</details>
<p><strong>Problem 4:</strong> Suppose <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>. Show that the estimator <span class="math inline">\(\hat{\mu} = \frac{1}{n} \sum_{i = 1}^n X_i\)</span> <em>and</em> the estimator <span class="math inline">\(\hat{\mu}_w = \sum_{i = 1}^n w_i X_i\)</span> are both unbiased estimators of <span class="math inline">\(\mu\)</span>, where <span class="math inline">\(\sum_{i = 1}^n w_i = 1\)</span>.</p>
<details>
<summary>
Solution:
</summary>
<p>We can write</p>
<p><span class="math display">\[
E[\hat{\mu}] = E\left[ \frac{1}{n} \sum_{i = 1}^n X_i \right] = \frac{1}{n}\sum_{i = 1}^n E[X_i] = \frac{1}{n}\sum_{i = 1}^n \mu = \mu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
E[\hat{\mu}_w] = E \left[ \sum_{i = 1}^n w_i X_i \right] = \sum_{i = 1}^n w_i E \left[ X_i \right] = \sum_{i = 1}^n w_i \mu = \mu \sum_{i = 1}^n w_i = \mu
\]</span></p>
<p>as desired.</p>
</details>
<p><strong>Problem 5:</strong> Determine whether the estimator <span class="math inline">\(\hat{\mu}\)</span> or <span class="math inline">\(\hat{\mu}_w\)</span> is more efficient, in Problem 4, if we additionally impose the constraint <span class="math inline">\(w_i \geq 0\)</span> <span class="math inline">\(\forall i\)</span>. (Hint: use the Cauchy-Schwarz inequality)</p>
<details>
<summary>
Solution:
</summary>
<p>To determine relative efficiency, we must compute the variance of each estimator. We have</p>
<p><span class="math display">\[
Var[\hat{\mu}] = Var \left[ \frac{1}{n} \sum_{i = 1}^n X_i \right] = \frac{1}{n^2} \sum_{i = 1}^n Var[X_i] = \frac{1}{n^2} \sum_{i = 1}^n \sigma^2 = \sigma^2 / n
\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align*}
    Var[\hat{\mu}_w] &amp; =  Var \left[ \sum_{i = 1}^n w_i X_i \right] \\
    &amp; = \sum_{i = 1}^n Var[w_i X_i] \\
    &amp; = \sum_{i = 1}^n w_i^2 Var[X_i] \\
    &amp; = \sum_{i = 1}^n w_i^2  \sigma^2 \\
    &amp; = \sigma^2 \sum_{i = 1}^n w_i^2
\end{align*}\]</span></p>
<p>And so to determine which estimator is more efficient, we need to determine if <span class="math inline">\(\frac{1}{n}\)</span> is less than <span class="math inline">\(\sum_{i = 1}^n w_i^2\)</span> (or not). The Cauchy-Schwarz inequality tells us that</p>
<p><span class="math display">\[\begin{align*}
    \left( \sum_{i = 1}^n w_i \cdot 1\right)^2 &amp; \leq \left( \sum_{i = 1}^n w_i^2 \right) \left( \sum_{i = 1}^n 1^2 \right) \\
    \left( \sum_{i = 1}^n w_i \right)^2 &amp; \leq \left( \sum_{i = 1}^n w_i^2 \right) n \\
    1 &amp; \leq \left( \sum_{i = 1}^n w_i^2 \right) n  \\
    \frac{1}{n} &amp; \leq \sum_{i = 1}^n w_i^2
\end{align*}\]</span></p>
<p>and therefore, <span class="math inline">\(\hat{\mu}\)</span> is more efficient than <span class="math inline">\(\hat{\mu}_w\)</span>.</p>
</details>
<p><strong>Problem 6:</strong> Suppose <span class="math inline">\(X_1, \dots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>. Show that the MLE for <span class="math inline">\(\sigma^2\)</span> is <em>biased</em>, and suggest a modified variance estimator for <span class="math inline">\(\sigma^2\)</span> that is <em>unbiased</em>.</p>
<details>
<summary>
Solution:
</summary>
<p>Recall that the MLE for <span class="math inline">\(\sigma^2\)</span> is given by <span class="math inline">\(\frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\)</span>. Then</p>
<p><span class="math display">\[\begin{align*}
    E\left[ \frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\right] &amp; = \frac{1}{n} \sum_{i = 1}^n E\left[ (X_i - \bar{X})^2\right] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E\left[ X_i^2 - 2X_i \bar{X} + \bar{X}^2\right] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - 2 E\left[ \frac{1}{n} \sum_{i = 1}^n X_i \bar{X} \right] + E[\bar{X}^2] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - 2 E\left[ \bar{X} \frac{1}{n} \sum_{i = 1}^n X_i  \right] + E[\bar{X}^2] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - 2 E\left[ \bar{X}^2  \right] + E[\bar{X}^2] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - E\left[ \bar{X}^2  \right]
\end{align*}\]</span></p>
<p>Recall that since <span class="math inline">\(X_i \overset{iid}{\sim} N(\mu, \sigma^2)\)</span>, <span class="math inline">\(\bar{X} \sim N(\mu, \sigma^2/n)\)</span>, and that we can write <span class="math inline">\(Var[Y] + E[Y]^2 = E[Y^2]\)</span> (definition of variance). Then we can write</p>
<p><span class="math display">\[\begin{align*}
    E\left[ \frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2 \right] &amp; = \frac{1}{n} \sum_{i = 1}^n E[X_i^2] - E\left[ \bar{X}^2  \right] \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n \left( \sigma^2 + \mu^2 \right) - \left( \frac{\sigma^2}{n} + \mu^2 \right) \\
    &amp; = \sigma^2 + \mu^2 - \frac{\sigma^2}{n} - \mu^2  \\
    &amp; = \sigma^2 - \frac{\sigma^2}{n} \\
    &amp; = \sigma^2 \left( 1 - \frac{1}{n} \right) \\
    &amp; = \sigma^2  \left( \frac{n-1}{n} \right)
\end{align*}\]</span></p>
<p>Therefore, since <span class="math inline">\(E[\hat{\sigma}^2_{MLE}] \neq \sigma^2\)</span>, the MLE is unbiased. Note that</p>
<p><span class="math display">\[\begin{align*}
    E\left[ \left( \frac{n}{n-1} \right)\frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\right] &amp; = \left( \frac{n}{n-1} \right) \left( \frac{n-1}{n} \right) \sigma^2   \\
    &amp; = \sigma^2
\end{align*}\]</span></p>
<p>and so the estimator <span class="math inline">\(\frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X})^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>. This estimator is often called the “sample variance”, and is denoted by <span class="math inline">\(S^2\)</span>.</p>
</details>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/math-stat-355\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./mom.html" class="pagination-link" aria-label="Method of Moments">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method of Moments</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./consistency.html" class="pagination-link" aria-label="Consistency">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Consistency</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>