[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH/STAT 455: Mathematical Statistics",
    "section": "",
    "text": "Welcome to Mathematical Statistics!\nThis book contains the course notes for MATH/STAT 455: Mathematical Statistics at Macalester College, as taught by Prof. Taylor Okonek. These notes draw from course notes created by Prof. Kelsey Grinde, and heavily from the course textbook, An Introduction to Mathematical Statistics and Its Applications by Richard Larsen and Morris Marx (6th Edition). Each chapter will contain (at a minimum): Learning Goals, Associated Readings, Definitions, Theorems, and Worked Examples.\nI will be editing and adding to these notes throughout Spring 2024, so please check consistently for updates!\nIf you find any typos or have other questions, please email tokonek@macalester.edu."
  },
  {
    "objectID": "probability.html#learning-objectives",
    "href": "probability.html#learning-objectives",
    "title": "1  Probability: A Brief Review",
    "section": "1.1 Learning Objectives",
    "text": "1.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDistinguish between important probability models (e.g., Normal, Binomial)\nDerive the expectation and variance of a single random variable or a sum of random variables\nDefine the moment generating function and use it to find moments or identify pdfs"
  },
  {
    "objectID": "probability.html#associated-readings",
    "href": "probability.html#associated-readings",
    "title": "1  Probability: A Brief Review",
    "section": "1.2 Associated Readings",
    "text": "1.2 Associated Readings\nChapters 2-4 (pages 15-277)"
  },
  {
    "objectID": "probability.html#definitions",
    "href": "probability.html#definitions",
    "title": "1  Probability: A Brief Review",
    "section": "1.3 Definitions",
    "text": "1.3 Definitions\nYou are expected to know the following definitions:\n\nProbability density function (discrete, continuous)\n\nNote: I don’t care if you call a pmf a pdf… I will probably do this continuously throughout the semester. We don’t need to be picky about this in MATH/STAT 455.\n\nCumulative distribution function (discrete, continuous)\nJoint probability density function\nConditional probability density function\nIndependence\nRandom Variable\nExpected Value / Expectation\nVariance\n\\(r^{th}\\) moment\nCovariance\nRandom Sample\nMoment Generating Function\n\nYou are expected to know the following probability distributions:\n\nTable of main probability distributions we will work with for MATH/STAT 455.\n\n\n\n\n\n\n\nDistribution\nPDF/PMF\nParameters\n\n\n\n\nUniform\n\\(\\pi(x) = \\frac{1}{\\beta - \\alpha}\\)\n\\(\\alpha \\in \\mathbb{R}\\), \\(\\beta\\in \\mathbb{R}\\)\n\n\nNormal\n\\(\\pi(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2)\\)\n\\(\\mu \\in \\mathbb{R}\\), \\(\\sigma &gt; 0\\)\n\n\nMultivariate Normal\n\\(\\pi(\\textbf{x}) - (2\\pi)^{-k/2} |\\Sigma|^{-1/2} \\exp(-\\frac{1}{2}(\\textbf{x} - \\mu)^\\top \\Sigma^{-1}(\\textbf{x} - \\mu)))\\)\n\\(\\mu \\in \\mathbb{R}^k\\), \\(\\Sigma \\in \\mathbb{R}^{k\\times k}\\) , positive semi-definite\n\n\nGamma\n\\(\\pi(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\\)\n\\(\\alpha \\text{ (shape)}, \\beta \\text{ (rate)} &gt; 0\\)\n\n\nChi-square\n\\(\\pi(x) = \\frac{2^{-\\nu/2}}{\\Gamma(\\nu/2)} x^{\\nu/2 - 1}e^{-x/2})\\)\n\\(\\nu &gt; 0\\)\n\n\nExponential\n\\(\\pi(x) = \\beta e^{-\\beta x}\\)\n\\(\\beta &gt; 0\\)\n\n\nStudent-$t$\n\\(\\pi(x) = \\frac{\\Gamma((\\nu + 1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu \\pi}} (1 + \\frac{x^2}{\\nu})^{-(\\nu + 1)/2}\\)\n\\(\\nu &gt; 0\\)\n\n\nBeta\n\\(\\pi(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha - 1}(1 - x)^{\\beta - 1}\\)\n\\(\\alpha, \\beta &gt; 0\\)\n\n\nPoisson\n\\(\\pi(x) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\n\\(\\lambda &gt; 0\\)\n\n\nBinomial\n\\(\\pi(x) = \\binom{n}{x} p^{x} (1 - p)^{n - x}\\)\n\\(p \\in [0,1], n = \\{0, 1, 2, \\dots\\}\\)\n\n\nMultinomial\n\\(\\pi(\\textbf{x}) = \\frac{n!}{x_1! \\dots x_k!} p_1^{x_1} \\dots p_k^{x_k}\\)\n\\(p_i &gt; 0\\), \\(p_1 + \\dots + p_k = 1\\), \\(n = \\{0, 1, 2, \\dots \\}\\)\n\n\nNegative Binomial\n\\(\\pi(x) = \\binom{k + r - 1}{k} (1-p)^k p^r\\)\n\\(r &gt; 0\\), \\(p \\in [0,1]\\)"
  },
  {
    "objectID": "probability.html#theorems",
    "href": "probability.html#theorems",
    "title": "1  Probability: A Brief Review",
    "section": "1.4 Theorems",
    "text": "1.4 Theorems\n\nLaw of Total Probability\n\\[\nP(A) = \\sum_n P(A \\cap B_n),\n\\]or\n\\[\nP(A) = \\sum_n P(A \\mid B_n) P(B_n)\n\\]\nBayes’ Theorem\n\\[\n\\pi(A \\mid B) = \\frac{\\pi(B \\mid A) \\pi(A)}{\\pi(B)}\n\\]\nRelationship between pdf and cdf\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t)dt\n\\]\n\\[\n\\frac{\\partial}{\\partial y}F_Y(y) = f_Y(y)\n\\]\nExpectation of random variables\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\n\\[\nE[X^2] = \\int_{-\\infty}^\\infty x^2 f(x) dx\n\\]\nExpectation and variance of linear transformations of random variables\n\\[\nE[cX + b] = c E[X] + b\n\\]\n\\[\nVar[cX + b] = c^2 Var[X]\n\\]\nRelationship between mean and variance\n\\[\nVar[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nAlso, recall that \\(Cov[X, X] = Var[X]\\).\nFinding a marginal pdf from a joint pdf\n\\[\nf_X(x) = \\int_{-\\infty}^\\infty f_{XY}(x, y) dy\n\\]\nIndependence of random variables and joint pdfs\nIf two random variables are independent, their joint pdf will be separable. For example, if \\(X\\) and \\(Y\\) are independent, we could write\n\\[\nf_{XY}(x, y) = f_{X}(x)f_Y(y)\n\\]\nExpected value of a product of independent random variables\nSuppose random variables \\(X_1, \\dots, X_n\\) are independent. Then we can write,\n\\[\nE\\left[\\prod_{i = 1}^n X_i\\right] = \\prod_{i = 1}^n E[X_i]\n\\]\nCovariance of independent random variables\nIf \\(X\\) and \\(Y\\) are independent, then \\(Cov(X, Y) = 0\\). We can show this by noting that\n\n\\[\n\\begin{align}\nCov(X, Y) & = E[(X - E[X])(Y - E[Y])] \\\\\n& = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\\\\n& = E[XY] - E[XE[Y]] - E[YE[X]] + E[X]E[Y] \\\\\n& =  2E[X]E[Y] - 2E[X]E[Y] \\\\\n& = 0\n\\end{align}\n\\]\n\nUsing MGFs to find moments\nRecall that the moment generating function of a random variable \\(X\\), denoted by \\(M_X(t)\\) is\n\\[\nM_X(t) = E[e^{tX}]\n\\]\nThen the \\(n\\)th moment of the probability distribution for \\(X\\) , \\(E[X^n]\\), is given by\n\\[\n\\frac{\\partial M_X}{\\partial t^n} \\Bigg|_{t = 0}\n\\]\nwhere the above reads as “the \\(n\\)th derivative of the moment generating function, evaluated at \\(t = 0\\).”\nUsing MGFs to identify pdfs\nMGFs uniquely identify probability density functions. If \\(X\\) and \\(Y\\) are two random variables where for all values of \\(t\\), \\(M_X(t) = M_Y(t)\\), then \\(F_X(x) = F_Y(y)\\).\nCentral Limit Theorem\nThe classical CLT states that for independent and identically distributed (iid) random variables \\(X_1, \\dots, X_n\\), with expected value \\(E[X_i] = \\mu\\) and \\(Var[X_i] = \\sigma^2 &lt; \\infty\\), the sample average (centered and standardized) converges in distribution to a standard normal distribution at a root-\\(n\\) rate. Notationally, this is written as\n\\[\n\\sqrt{n} (\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\n A fun aside: this is only one CLT, often referred to as the Levy CLT. There are other CLTs, such as the Lyapunov CLT and Lindeberg-Feller CLT!"
  },
  {
    "objectID": "probability.html#worked-examples",
    "href": "probability.html#worked-examples",
    "title": "1  Probability: A Brief Review",
    "section": "1.5 Worked Examples",
    "text": "1.5 Worked Examples\nProblem 1: Suppose \\(X \\sim Exponential(\\lambda)\\). Calculate \\(E[X]\\) and \\(Var[X]\\).\nProblem 2: Show that an exponentially distributed random variable is “memoryless”, i.e. show that \\(\\Pr(X &gt; s + x \\mid X &gt; s) = \\Pr(X &gt; x)\\), \\(\\forall s, t\\geq 0\\).\nProblem 3: Suppose we have two, independent random variables \\(X, Y \\sim Exponential(\\lambda).\\) Show that \\(\\frac{X}{X + Y} \\sim Uniform(0,1)\\).\nProblem 4:"
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "2  Maximum Likelihood Estimation",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "mom.html",
    "href": "mom.html",
    "title": "3  Method of Moments",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "properties.html",
    "href": "properties.html",
    "title": "4  Properties of Estimators",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "consistency.html",
    "href": "consistency.html",
    "title": "5  Consistency",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "asymptotics.html",
    "href": "asymptotics.html",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "",
    "text": "Insert text here"
  },
  {
    "objectID": "computation.html",
    "href": "computation.html",
    "title": "7  Computational Optimization",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "8  Bayesian Inference",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "decision.html",
    "href": "decision.html",
    "title": "9  Decision Theory",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "10  Hypothesis Testing",
    "section": "",
    "text": "Insert text here\n\n1 + 1\n\n[1] 2"
  }
]