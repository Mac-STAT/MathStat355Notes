[
  {
    "objectID": "mle.html#learning-objectives",
    "href": "mle.html#learning-objectives",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive maximum likelihood estimators for parameters of common probability density functions\nCalculate maximum likelihood estimators “by hand” for common probability density functions\nExplain (in plain English) why maximum likelihood estimation is an intuitive approach to estimating unknown parameters using a combination of (1) observed data, and (2) a distributional assumption"
  },
  {
    "objectID": "mle.html#reading-guide",
    "href": "mle.html#reading-guide",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.2 Reading Guide",
    "text": "2.2 Reading Guide\nAssociated Readings: Chapter 5 (Introduction through Example 5.2.5)\n\n2.2.1 Reading Questions\n\nWhat is the intuition behind the maximum likelihood estimation (MLE) approach?\nWhat are the typical steps to find a MLE? (see Ex 5.2.1, 5.2.2, and Case Study 5.2.1; work through at least one of these examples in detail, filling in any steps that the textbook left out)\nAre there ever situations when the typical steps to finding a MLE don’t work? If so, what can we do instead to find the MLE? (see Ex 5.2.3, 5.2.4)\nHow do the steps to finding a MLE change when we have more than one unknown parameter? (see Ex 5.2.5)"
  },
  {
    "objectID": "mle.html#definitions",
    "href": "mle.html#definitions",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.3 Definitions",
    "text": "2.3 Definitions\nYou are expected to know the following definitions:\nParameter\nIn a frequentist* framework, a parameter is a fixed, unknown truth (very philosophical). By fixed, I mean “not random”. We assume that there is some true unknown value, governing the generation of all possible random observations of all possible people and things in the whole world. We sometimes call this unknown governing process the “superpopulation” (think: all who ever have been, all who are, and all who ever will be).\nPractically speaking, parameters are things that we want to estimate, and we will estimate them using observed data!\n*Two main schools of thought in statistics are: (1) Frequentist (everything you’ve ever learned so far in statistics, realistically), and (2) Bayesian. We’ll cover the latter, and differences between the two, in a later chapter. There’s also technically Fiducial inference as a third school of thought, but that one’s never been widely accepted.\nStatistic/Estimator\nA statistic (or “estimator”) is a function of your data, used to “estimate” an unknown parameter. Often, statistics/estimators will be functions of means or averages, as we’ll see in the worked examples for this chapter!\nLikelihood Function\nLet \\(x_1, \\dots, x_n\\) be a sample of size \\(n\\) of independent observations from the probability density function \\(f_X(x \\mid \\boldsymbol{\\theta})\\), where \\(\\boldsymbol{\\theta}\\) is a set of unknown parameters that define the pdf. Then the likelihood function \\(L(\\boldsymbol{\\theta})\\) is the product of the pdf evaluated at each \\(x_i\\),\n\\[\nL(\\boldsymbol{\\theta}) = \\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta}).\n\\]\nNote that this looks exactly like the joint pdf for \\(n\\) independent random variables, but it is interpreted differently. A likelihood is a function of parameters, given a set of observations (random variables). A joint pdf is a function of random variables.\nNote: The likelihood function is one of the reasons why we like independent observations so much! If observations aren’t independent, we can’t simply multiply all of their pdfs together to get a likelihood function.\nMaximum Likelihood Estimate (MLE)\nLet \\(L(\\boldsymbol{\\theta}) = \\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta})\\) be the likelihood function corresponding to a random sample of observations \\(x_1, \\dots, x_n\\). If \\(\\boldsymbol{\\theta}_e\\) is such that \\(L(\\boldsymbol{\\theta}_e) \\geq L(\\boldsymbol{\\theta})\\) for all possible values \\(\\boldsymbol{\\theta}\\), then \\(\\boldsymbol{\\theta}_e\\) is called a maximum likelihood estimate for \\(\\boldsymbol{\\theta}\\).\nLog-likelihood\nIn statistics, when we say “log,” we essentially always mean “ln” (or, natural log). The log-likelihood is then, hopefully unsurprisingly, given by \\(\\log(L(\\boldsymbol{\\theta}))\\). One thing that’s useful to note (and will come in handy when calculating MLEs, is that the log of a product is equal to a sum of logs. For likelihoods, that means\n\\[\n\\log(L(\\boldsymbol{\\theta})) = \\log \\left(\\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta})\\right) = \\sum_{i = 1}^n \\log(f_X(x_i \\mid \\boldsymbol{\\theta}))\n\\]\nThis will end up making it much easier to take derivatives than needing to deal with products!\nOrder Statistic\nThe \\(k\\)th order statistic is equal to a sample’s \\(k\\)th smallest value. Practically speaking, there are essentially three order statistics we typically care about: the minimum, the median, and the maximum. We denote the minimum (or, first order statistic) in a sample of random variables \\(X_1, \\dots, X_n\\) as \\(X_{(1)}\\) , the maximum as \\(X_{(n)}\\), and the median \\(X_{(m+1)}\\) where \\(n = 2m + 1\\) when \\(n\\) is odd. Note that median is in fact not an order statistic if \\(n\\) is even (since the median is an average of two values, \\(X_{(m)}\\) and \\(X_{(m+1)}\\), in this case.\nSee Example 5.2.4 in the Textbook for an example of where order statistics occasionally come into play when calculating maximum likelihood estimates."
  },
  {
    "objectID": "mle.html#theorems",
    "href": "mle.html#theorems",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.4 Theorems",
    "text": "2.4 Theorems\nNone for this chapter!"
  },
  {
    "objectID": "mle.html#worked-examples",
    "href": "mle.html#worked-examples",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.5 Worked Examples",
    "text": "2.5 Worked Examples\nProblem 1: Suppose we observe \\(n\\) independent observations \\(X_1, \\dots, X_n \\sim Bernoulli(p)\\), where \\(f_X(x) = p^x(1-p)^{1-x}\\). Find the MLE of \\(p\\).\nWe can write the likelihood function as\n\\[\nL(p) = \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i}\n\\]\nThen the log-likelihood is given by\n\\[\\begin{align*}\n\\log(L(p)) & = \\log \\left[ \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\right] \\\\\n& = \\sum_{i = 1}^n \\log \\left[p^{x_i} (1-p)^{1 - x_i} \\right] \\\\\n& = \\sum_{i = 1}^n \\left[ \\log(p^{x_i}) + \\log((1-p)^{1-x_i}) \\right] \\\\\n& = \\sum_{i = 1}^n \\left[ x_i \\log(p) + (1 - x_i) \\log(1-p) \\right] \\\\\n& = \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p) \\sum_{i = 1}^n (1 - x_i)  \\\\\n& = \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p)  (n - \\sum_{i = 1}^n x_i)\n\\end{align*}\\]\nWe can take the derivative of the log-likelihood with respect to \\(p\\), and set it equal to zero…\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial p} \\log(L(p)) & = \\frac{\\partial}{\\partial p} \\left[ \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p)  (n - \\sum_{i = 1}^n x_i) \\right] \\\\\n& = \\frac{\\sum_{i = 1}^n x_i }{p} - \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n0 & \\equiv \\frac{\\sum_{i = 1}^n x_i }{p} - \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n\\frac{\\sum_{i = 1}^n x_i }{p}  & = \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n(1-p) \\sum_{i = 1}^n x_i & = p (n - \\sum_{i = 1}^n x_i) \\\\\n\\sum_{i = 1}^n x_i - p\\sum_{i = 1}^n x_i & = pn - p \\sum_{i = 1}^n x_i \\\\\n\\sum_{i = 1}^n x_i & = pn \\\\\n\\frac{1}{n} \\sum_{i = 1}^n x_i & = p\n\\end{align*}\\]\nand by solving for \\(p\\), we get that the MLE of \\(p\\) is equal to \\(\\frac{1}{n}\\sum_{i = 1}^n x_i\\). We will often see that the MLEs of parameters are functions of sample averages (in this case, just the identity function!).\nProblem 2: Suppose \\(X_1, X_2, \\dots, X_n\\) are a random sample from the Normal pdf with parameters \\(\\mu\\) and \\(\\sigma^2\\):\n\\[\nf_X(x ; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2},\n\\]\nfor \\(-\\infty &lt; x &lt; \\infty, \\ -\\infty &lt; \\mu &lt; \\infty,\\) and \\(\\sigma^2 &gt; 0\\). Find the MLEs of \\(\\mu\\) and \\(\\sigma^2\\). (Note that this is Question 5 on the MLE section of Problem Set 1! For your HW, try your best to do this problem from scratch, without looking at the course notes!)\nSince we are dealing with a likelihood with two parameters, we’ll need to solve a system of equations to obtain the MLEs for \\(\\mu\\) and \\(\\sigma^2\\).\n\\[\\begin{align*}\n    \\log(L(\\mu, \\sigma^2)) & = \\log( \\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x_i - \\mu)^2) ) \\\\\n    & = \\sum_{i = 1}^n \\left[ \\log(\\frac{1}{\\sqrt{2\\pi \\sigma^2}})  - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ -\\frac{1}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right] \\\\\n    & = \\frac{-n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (x_i - \\mu)^2\n\\end{align*}\\]\nNow we need to find \\(\\frac{\\partial}{\\partial \\sigma^2}\\log(L(\\mu, \\sigma^2))\\) and \\(\\frac{\\partial}{\\partial \\mu}\\log(L(\\mu, \\sigma^2))\\). Let’s make our lives a little bit easier by setting \\(\\sigma^2 \\equiv \\theta\\) (so we don’t trip ourselves up with the exponent). We get\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\theta}\\log(L(\\mu, \\theta)) & = \\frac{\\partial}{\\partial \\theta} \\left(\\frac{-n}{2} \\log(2\\pi \\theta) - \\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i - \\mu)^2 \\right)\\\\\n    & = \\frac{-2\\pi n}{4 \\pi \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2}\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\log(L(\\mu, \\theta)) & = \\frac{\\partial}{\\partial \\mu} \\left(\\frac{-n}{2} \\log(2\\pi \\theta) - \\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i - \\mu)^2 \\right)\\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i^2 - 2 \\mu x_i + \\mu^2)\\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} ( \\sum_{i = 1}^n x_i^2 - 2 \\mu \\sum_{i = 1}^n x_i + n\\mu^2 )\\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} (- 2 \\mu \\sum_{i = 1}^n x_i + n\\mu^2 ) \\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left(   \\frac{\\sum_{i = 1}^n x_i}{\\theta} \\mu - \\frac{n}{2\\theta}\\mu^2  \\right) \\\\\n    & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu\n\\end{align*}\\]\nWe now have the following system of equations to solve:\n\\[\\begin{align*}\n    0 & \\equiv \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    0 & \\equiv \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu\n\\end{align*}\\]\nTypically, we solve one of the equations for one of the parameters, plug that into the other equation, and then go from there. We’ll start by solving the second equation for \\(\\mu\\).\n\\[\\begin{align*}\n    0 & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu \\\\\n    \\frac{n}{\\theta} \\mu & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} \\\\\n    \\mu & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nWell that’s convenient! We already have the MLE for \\(\\mu\\) as being just the sample average. Plugging this into the first equation in our system we obtain\n\\[\\begin{align*}\n    0 & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    0 & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{2 \\theta^2} \\\\\n    \\frac{n}{2 \\theta} & = \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{2 \\theta^2} \\\\\n    n & = \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{\\theta} \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2] \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x} )^2\n\\end{align*}\\]\nwhere \\(\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i\\). And so finally, we have that the MLE for \\(\\sigma^2\\) is given by \\(\\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x} )^2\\), and the MLE for \\(\\mu\\) is given by \\(\\bar{x}\\)!"
  },
  {
    "objectID": "probability.html#learning-objectives",
    "href": "probability.html#learning-objectives",
    "title": "1  Probability: A Brief Review",
    "section": "1.1 Learning Objectives",
    "text": "1.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDistinguish between important probability models (e.g., Normal, Binomial)\nDerive the expectation and variance of a single random variable or a sum of random variables\nDefine the moment generating function and use it to find moments or identify pdfs"
  },
  {
    "objectID": "probability.html#reading-guide",
    "href": "probability.html#reading-guide",
    "title": "1  Probability: A Brief Review",
    "section": "1.2 Reading Guide",
    "text": "1.2 Reading Guide\nAssociated Readings: Chapters 2-4 (pages 15-277)\n\n1.2.1 Reading Questions\n\nWhich probability distributions are appropriate for quantitative (continuous) random variables?\nWhich probability distributions are appropriate for categorical random variables?\nIndependently and Identically Distributed (iid) random variables are an incredibly important assumption involved in many statistical methods. Why do you think it might be important/useful for random variables to have this property?"
  },
  {
    "objectID": "probability.html#definitions",
    "href": "probability.html#definitions",
    "title": "1  Probability: A Brief Review",
    "section": "1.3 Definitions",
    "text": "1.3 Definitions\nYou are expected to know the following definitions:\nRandom Variable\nA random variable is a function that takes inputs from a sample space of all possible outcomes, and outputs real values or probabilities. As an example, consider a coin flip. The sample space of all possible outcomes consists of “heads” and “tails”, and each outcome is associated with a probability (50% each, for a fair coin). For our purposes, you should know that random variables have probability density (or mass) functions, and are either discrete or continuous based on the number of possible outcomes a random variable may take. Random variables are often denoted with capital Roman letters, like \\(X\\), \\(Y\\), \\(Z\\), etc.\nProbability density function (discrete, continuous)\n\nNote: I don’t care if you call a pmf a pdf… I will probably do this continuously throughout the semester. We don’t need to be picky about this in MATH/STAT 455.\n\nThere are many different accepted ways to write the notation for a pdf of a random variables. Any of the following are perfectly appropriate for this class: \\(f(x)\\), \\(\\pi(x)\\), \\(p(x)\\), \\(f_X(x)\\). I typically use either \\(\\pi\\) or \\(p\\), but might mix it up occasionally.\nKey things I want you to know about probability density functions:\n\n\\(\\pi(x) \\geq 0\\), everywhere. This should make sense (hopefully) because probabilities cannot be negative!\n\\(\\int_{-\\infty}^\\infty \\pi(x) = 1\\). This should also (hopefully) makes sense. Probabilities can’t be greater than one, and the probability of event occurring at all (ever) should be equal to one, if the event \\(x\\) is a random variable.\n\nCumulative distribution function (discrete, continuous)\nCumulative distribution functions we’ll typically write as \\(F_X(x)\\). or \\(F(x)\\), for short. It is important to know that\n\\[\nF_X(x) = \\Pr(X \\leq x),\n\\]\nor in words, “the cumulative distribution function is the probability that a random variable lies before \\(x\\).” If you write \\(\\Pr(X &lt; x)\\) instead of \\(\\leq\\), you’re fine. The probability that a random variable is exactly one number (for an RV with a continuous pdf) is zero anyway, so these are the same thing. Key things I want you to know about cumulative distribution functions:\n\n\\(F(x)\\) is non-decreasing. This is in part where the “cumulative” piece comes in to play. Recall that probabilities are basically integrals or sums. If we’re integrating over something positive, and our upper bound for our integral increases, the area under the curve (cumulative probability) will increase as well.\n\\(0 \\leq F(x) \\leq 1\\) (since probabilities have to be between zero and one!)\n\\(\\Pr(a &lt; X \\leq b) = F(a) - F(b)\\) (because algebra)\n\nJoint probability density function\nA joint probability density function is a probability distribution defined for more than one random variable at a time. For two random variables, \\(X\\) and \\(Z\\), we could write their joint density function as \\(f_{X,Z}(x, z)\\) , or \\(f(x,z)\\) for short. The joint density function encodes all sorts of fun information, including marginal distributions for \\(X\\) and \\(Z\\), and conditional distributions (see next bold definition). We can think of the joint pdf as listing all possible pairs of outputs from the density function \\(f(x,z)\\), for varying values of \\(x\\) and \\(z\\). Key things I want you to know about joint pdfs:\n\nHow to get a marginal pdf from a joint pdf:\nSuppose I want to know \\(f_X(x)\\), and I know \\(f_{X,Z}(x,z)\\). Then I can integrate or “average over” \\(Z\\) to get\n\\[\nf_X(x) = \\int f_{X,Z}(x,z)dz\n\\]\nThe relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule\nHow to obtain a joint pdf for independent random variables: just multiply their marginal pdfs together! This is how we will (typically) think about likelihoods!\nHow to obtain a marginal pdf from a joint pdf when random variables are independent without integrating (think, “separability”)\n\nConditional probability density function\nA conditional pdf denotes the probability distribution for a (set of) random variable(s), given that the value for another (set of) random variable(s) is known. For two random variables, \\(X\\) and \\(Z\\), we could write the conditional distribution of \\(X\\) “given” \\(Z\\) as \\(f_{X \\mid Z}(x \\mid z)\\) , where the “conditioning” is denoted by a vertical bar (in LaTeX, this is typeset using “\\mid”). Key things I want you to know about conditional pdfs:\n\nThe relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule\nHow to obtain a conditional pdf from a joint pdf (again, think Bayes’ rule)\nRelationship between conditional pdfs and independence (see next bold definition)\n\nIndependence\nTwo random variables \\(X\\) and \\(Z\\) are independent if and only if:\n\n\\(f_{X,Z}(x,z) = f_X(x) f_Z(z)\\) (their joint pdf is “separable”)\n\\(f_{X\\mid Z}(x\\mid z) = f_X(x)\\) (the pdf for \\(X\\) does not depend on \\(Z\\) in any way)\nNote that the “opposite” is also true: \\(f_{Z\\mid X}(z\\mid x) = f_Z(z)\\)\n\nIn notation, we denote that two variables are independent as \\(X \\perp\\!\\!\\!\\perp Z\\), or \\(X \\perp Z\\). In LaTeX, the latter is typeset as “\\perp”, and the former is typeset as “\\perp\\!\\!\\!\\perp”. As a matter of personal preference, I (Taylor) prefer \\(\\perp\\!\\!\\!\\perp\\), but I don’t like typing it out every time. Consider using the “\\newcommand” functionality in LaTeX to create a shorthand for this for your documents!\nExpected Value / Expectation\nThe expectation (or expected value) of a random variable is defined as:\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\nExpected value is a weighted average, where the average is over all possible values a random variable can take, weighted by the probability that those values occur. Key things I want you to know about expectation:\n\nThe relationship between expectation, variance, and moments (specifically, that \\(E[X]\\) is the 1st moment!)\nThe “law of the unconscious statistician” (see the Theorems section of this chapter)\nExpectation of linear transformations of random variables (see Theorems section of this chapter)\n\nVariance\nThe variance of a random variable is defined as:\n\\[\nVar[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nIn words, we can read this as “the expected value of the squared deviation from the mean” of a random variable \\(X\\). Key things I want you to know about variance:\n\nThe relationship between expectation, variance, and moments (hopefully clear, given the formula for variance)\nThe relationship between variance and standard deviation: \\(Var(X) = sd(X)^2\\)\nThe relationship between variance and covariance: \\(Var(X) = Cov(X, X)\\)\n\\(Var(X) \\geq 0\\). This should make sense, given that we’re taking the expectation of something “squared” in order to calculate it!\n\\(Var(c) = 0\\) for any constant, \\(c\\).\nVariance of linear transformations of random variables (see Theorems section of this chapter)\n\n\\(r^{th}\\) moment\nThe \\(r^{th}\\) moment of a probability distribution is given by \\(E[X^r]\\). For example, when \\(r = 1\\), the \\(r^{th}\\) moment is just the expectation of the random variable \\(X\\). Key things I want you to know about moments:\n\nThe relationship between moments, expectation, and variance\n\nFor example, if you know the first and second moments of a distribution, you should be able to calculate the variance of a random variable with that distribution!\n\nThe relationship between moments and moment generating functions (see Theorems section of this chapter)\n\nCovariance\nThe covariance of two random variables is a measure of their joint variability. We denote the covariance of two random variables \\(X\\) and \\(Z\\) as \\(Cov(X,Z)\\), and\n\\[\nCov(X, Z) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\n\\]\nSome things I want you to know about covariance:\n\n\\(Cov(X, X) = Var(X)\\)\n\\(Cov(X, Y) = Cov(Y, X)\\) (order doesn’t matter)\n\nMoment Generating Function (MGF)\nThe moment generating function of a random variable \\(X\\) is defined as\n\\[\nM_X(t) = E[e^{tX}]\n\\]\nA few things to note:\n\n\\(M_X(0) = 1\\), always.\nIf two random variables have the same MGF, they have the same probability distribution!\nMGFs are sometimes useful for showing how different random variables are related to each other\n\nYou are also expected to know the probability distributions contained in Table 1, below. Note that you do not need to memorize the pdfs for these distributions, but you should be familiar with what types of random variables (continuous/quantitative, categorical, integer-valued, etc.) may take on different distributions. The more familiar you are with the forms of the pdfs, the easier/faster it will be to work through problem sets and quizzes.\n\nTable 1. Table of main probability distributions we will work with for MATH/STAT 455.\n\n\n\n\n\n\n\nDistribution\nPDF/PMF\nParameters\n\n\n\n\nUniform\n\\(\\pi(x) = \\frac{1}{\\beta - \\alpha}\\)\n\\(\\alpha \\in \\mathbb{R}\\), \\(\\beta\\in \\mathbb{R}\\)\n\n\nNormal\n\\(\\pi(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2)\\)\n\\(\\mu \\in \\mathbb{R}\\), \\(\\sigma &gt; 0\\)\n\n\nMultivariate Normal\n\\(\\pi(\\textbf{x}) = (2\\pi)^{-k/2} |\\Sigma|^{-1/2} \\exp(-\\frac{1}{2}(\\textbf{x} - \\mu)^\\top \\Sigma^{-1}(\\textbf{x} - \\mu)))\\)\n\\(\\mu \\in \\mathbb{R}^k\\), \\(\\Sigma \\in \\mathbb{R}^{k\\times k}\\) , positive semi-definite (in practice, almost always positive definite)\n\n\nGamma\n\\(\\pi(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\\)\n\\(\\alpha \\text{ (shape)}, \\beta \\text{ (rate)} &gt; 0\\)\n\n\nChi-square\n\\(\\pi(x) = \\frac{2^{-\\nu/2}}{\\Gamma(\\nu/2)} x^{\\nu/2 - 1}e^{-x/2})\\)\n\\(\\nu &gt; 0\\)\n\n\nExponential\n\\(\\pi(x) = \\beta e^{-\\beta x}\\)\n\\(\\beta &gt; 0\\)\n\n\nStudent-$t$\n\\(\\pi(x) = \\frac{\\Gamma((\\nu + 1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu \\pi}} (1 + \\frac{x^2}{\\nu})^{-(\\nu + 1)/2}\\)\n\\(\\nu &gt; 0\\)\n\n\nBeta\n\\(\\pi(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha - 1}(1 - x)^{\\beta - 1}\\)\n\\(\\alpha, \\beta &gt; 0\\)\n\n\nPoisson\n\\(\\pi(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\n\\(\\lambda &gt; 0\\)\n\n\nBinomial\n\\(\\pi(x) = \\binom{n}{x} p^{x} (1 - p)^{n - x}\\)\n\\(p \\in [0,1], n = \\{0, 1, 2, \\dots\\}\\)\n\n\nMultinomial\n\\(\\pi(\\textbf{x}) = \\frac{n!}{x_1! \\dots x_k!} p_1^{x_1} \\dots p_k^{x_k}\\)\n\\(p_i &gt; 0\\), \\(p_1 + \\dots + p_k = 1\\), \\(n = \\{0, 1, 2, \\dots \\}\\)\n\n\nNegative Binomial\n\\(\\pi(x) = \\binom{k + r - 1}{k} (1-p)^k p^r\\)\n\\(r &gt; 0\\), \\(p \\in [0,1]\\)"
  },
  {
    "objectID": "probability.html#theorems",
    "href": "probability.html#theorems",
    "title": "1  Probability: A Brief Review",
    "section": "1.4 Theorems",
    "text": "1.4 Theorems\n\nLaw of Total Probability\n\\[\nP(A) = \\sum_n P(A \\cap B_n),\n\\]or\n\\[\nP(A) = \\sum_n P(A \\mid B_n) P(B_n)\n\\]\nBayes’ Theorem\n\\[\n\\pi(A \\mid B) = \\frac{\\pi(B \\mid A) \\pi(A)}{\\pi(B)}\n\\]\nRelationship between pdf and cdf\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t)dt\n\\]\n\\[\n\\frac{\\partial}{\\partial y}F_Y(y) = f_Y(y)\n\\]\nExpectation of random variables\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\n\\[\nE[X^2] = \\int_{-\\infty}^\\infty x^2 f(x) dx\n\\]\n\n“Law of the Unconscious Statistician”\n\\[\nE[g(X)] = \\int_{-\\infty}^\\infty g(x)f(x)dx\n\\]\n\nExpectation and variance of linear transformations of random variables\n\\[\nE[cX + b] = c E[X] + b\n\\]\n\\[\nVar[cX + b] = c^2 Var[X]\n\\]\nRelationship between mean and variance\n\\[\nVar[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nAlso, recall that \\(Cov[X, X] = Var[X]\\).\nFinding a marginal pdf from a joint pdf\n\\[\nf_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(x, y) dy\n\\]\nIndependence of random variables and joint pdfs\nIf two random variables are independent, their joint pdf will be separable. For example, if \\(X\\) and \\(Y\\) are independent, we could write\n\\[\nf_{X,Y}(x, y) = f_{X}(x)f_Y(y)\n\\]\nExpected value of a product of independent random variables\nSuppose random variables \\(X_1, \\dots, X_n\\) are independent. Then we can write,\n\\[\nE\\left[\\prod_{i = 1}^n X_i\\right] = \\prod_{i = 1}^n E[X_i]\n\\]\nCovariance of independent random variables\nIf \\(X\\) and \\(Y\\) are independent, then \\(Cov(X, Y) = 0\\). We can show this by noting that\n\n\\[\\begin{align}\nCov(X, Y) & = E[(X - E[X])(Y - E[Y])] \\\\\n& = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\\\\n& = E[XY] - E[XE[Y]] - E[YE[X]] + E[X]E[Y] \\\\\n& =  2E[X]E[Y] - 2E[X]E[Y] \\\\\n& = 0\n\\end{align}\\]\n\nUsing MGFs to find moments\nRecall that the moment generating function of a random variable \\(X\\), denoted by \\(M_X(t)\\) is\n\\[\nM_X(t) = E[e^{tX}]\n\\]\nThen the \\(n\\)th moment of the probability distribution for \\(X\\) , \\(E[X^n]\\), is given by\n\\[\n\\frac{\\partial M_X}{\\partial t^n} \\Bigg|_{t = 0}\n\\]\nwhere the above reads as “the \\(n\\)th derivative of the moment generating function, evaluated at \\(t = 0\\).”\nUsing MGFs to identify pdfs\nMGFs uniquely identify probability density functions. If \\(X\\) and \\(Y\\) are two random variables where for all values of \\(t\\), \\(M_X(t) = M_Y(t)\\), then \\(F_X(x) = F_Y(y)\\).\nCentral Limit Theorem\nThe classical CLT states that for independent and identically distributed (iid) random variables \\(X_1, \\dots, X_n\\), with expected value \\(E[X_i] = \\mu\\) and \\(Var[X_i] = \\sigma^2 &lt; \\infty\\), the sample average (centered and standardized) converges in distribution to a standard normal distribution at a root-\\(n\\) rate. Notationally, this is written as\n\\[\n\\sqrt{n} (\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\n A fun aside: this is only one CLT, often referred to as the Levy CLT. There are other CLTs, such as the Lyapunov CLT and Lindeberg-Feller CLT!\n\n\n1.4.1 Transforming Continuous Random Variables\nWe will often take at face value previously proven relationships between random variables. What I mean by this, as an example, is that it is a nice (convenient) fact that a sum of two independent normal random variables is still normally distributed, with a nice form for the mean and variance. In particular, if \\(X \\sim N(\\mu, \\sigma^2)\\) and \\(Y \\sim N(\\theta, \\nu^2)\\), then \\(X + Y \\sim N(\\mu + \\theta, \\sigma^2 + \\nu^2)\\). Most frequently used examples of these sorts of relationships can be found in the ``Related Distributions” section of the Wikipedia page for a given probability distribution. Unless I explicitly ask you to derive/show how certain variables are related to each other, you can just state the known relationship, use it, and move on!\nIf I do ask you to derive/show these things, there a few different ways we can go about this. For this course, I only expect you to know the “CDF method” for one function of one random variable, as we’ll demonstrate below.\nTheorem. Let \\(X\\) be a continuous random variable with pdf \\(f_X(x)\\). Define a new random variable \\(Y = g(X)\\), for nice* functions \\(g\\). Then \\(f_Y(y) = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))}\\).\n *By nice functions we mean functions that are strictly increasing and smooth on the required range. As an example, \\(exp(x)\\) is a smooth, strictly increasing function; \\(|x|\\) is not on the whole real line, but is from \\((0, \\infty)\\) (where a lot of useful pdfs are defined). For the purposes of this class, every function that you will need to do this for will be “nice.” Note that there are also considerations that need to be taken regarding the range of continuous random variables when considering transforming them. We will mostly ignore these considerations in this class, but a technically complete derivation (or proof) must consider them.\nProof. We can write\n\\[\\begin{align*}\n    f_Y(y) & = \\frac{\\partial}{\\partial y} F_Y(y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(Y \\leq y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(g(X) \\leq y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(X \\leq g^{-1}(y)) \\\\\n    & = \\frac{\\partial}{\\partial y} F_X(g^{-1}(y)) \\\\\n    & = f_X(g^{-1}(y)) \\times \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align*}\\]\nwhere to obtain the last equality we use chain rule! Now we require some statistical trickery to continue… (note that this method is called the “CDF method” because we go through the CDF to derive the distribution for \\(Y\\))\nYou will especially see this in the Bayes chapter of our course notes, but it is often true that our lives are made easier as statisticians if we multiply things by one, or add zero. What exactly do I mean? Rearranging gross looking formulas into things we are familiar with (like pdfs, for example) often makes our lives easier and allows us to avoid dealing with such grossness. Here, the grossness is less obvious, but nonetheless relevant. Note that we can write\n\\[\\begin{align*}\n    y & = y \\\\\n    y & = g(g^{-1}(y)) \\\\\n    \\frac{\\partial}{\\partial y} y & = \\frac{\\partial}{\\partial y} g(g^{-1}(y)) \\\\\n    1 & = g'(g^{-1}(y)) \\frac{\\partial}{\\partial y} g^{-1}(y) \\hspace{1cm} \\text{(chain rule again!)} \\\\\n    \\frac{1}{g'(g^{-1}(y))} & = \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align*}\\]\nThe right-hand side should look familiar: it is exactly what we needed to “deal with” in our proof! Returning to that proof, we have\n\\[\\begin{align*}\n    f_Y(y) & = f_X(g^{-1}(y)) \\times \\frac{\\partial}{\\partial y} g^{-1}(y) \\\\\n    & = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))}\n\\end{align*}\\]\nas desired."
  },
  {
    "objectID": "probability.html#worked-examples",
    "href": "probability.html#worked-examples",
    "title": "1  Probability: A Brief Review",
    "section": "1.5 Worked Examples",
    "text": "1.5 Worked Examples\nProblem 1: Suppose \\(X \\sim Exponential(\\lambda)\\). Calculate \\(E[X]\\) and \\(Var[X]\\).\nWe know that \\(f(x) = \\lambda e^{-\\lambda x}\\). If we can calculate \\(E[X]\\) and \\(E[X^2]\\), then we’re basically done! We can write\n\\[\\begin{align*}    \nE[X] & = \\int_0^\\infty x \\lambda e^{-\\lambda x} dx \\\\    \n& = \\lambda \\int_0^\\infty x e^{-\\lambda x} dx\n\\end{align*}\\]\nAnd now we need integration by parts! Set \\(u = x\\), \\(dv = e^{-\\lambda x} dx\\). Then \\(du = 1dx\\) and \\(v = \\frac{-1}{\\lambda} e^{-\\lambda x}\\). Since \\(\\int u dv = uv - \\int vdu\\), we can continue\n\\[\\begin{align*}    \nE[X] & = \\lambda \\int_0^\\infty x e^{-\\lambda x} dx \\\\    \n& = \\lambda \\left( -\\frac{x}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  - \\int_0^\\infty \\frac{-1}{\\lambda} e^{-\\lambda x} dx \\right) \\\\    \n& = \\lambda \\left( - \\int_0^\\infty \\frac{-1}{\\lambda} e^{-\\lambda x} dx \\right) \\\\    \n& = \\lambda \\left( \\frac{-1}{\\lambda^2} e^{-\\lambda x}  \\bigg|_0^\\infty \\right) \\\\    \n& = \\frac{-1}{\\lambda} e^{-\\lambda x}  \\bigg|_0^\\infty \\\\    \n& = \\frac{1}{\\lambda} e^{-0} \\\\    \n& = \\frac{1}{\\lambda}\n\\end{align*}\\]\nWe can follow a similar process to get \\(E[X^2]\\) (using the law of the unconscious statistician!). We can write\n\\[\\begin{align*}\n    E[X^2] & = \\int_0^\\infty x^2 \\lambda e^{-\\lambda x} dx \\\\\n    & = \\lambda \\int_0^\\infty x^2 e^{-\\lambda x} dx\n\\end{align*}\\]\nAnd now we need integration by parts again! Set \\(u = x^2\\), \\(dv = e^{-\\lambda x} dx\\). Then \\(du = 2xdx\\) and \\(v = \\frac{-1}{\\lambda} e^{-\\lambda x}\\). Since \\(\\int u dv = uv - \\int vdu\\), we can continue\n\\[\\begin{align*}\n    E[X] & = \\lambda \\int_0^\\infty x^2 e^{-\\lambda x} dx \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  - \\int_0^\\infty \\frac{-2}{\\lambda} xe^{-\\lambda x} dx \\right) \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  + \\frac{2}{\\lambda} \\int_0^\\infty  xe^{-\\lambda x} dx \\right) \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  + \\frac{2}{\\lambda^3} \\right)\\\\\n    & = \\lambda \\left( 0  + \\frac{2}{\\lambda^3} \\right) \\\\\n    & = \\frac{2}{\\lambda^2}\n\\end{align*}\\]\nNow we can calculate \\(Var[X] = E[X^2] - E[X]^2\\) as \\[\nVar[X] = E[X^2] - E[X]^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\n\\] And so we have \\(E[X] = \\frac{1}{\\lambda}\\) and \\(Var[X] = \\frac{1}{\\lambda^2}\\).\nProblem 2: Show that an exponentially distributed random variable is “memoryless”, i.e. show that \\(\\Pr(X &gt; s + x \\mid X &gt; s) = \\Pr(X &gt; x)\\), \\(\\forall s\\).\nRecall that the CDF of an exponential distribution is given by \\(F(x) = 1-e^{-\\lambda x}\\). Thanks to Bayes rule, we can write\n\\[\\begin{align*}\n    \\Pr(X &gt; s + x \\mid X &gt; s) & = \\frac{\\Pr(X &gt; s + x , X &gt; s)}{\\Pr(X &gt; s)} \\\\\n    & = \\frac{\\Pr(X &gt; s + x)}{\\Pr(X &gt; s)} \\\\\n    & = \\frac{1 - \\Pr(X &lt; s + x)}{1 - \\Pr(X &lt; s)} \\\\\n    & = \\frac{1 - F(s + x)}{1 - F(s)}\n\\end{align*}\\]\nwhere the second equality is true because \\(x &gt; 0\\). Then we can write\n\\[\\begin{align*}\n    \\Pr(X &gt; s + x \\mid X &gt; s) & = \\frac{1 - F(s + x)}{1 - F(s)} \\\\\n    & = \\frac{1 - \\left(1 - e^{-\\lambda(s + x)}\\right)}{1 - \\left(1 - e^{-\\lambda s}\\right)} \\\\\n    & = \\frac{e^{-\\lambda(s + x)}}{e^{-\\lambda s}} \\\\\n    & = \\frac{e^{-\\lambda s - \\lambda x}}{e^{-\\lambda s}} \\\\\n    & = e^{-\\lambda x} \\\\\n    & = 1 - F(x) \\\\\n    & = \\Pr(X &gt; x)\n\\end{align*}\\]\nand we’re done!\nProblem 3: Suppose \\(X \\sim Exponential(1/\\lambda)\\), and \\(Y \\mid X \\sim Poisson(X)\\). Show that \\(Y \\sim Geometric(1/(1 + \\lambda))\\).\nNote that we can write \\(f(x, y) = f(y \\mid x) f(x)\\), and \\(f(y) = \\int f(x, y) dx\\). Then\n\\[\nf(x, y) = \\left( \\frac{1}{\\lambda} e^{-x/\\lambda} \\right) \\left( \\frac{x^y e^{-x}}{y!} \\right)\n\\] And so,\n\\[\\begin{align*}\n    f(y) & = \\int f(x, y) dx \\\\\n    & = \\int \\left( \\frac{1}{\\lambda} e^{-x/\\lambda} \\right) \\left( \\frac{x^y e^{-x}}{y!} \\right) dx \\\\\n    & = \\frac{1}{\\lambda y!} \\int x^y e^{-x(1 + \\lambda)/\\lambda} dx\n\\end{align*}\\]\nAnd we can again use integration by parts! Let \\(u = x^y\\) and \\(dv = e^{-x(1 + \\lambda)/\\lambda} dx\\). Then we have \\(du = yx^{y-1} dx\\) and \\(v = -\\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda}\\), and we can write\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda y!} \\int x^y e^{-x(1 + \\lambda)/\\lambda} dx \\\\\n    & = \\frac{1}{\\lambda y!} \\left( x^y \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} \\bigg|_{x = 0}^{x = \\infty}  + \\int \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} yx^{y-1} dx\\right) \\\\\n    & = \\frac{1}{\\lambda y!} \\left(  \\int \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} yx^{y-1} dx \\right) \\\\\n    & = \\frac{1}{\\lambda y!} \\left( \\frac{\\lambda }{1 + \\lambda} \\right) y \\left(  \\int e^{-x(1 + \\lambda)/\\lambda} x^{y-1} dx \\right)\n\\end{align*}\\]\nThis looks gross, but it’s actually not so bad. Note that, since \\(Y\\) is Poisson, it can only take integer values beginning at 1! Then we can repeat the process of integration by parts \\(y\\) times in order to get rid of \\(x^{y\\dots}\\) term on the inside of the integral. Specifically, each time we do this process we will pull out a \\(\\left( \\frac{\\lambda }{1 + \\lambda} \\right)\\), and a \\(y - i\\) for the \\(i\\)th integration by parts step (try this one or two steps for yourself to see how it will simplify if you find this unintuitive!). We end up with,\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda y!} \\left( \\frac{\\lambda }{1 + \\lambda} \\right)^y y! \\\\\n    & = \\frac{1}{\\lambda} \\left(\\frac{\\lambda}{1 + \\lambda}\\right)^y\n\\end{align*}\\]\nNow let \\(p = \\frac{1}{1 + \\lambda}\\). If we can show that \\(f(y) \\sim Geometric(p)\\) then we’re done. Note that \\(1 - p = \\lambda/(1 + \\lambda)\\). We have\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda} (1 - p)^y \\\\\n    & = \\frac{1}{\\lambda} (1 - p)^{y-1} (1-p) \\\\\n    & = (1 - p)^{y-1} \\frac{1}{\\lambda} \\left( \\frac{\\lambda}{1 + \\lambda} \\right) \\\\\n    & = (1 - p)^{y-1} \\left( \\frac{1}{1 + \\lambda} \\right) \\\\\n    & = (1 - p)^{y-1} p\n\\end{align*}\\]\nwhich is exactly the pdf of a geometric random variable with parameter \\(p\\) and trials that begin at 1.\nAn alternative solution (which perhaps embodies the phrase “work smarter, not harder”) actually doesn’t involve integration by parts at all! As statisticians, we typically like to avoid actually integrating anything whenever possible, and this is often achieved by manipulating algebra enough to essentially “create” a pdf out of what we see (since pdfs integrate to \\(1\\)!). Massive props to a student for solving this problem in a much “easier” way, answer below:\n\\[\\begin{align*}\n        f(y) &= \\int_{0}^{\\infty} f(y \\mid x) f(x) dx \\\\\n        &= \\int_{0}^{\\infty} (\\frac{1}{\\lambda}e^{-\\frac{x}{\\lambda}}) (\\frac{x^y}{y!} e^{-x}) dx \\\\\n        &= \\frac{1}{\\lambda y!} \\int_{0}^{\\infty} x^y e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx \\\\\n        &= \\frac{1}{\\lambda y!} \\int_{0}^{\\infty} \\frac{(\\frac{1+\\lambda}{\\lambda})^{y+1}}{(\\frac{1+\\lambda}{\\lambda})^{y+1}} \\frac{\\Gamma(y+1)}{\\Gamma(y+1)} x^{(y+1)-1} e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx\\\\\n        &= \\frac{\\Gamma(y+1)}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} \\int_{0}^{\\infty} \\frac{(\\frac{1+\\lambda}{\\lambda})^{y+1}}{\\Gamma(y+1)} x^{(y+1)-1} e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx\\\\\n        &= \\frac{\\Gamma(y+1)}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} (1)\\\\\n        &= \\frac{y!}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} \\\\\n        &= \\frac{\\lambda^{-1}}{(\\frac{1+\\lambda}{\\lambda})^{y+1}}\\\\\n        &= \\frac{\\lambda^y}{(1+\\lambda)^{y+1}}\\\\\n        &= \\frac{1}{(1+\\lambda)} \\frac{\\lambda^y}{(1+\\lambda)^y}\\\\\n        &= \\frac{1}{(1+\\lambda)} (1 - \\frac{1}{(1+\\lambda)})^y \\\\\n        &=p(1-p)^y\\qquad (\\text{where }p=\\frac{1}{1+\\lambda})\n    \\end{align*}\\]\nNote that we arrive at a slightly different answer with this approach. Specifically, we arrive at the pdf of a geometric random variable with parameter \\(p\\) and trials that begin at 0, as opposed to 1. There’s some subtlety here that we’re going to choose to ignore.\nProblem 4: Suppose that \\(X \\sim N(\\mu, \\sigma^2)\\), and let \\(Y = \\frac{X - \\mu}{\\sigma}\\). Find the distribution of \\(Y\\) (simplifying all of your math will be useful for this problem).\nTo solve this problem, we can use the theorem on transforming continuous random variables. We must first define our function \\(g\\) that relates \\(X\\) and \\(Y\\). In this case, we have \\(g(a) = \\frac{a - \\mu}{\\sigma}\\). Now all we need to do is collect the mathematical “pieces” we need to use theorem: \\(g^{-1}(a)\\), and \\(g'(a)\\), and finally, the pdf of a normal random variable. We have\n\\[\\begin{align*}\n    f_X(x) & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2) \\\\\n    g^{-1}(a) & = \\sigma a + \\mu \\\\\n    g'(a) & = \\frac{\\partial}{\\partial a} \\left(\\frac{a - \\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}\n\\end{align*}\\]\nPutting it all together, we have\n\\[\\begin{align*}\n    f_Y(y) & = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))} \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (\\sigma y + \\mu - \\mu)^2) \\times \\sigma \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp(-\\frac{1}{2\\sigma^2} (\\sigma y)^2) \\times \\sigma \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2\\sigma^2} \\sigma^2 y^2) \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2} y^2)\n\\end{align*}\\]\nand note that this is the pdf of a normally distributed random variable with mean \\(0\\) and variance \\(1\\)! Thus, we have shown that \\(\\frac{X - \\mu}{\\sigma} \\sim N(0,1)\\). Fun Fact: If this random variable reminds you of a Z-score, it should!\nProblem 5: Suppose the joint pdf of two random variables \\(X\\) and \\(Y\\) is given by \\(f_{X,Y}(x,y) = \\lambda \\beta e^{-x\\lambda - y\\beta}\\). Determine if \\(X\\) and \\(Y\\) are independent, showing why or why not.\nTo determine whether \\(X\\) and \\(Y\\) are independent (or not), we need to determine if their joint pdf is “separable.” Doing some algebra, we can see that\n\\[\\begin{align*}    \nf_{X,Y}(x,y) & = \\lambda \\beta e^{-x \\lambda - y\\beta} \\\\    \n& = \\lambda \\beta e^{-x \\lambda} e^{-y \\beta} \\\\   \n& = \\left( \\lambda  e^{-x \\lambda} \\right) \\left( \\beta e^{-y \\beta} \\right)\n\\end{align*}\\]\nand so since we can write the joint distribution as a function of \\(X\\) multiplied by a function of \\(Y\\), \\(X\\) and \\(Y\\) are independent (and in this case, both have exponential distributions).\nProblem 6: Suppose the joint pdf of two random variables \\(X\\) and \\(Y\\) is given by \\(f_{X,Y}(x,y) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\binom{n}{y} x^{y + \\alpha - 1} (1-x)^{n-y + \\beta - 1}\\). Determine if \\(X\\) and \\(Y\\) are independent, showing why or why not.\nTo determine whether \\(X\\) and \\(Y\\) are independent (or not), we need to determine if their joint pdf is “separable.” Right away, we should note that a piece of the pdf contains \\(x^y\\), and therefore we are never going to be able to fully separate out this joint pdf into a function of \\(x\\) times a function \\(y\\). Therefore, \\(X\\) and \\(Y\\) are not independent. In this case, we actually have \\(X \\sim Beta(\\alpha, \\beta)\\), and \\(Y \\mid X \\sim Binomial(n, y)\\) (we’ll return to this example in the Bayes chapter!)."
  },
  {
    "objectID": "mom.html",
    "href": "mom.html",
    "title": "3  Method of Moments",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "properties.html",
    "href": "properties.html",
    "title": "4  Properties of Estimators",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "consistency.html",
    "href": "consistency.html",
    "title": "5  Consistency",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "asymptotics.html",
    "href": "asymptotics.html",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "computation.html",
    "href": "computation.html",
    "title": "7  Computational Optimization",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "8  Bayesian Inference",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "decision.html",
    "href": "decision.html",
    "title": "9  Decision Theory",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "10  Hypothesis Testing",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH/STAT 455: Mathematical Statistics",
    "section": "",
    "text": "Welcome to Mathematical Statistics!\nThis book contains the course notes for MATH/STAT 455: Mathematical Statistics at Macalester College, as taught by Prof. Taylor Okonek. These notes draw from course notes created by Prof. Kelsey Grinde, and heavily from the course textbook, An Introduction to Mathematical Statistics and Its Applications by Richard Larsen and Morris Marx (6th Edition). Each chapter will contain (at a minimum):\n\nLearning Objectives\nReading Guide\nDefinitions\nTheorems\nWorked Examples\n\nI will be editing and adding to these notes throughout Spring 2024, so please check consistently for updates!\nIf you find any typos or have other questions, please email tokonek@macalester.edu."
  }
]