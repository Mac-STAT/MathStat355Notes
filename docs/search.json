[
  {
    "objectID": "decision.html#learning-objectives",
    "href": "decision.html#learning-objectives",
    "title": "9  Decision Theory",
    "section": "9.1 Learning Objectives",
    "text": "9.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive a Bayes estimate for a common loss function\nDistinguish between admissible and inadmissible decision rules"
  },
  {
    "objectID": "decision.html#reading-guide",
    "href": "decision.html#reading-guide",
    "title": "9  Decision Theory",
    "section": "9.2 Reading Guide",
    "text": "9.2 Reading Guide\nAssociated Readings: Chapter 5, Section 5.8 (beyond Example 5.8.4)\nThis section of the textbook goes into a teeny tiny bit of decision theory, primarily Bayes estimates, but does not approach decision theory broadly, or from a more formal perspective.\n\n9.2.1 Reading Questions\n\nWhat are some examples of commonly-used loss functions?\nWhat are the typical steps to finding a Bayes estimate?\nWhat are the Bayes estimates for absolute error loss and squared error loss?\nWhat does it mean for a decision rule to be admissible (in colloquial language)?\nWhat does it mean for a decision rule to be minimax (in colloquial language)?"
  },
  {
    "objectID": "decision.html#definitions",
    "href": "decision.html#definitions",
    "title": "9  Decision Theory",
    "section": "9.3 Definitions",
    "text": "9.3 Definitions\nLoss Function\nLet \\(\\hat{\\theta}\\) be an estimator for \\(\\theta\\). A loss function associated with \\(\\hat{\\theta}\\) is denoted \\(L(\\hat{\\theta}, \\theta)\\), where \\(L(\\hat{\\theta}, \\theta) \\geq 0\\) and \\(L(\\theta, \\theta) = 0\\). A reasonable loss function will increase the further away \\(\\hat{\\theta}\\) and \\(\\theta\\) are from each other.\nDecision Rule\nFor the purposes of this class, an estimator! In the statistics literature, you will often see this denoted \\(D\\), but we can also denote the decision rule \\(\\hat{\\theta}\\) for this class.\nRisk\nIn words, risk is the expected loss of our decision, given our data. In math,\n\\[\nR(\\hat{\\theta}, \\theta) = E[L(\\hat{\\theta}, \\theta) \\mid \\textbf{Y}] = \\int L(\\hat{\\theta}, \\theta) \\pi(\\theta \\mid \\textbf{Y}) d\\theta\n\\]\nBayes Estimate\nA Bayes estimate is the estimate or decision rule that minimizes risk (expected posterior loss). This is sometimes called a “Bayes rule” in the literature.\nUnique Bayes Rule\nFor a given prior \\(\\pi(\\theta)\\), a decision rule \\(D_\\pi\\) is a unique Bayes rule (estimate) if, for all \\(\\theta\\), a decision rule is a Bayes rule if and only if it is equal to \\(D_\\pi\\). Bayes rules are unique when:\n\nThe loss function used is MSE loss\nThe risk of the Bayes rule is finite\nA \\(\\sigma\\)-field condition is satisfied (well beyond the scope of this course)\n\nFor what we consider in this course, whenever we use MSE loss in this course, the other two conditions will be satisfied.\nAdmissibility\nAn decision rule \\(D\\) is inadmissible if there exists a rule \\(D'\\) such that\n\\[\\begin{align*}\nR(D', \\theta) & \\leq R(D, \\theta) \\quad \\forall \\theta \\\\\nR(D', \\theta) & &lt; R(D, \\theta) \\quad \\text{ for some } \\theta\n\\end{align*}\\]\nwhere \\(R(D, \\theta)\\) is the risk of a decision \\(D\\) for a paramater \\(\\theta\\). If \\(D\\) is not inadmissible, it is admissible."
  },
  {
    "objectID": "decision.html#theorems",
    "href": "decision.html#theorems",
    "title": "9  Decision Theory",
    "section": "9.4 Theorems",
    "text": "9.4 Theorems\nTheorem (Unique Bayes rules are admissible). Any unique Bayes rule is admissible.\nProof. We’ll prove this by contradiction!\nSuppose that \\(D_\\pi\\) is a unique Bayes rule with respect to some prior \\(\\pi(\\theta)\\), and that \\(D_\\pi\\) is inadmissible. Then there exists some other decision rule \\(D'\\) such that \\(R(D', \\theta) \\leq R(D_\\pi, \\theta)\\), for all \\(\\theta\\). Then,\n\\[\\begin{align*}\n    R(D', \\theta) & \\leq R(D_{\\pi}, \\theta) \\quad \\quad \\text{(inadmissibility)} \\\\\n    & = \\inf_D R(D, \\pi) \\quad \\quad \\text{($D_\\pi$ is Bayes)}\n\\end{align*}\\]\nand since \\(R(D', \\theta) \\leq \\inf_D R(D, \\pi)\\), \\(D'\\) is Bayes. But \\(D_\\pi\\) is unique Bayes by assumption, so this is a contradiction.\nTherefore, \\(D_\\pi\\) is admissible."
  },
  {
    "objectID": "decision.html#worked-examples",
    "href": "decision.html#worked-examples",
    "title": "9  Decision Theory",
    "section": "9.5 Worked Examples",
    "text": "9.5 Worked Examples\nProblem 1: Show that the posterior median is the decision rule that minimizes risk with respect to absolute loss, \\(L(\\hat{\\theta}, \\theta) = |\\hat{\\theta} - \\theta|\\).\n\n\nSolution:\n\nWe can write the risk with respect to absolute loss as\n\\[\\begin{align*}\n    R(\\theta_0, \\theta) & = E[L(\\theta_0, \\theta) \\mid \\textbf{Y}] \\\\\n    & = \\int L(\\theta_0, \\theta) \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int |\\theta_0 - \\theta| \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int_{I\\{\\theta_0 \\geq \\theta \\}} \\left( \\theta_0 - \\theta \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{I\\{\\theta_0 &lt; \\theta \\}} \\left(  \\theta - \\theta_0 \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int_{-\\infty}^{\\theta_0} \\left( \\theta_0 - \\theta \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{\\theta_0}^\\infty \\left( \\theta - \\theta_0 \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta\n\\end{align*}\\]\nTaking the derivative with respect to \\(\\theta_0\\), and setting this equal to zero we get\n\\[\\begin{align*}\n    0 & \\equiv \\frac{\\partial}{\\partial \\theta_0} R(\\theta_0, \\theta) \\\\\n    & = \\frac{\\partial}{\\partial \\theta_0} \\left( \\int_{-\\infty}^{\\theta_0} \\left( \\theta_0 - \\theta \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{\\theta_0}^\\infty \\left( \\theta - \\theta_0 \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta  \\right) \\\\\n    & =  \\left( \\theta_0 - \\theta_0 \\right) \\pi(\\theta_0 \\mid \\textbf{y}) - \\int_{-\\infty}^{\\theta_0} \\pi(\\theta \\mid \\textbf{y}) d\\theta - \\left( \\theta_0 - \\theta_0 \\right) \\pi(\\theta_0 \\mid \\textbf{y}) + \\int_{\\theta_0}^\\infty \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = - \\int_{-\\infty}^{\\theta_0} \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{\\theta_0}^\\infty \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    \\int_{-\\infty}^{\\theta_0} \\pi(\\theta \\mid \\textbf{y}) d\\theta & = \\int_{\\theta_0}^\\infty \\pi(\\theta \\mid \\textbf{y}) d\\theta\n\\end{align*}\\]\n(recalling that \\(\\int_{-\\infty}^x f(y) dy = f(x)\\) and \\(\\int_x^\\infty f(y) dy = -f(x)\\) and applying chain rule), and note that these two integrals are equal when \\(\\theta_0\\) is the posterior median.\n\nProblem 2: Show that the posterior mode is the decision rule that minimizes risk with respect to 0-1 loss,\n\\[\nL(\\hat{\\theta}, \\theta) = \\begin{cases} 0 & \\text{if $\\hat{\\theta} = \\theta$} \\\\ 1 & \\text{if $\\hat{\\theta} \\neq \\theta$}\\end{cases}\n\\]\nwhen \\(\\theta\\) is a discrete random variable.\n\n\nSolution:\n\nNote that we can rewrite the 0-1 loss function as \\(L(\\hat{\\theta},\\theta) = 1 - I\\{\\hat{\\theta} = \\theta\\}\\). Then we can write,\n\\[\\begin{align*}\n    R(\\theta_0, \\theta) & = E[L(\\theta_0, \\theta) \\mid \\textbf{Y}] \\\\\n    & = \\sum_{\\theta} L(\\theta_0, \\theta) \\pi(\\theta \\mid \\textbf{y})  \\\\\n    & = \\sum_{\\theta} \\left( 1 - I\\{\\theta_0 = \\theta\\} \\right) \\pi(\n    \\theta \\mid \\textbf{y} )  \\\\\n    & = \\sum_{\\theta} \\pi(\n    \\theta \\mid \\textbf{y} ) - \\sum_{\\theta} I\\{\\theta_0 = \\theta\\} \\pi(\n    \\theta \\mid \\textbf{y})  \\\\\n    & = 1 - \\pi(\\theta_0 \\mid \\textbf{y})\n\\end{align*}\\]\nsince pmfs sum to \\(1\\). Then taking the derivative and setting this equal to zero gives\n\\[\\begin{align*}\n    0 & \\equiv \\frac{\\partial}{\\partial \\theta_0} R(\\theta_0, \\theta) \\\\\n    & = \\frac{\\partial}{\\partial \\theta_0}  \\left( 1 - \\pi(\\theta_0 \\mid \\textbf{y}) \\right) \\\\\n    & = \\frac{\\partial}{\\partial \\theta_0} \\pi(\\theta_0 \\mid \\textbf{y})\n\\end{align*}\\]\nand the solution to this equation is, by definition, the posterior mode.\n\nNote: For the case where \\(\\theta\\) is a continuous random variable, we need something called a Dirac delta function to prove this. The reasoning for why we need this (and the proof, which is similar to the discrete case) is given below.\n\n\nSolution for continuous \\(\\theta\\) :\n\nNote that we can rewrite the 0-1 loss function as \\(L(\\hat{\\theta},\\theta) = 1 - \\delta\\{\\hat{\\theta} - \\theta\\}\\), where \\(\\delta\\) is the Dirac delta function. Then we can write,\n\\[\\begin{align*}\n    R(\\theta_0, \\theta) & = E[L(\\theta_0, \\theta) \\mid \\textbf{Y}] \\\\\n    & = \\int L(\\theta_0, \\theta) \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int \\left( 1 - \\delta(\\theta_0 - \\theta) \\right) \\pi(\n    \\theta \\mid \\textbf{y} ) d\\theta  \\\\\n    & = \\int \\pi(\n    \\theta \\mid \\textbf{y} ) d\\theta  - \\int \\delta(\\theta_0 - \\theta) \\pi(\n    \\theta \\mid \\textbf{y}) d\\theta   \\\\\n    & = 1 - \\pi(\\theta_0 \\mid \\textbf{y})\n\\end{align*}\\]\nsince pdfs integrate to \\(1\\). The reason why we can’t use the same indicator definition as for the discrete case is because the integral of an indicator that is only positive at a single observation is zero. The Dirac delta function, on the other hand, has positive mass (equal to 1) at \\(\\theta_0 - \\theta = 0\\). Take Projects in Real Analysis to learn more! Taking the derivative and setting this equal to zero gives the same result as in the discrete case."
  },
  {
    "objectID": "probability.html#learning-objectives",
    "href": "probability.html#learning-objectives",
    "title": "1  Probability: A Brief Review",
    "section": "1.1 Learning Objectives",
    "text": "1.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDistinguish between important probability models (e.g., Normal, Binomial)\nDerive the expectation and variance of a single random variable or a sum of random variables\nDefine the moment generating function and use it to find moments or identify pdfs"
  },
  {
    "objectID": "probability.html#reading-guide",
    "href": "probability.html#reading-guide",
    "title": "1  Probability: A Brief Review",
    "section": "1.2 Reading Guide",
    "text": "1.2 Reading Guide\nAssociated Readings: Chapters 2-4 (pages 15-277)\n\n1.2.1 Reading Questions\n\nWhich probability distributions are appropriate for quantitative (continuous) random variables?\nWhich probability distributions are appropriate for categorical random variables?\nIndependently and Identically Distributed (iid) random variables are an incredibly important assumption involved in many statistical methods. Why do you think it might be important/useful for random variables to have this property?"
  },
  {
    "objectID": "probability.html#definitions",
    "href": "probability.html#definitions",
    "title": "1  Probability: A Brief Review",
    "section": "1.3 Definitions",
    "text": "1.3 Definitions\nYou are expected to know the following definitions:\nRandom Variable\nA random variable is a function that takes inputs from a sample space of all possible outcomes, and outputs real values or probabilities. As an example, consider a coin flip. The sample space of all possible outcomes consists of “heads” and “tails”, and each outcome is associated with a probability (50% each, for a fair coin). For our purposes, you should know that random variables have probability density (or mass) functions, and are either discrete or continuous based on the number of possible outcomes a random variable may take. Random variables are often denoted with capital Roman letters, like \\(X\\), \\(Y\\), \\(Z\\), etc.\nProbability density function (discrete, continuous)\n\nNote: I don’t care if you call a pmf a pdf… I will probably do this continuously throughout the semester. We don’t need to be picky about this in MATH/STAT 455.\n\nThere are many different accepted ways to write the notation for a pdf of a random variables. Any of the following are perfectly appropriate for this class: \\(f(x)\\), \\(\\pi(x)\\), \\(p(x)\\), \\(f_X(x)\\). I typically use either \\(\\pi\\) or \\(p\\), but might mix it up occasionally.\nKey things I want you to know about probability density functions:\n\n\\(\\pi(x) \\geq 0\\), everywhere. This should make sense (hopefully) because probabilities cannot be negative!\n\\(\\int_{-\\infty}^\\infty \\pi(x) = 1\\). This should also (hopefully) makes sense. Probabilities can’t be greater than one, and the probability of event occurring at all (ever) should be equal to one, if the event \\(x\\) is a random variable.\n\nCumulative distribution function (discrete, continuous)\nCumulative distribution functions we’ll typically write as \\(F_X(x)\\). or \\(F(x)\\), for short. It is important to know that\n\\[\nF_X(x) = \\Pr(X \\leq x),\n\\]\nor in words, “the cumulative distribution function is the probability that a random variable lies before \\(x\\).” If you write \\(\\Pr(X &lt; x)\\) instead of \\(\\leq\\), you’re fine. The probability that a random variable is exactly one number (for an RV with a continuous pdf) is zero anyway, so these are the same thing. Key things I want you to know about cumulative distribution functions:\n\n\\(F(x)\\) is non-decreasing. This is in part where the “cumulative” piece comes in to play. Recall that probabilities are basically integrals or sums. If we’re integrating over something positive, and our upper bound for our integral increases, the area under the curve (cumulative probability) will increase as well.\n\\(0 \\leq F(x) \\leq 1\\) (since probabilities have to be between zero and one!)\n\\(\\Pr(a &lt; X \\leq b) = F(a) - F(b)\\) (because algebra)\n\nJoint probability density function\nA joint probability density function is a probability distribution defined for more than one random variable at a time. For two random variables, \\(X\\) and \\(Z\\), we could write their joint density function as \\(f_{X,Z}(x, z)\\) , or \\(f(x,z)\\) for short. The joint density function encodes all sorts of fun information, including marginal distributions for \\(X\\) and \\(Z\\), and conditional distributions (see next bold definition). We can think of the joint pdf as listing all possible pairs of outputs from the density function \\(f(x,z)\\), for varying values of \\(x\\) and \\(z\\). Key things I want you to know about joint pdfs:\n\nHow to get a marginal pdf from a joint pdf:\nSuppose I want to know \\(f_X(x)\\), and I know \\(f_{X,Z}(x,z)\\). Then I can integrate or “average over” \\(Z\\) to get\n\\[\nf_X(x) = \\int f_{X,Z}(x,z)dz\n\\]\nThe relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule\nHow to obtain a joint pdf for independent random variables: just multiply their marginal pdfs together! This is how we will (typically) think about likelihoods!\nHow to obtain a marginal pdf from a joint pdf when random variables are independent without integrating (think, “separability”)\n\nConditional probability density function\nA conditional pdf denotes the probability distribution for a (set of) random variable(s), given that the value for another (set of) random variable(s) is known. For two random variables, \\(X\\) and \\(Z\\), we could write the conditional distribution of \\(X\\) “given” \\(Z\\) as \\(f_{X \\mid Z}(x \\mid z)\\) , where the “conditioning” is denoted by a vertical bar (in LaTeX, this is typeset using “\\mid”). Key things I want you to know about conditional pdfs:\n\nThe relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule\nHow to obtain a conditional pdf from a joint pdf (again, think Bayes’ rule)\nRelationship between conditional pdfs and independence (see next bold definition)\n\nIndependence\nTwo random variables \\(X\\) and \\(Z\\) are independent if and only if:\n\n\\(f_{X,Z}(x,z) = f_X(x) f_Z(z)\\) (their joint pdf is “separable”)\n\\(f_{X\\mid Z}(x\\mid z) = f_X(x)\\) (the pdf for \\(X\\) does not depend on \\(Z\\) in any way)\nNote that the “opposite” is also true: \\(f_{Z\\mid X}(z\\mid x) = f_Z(z)\\)\n\nIn notation, we denote that two variables are independent as \\(X \\perp\\!\\!\\!\\perp Z\\), or \\(X \\perp Z\\). In LaTeX, the latter is typeset as “\\perp”, and the former is typeset as “\\perp\\!\\!\\!\\perp”. As a matter of personal preference, I (Taylor) prefer \\(\\perp\\!\\!\\!\\perp\\), but I don’t like typing it out every time. Consider using the “\\newcommand” functionality in LaTeX to create a shorthand for this for your documents!\nExpected Value / Expectation\nThe expectation (or expected value) of a random variable is defined as:\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\nExpected value is a weighted average, where the average is over all possible values a random variable can take, weighted by the probability that those values occur. Key things I want you to know about expectation:\n\nThe relationship between expectation, variance, and moments (specifically, that \\(E[X]\\) is the 1st moment!)\nThe “law of the unconscious statistician” (see the Theorems section of this chapter)\nExpectation of linear transformations of random variables (see Theorems section of this chapter)\n\nVariance\nThe variance of a random variable is defined as:\n\\[\nVar[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nIn words, we can read this as “the expected value of the squared deviation from the mean” of a random variable \\(X\\). Key things I want you to know about variance:\n\nThe relationship between expectation, variance, and moments (hopefully clear, given the formula for variance)\nThe relationship between variance and standard deviation: \\(Var(X) = sd(X)^2\\)\nThe relationship between variance and covariance: \\(Var(X) = Cov(X, X)\\)\n\\(Var(X) \\geq 0\\). This should make sense, given that we’re taking the expectation of something “squared” in order to calculate it!\n\\(Var(c) = 0\\) for any constant, \\(c\\).\nVariance of linear transformations of random variables (see Theorems section of this chapter)\n\n\\(r^{th}\\) moment\nThe \\(r^{th}\\) moment of a probability distribution is given by \\(E[X^r]\\). For example, when \\(r = 1\\), the \\(r^{th}\\) moment is just the expectation of the random variable \\(X\\). Key things I want you to know about moments:\n\nThe relationship between moments, expectation, and variance\n\nFor example, if you know the first and second moments of a distribution, you should be able to calculate the variance of a random variable with that distribution!\n\nThe relationship between moments and moment generating functions (see Theorems section of this chapter)\n\nCovariance\nThe covariance of two random variables is a measure of their joint variability. We denote the covariance of two random variables \\(X\\) and \\(Z\\) as \\(Cov(X,Z)\\), and\n\\[\nCov(X, Z) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\n\\]\nSome things I want you to know about covariance:\n\n\\(Cov(X, X) = Var(X)\\)\n\\(Cov(X, Y) = Cov(Y, X)\\) (order doesn’t matter)\n\nMoment Generating Function (MGF)\nThe moment generating function of a random variable \\(X\\) is defined as\n\\[\nM_X(t) = E[e^{tX}]\n\\]\nA few things to note:\n\n\\(M_X(0) = 1\\), always.\nIf two random variables have the same MGF, they have the same probability distribution!\nMGFs are sometimes useful for showing how different random variables are related to each other\n\n\n1.3.1 Distributions Table\nYou are also expected to know the probability distributions contained in Table 1, below. Note that you do not need to memorize the pdfs for these distributions, but you should be familiar with what types of random variables (continuous/quantitative, categorical, integer-valued, etc.) may take on different distributions. The more familiar you are with the forms of the pdfs, the easier/faster it will be to work through problem sets and quizzes.\n\nTable 1. Table of main probability distributions we will work with for MATH/STAT 455.\n\n\n\n\n\n\n\n\nDistribution\nPDF/PMF\nParameters\nSupport\n\n\n\n\nUniform\n\\(\\pi(x) = \\frac{1}{\\beta - \\alpha}\\)\n\\(\\alpha \\in \\mathbb{R}\\), \\(\\beta\\in \\mathbb{R}\\)\n\\(x \\in [\\alpha, \\beta]\\)\n\n\nNormal\n\\(\\pi(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2)\\)\n\\(\\mu \\in \\mathbb{R}\\), \\(\\sigma &gt; 0\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nMultivariate Normal\n\\(\\pi(\\textbf{x}) = (2\\pi)^{-k/2} |\\Sigma|^{-1/2} \\exp(-\\frac{1}{2}(\\textbf{x} - \\mu)^\\top \\Sigma^{-1}(\\textbf{x} - \\mu)))\\)\n\\(\\mu \\in \\mathbb{R}^k\\), \\(\\Sigma \\in \\mathbb{R}^{k\\times k}\\) , positive semi-definite (in practice, almost always positive definite)\n\\(x \\in \\mathbb{R}^{k}\\)\n\n\nGamma\n\\(\\pi(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\\)\n\\(\\alpha \\text{ (shape)}, \\beta \\text{ (rate)} &gt; 0\\)\n\\(x \\in (0,\\infty)\\)\n\n\nChi-squared\n\\(\\pi(x) = \\frac{2^{-\\nu/2}}{\\Gamma(\\nu/2)} x^{\\nu/2 - 1}e^{-x/2}\\)\n\\(\\nu &gt; 0\\)\n\\(x \\in [0, \\infty)\\)\n\n\n\\(F\\)\n\\(\\pi(x) = \\frac{\\Gamma(\\frac{\\nu_1 + \\nu_2}{2})}{\\Gamma(\\frac{\\nu_1}{2}) \\Gamma(\\frac{\\nu_2}{2})} \\left( \\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2} \\left( \\frac{x^{(\\nu_1 - 2)/2}}{\\left( 1 + \\left( \\frac{\\nu_1}{\\nu_2}\\right)x\\right)^{(\\nu_1 + \\nu_2)/2}}\\right)\\)\n\\(\\nu_1, \\nu_2 &gt; 0\\)\n\\(x \\in [0, \\infty)\\)\n\n\nExponential\n\\(\\pi(x) = \\beta e^{-\\beta x}\\)\n\\(\\beta &gt; 0\\)\n\\(x \\in [0, \\infty)\\)\n\n\nStudent-\\(t\\)\n\\(\\pi(x) = \\frac{\\Gamma((\\nu + 1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu \\pi}} (1 + \\frac{x^2}{\\nu})^{-(\\nu + 1)/2}\\)\n\\(\\nu &gt; 0\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nBeta\n\\(\\pi(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha - 1}(1 - x)^{\\beta - 1}\\)\n\\(\\alpha, \\beta &gt; 0\\)\n\\(x \\in [0,1]\\)\n\n\nPoisson\n\\(\\pi(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\n\\(\\lambda &gt; 0\\)\n\\(x \\in \\mathbb{N}\\)\n\n\nBinomial\n\\(\\pi(x) = \\binom{n}{x} p^{x} (1 - p)^{n - x}\\)\n\\(p \\in [0,1], n = \\{0, 1, 2, \\dots\\}\\)\n\\(x \\in \\{0, 1, \\dots, n\\}\\)\n\n\nMultinomial\n\\(\\pi(\\textbf{x}) = \\frac{n!}{x_1! \\dots x_k!} p_1^{x_1} \\dots p_k^{x_k}\\)\n\\(p_i &gt; 0\\), \\(p_1 + \\dots + p_k = 1\\), \\(n = \\{0, 1, 2, \\dots \\}\\)\n\\(\\{ x_1, \\dots, x_k \\mid \\sum_{i = 1}^k x_i = n, x_i \\geq 0 (i = 1, \\dots, k)\\}\\)\n\n\nNegative Binomial\n\\(\\pi(x) = \\binom{x + r - 1}{x} (1-p)^x p^r\\)\n\\(r &gt; 0\\), \\(p \\in [0,1]\\)\n\\(x \\in \\{0, 1, \\dots\\}\\)"
  },
  {
    "objectID": "probability.html#theorems",
    "href": "probability.html#theorems",
    "title": "1  Probability: A Brief Review",
    "section": "1.4 Theorems",
    "text": "1.4 Theorems\n\nLaw of Total Probability\n\\[\nP(A) = \\sum_n P(A \\cap B_n),\n\\]or\n\\[\nP(A) = \\sum_n P(A \\mid B_n) P(B_n)\n\\]\nBayes’ Theorem\n\\[\n\\pi(A \\mid B) = \\frac{\\pi(B \\mid A) \\pi(A)}{\\pi(B)}\n\\]\nRelationship between pdf and cdf\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t)dt\n\\]\n\\[\n\\frac{\\partial}{\\partial y}F_Y(y) = f_Y(y)\n\\]\nExpectation of random variables\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\n\\[\nE[X^2] = \\int_{-\\infty}^\\infty x^2 f(x) dx\n\\]\n\n“Law of the Unconscious Statistician”\n\\[\nE[g(X)] = \\int_{-\\infty}^\\infty g(x)f(x)dx\n\\]\n\nExpectation and variance of linear transformations of random variables\n\\[\nE[cX + b] = c E[X] + b\n\\]\n\\[\nVar[cX + b] = c^2 Var[X]\n\\]\nRelationship between mean and variance\n\\[\nVar[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nAlso, recall that \\(Cov[X, X] = Var[X]\\).\nFinding a marginal pdf from a joint pdf\n\\[\nf_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(x, y) dy\n\\]\nIndependence of random variables and joint pdfs\nIf two random variables are independent, their joint pdf will be separable. For example, if \\(X\\) and \\(Y\\) are independent, we could write\n\\[\nf_{X,Y}(x, y) = f_{X}(x)f_Y(y)\n\\]\nExpected value of a product of independent random variables\nSuppose random variables \\(X_1, \\dots, X_n\\) are independent. Then we can write,\n\\[\nE\\left[\\prod_{i = 1}^n X_i\\right] = \\prod_{i = 1}^n E[X_i]\n\\]\nCovariance of independent random variables\nIf \\(X\\) and \\(Y\\) are independent, then \\(Cov(X, Y) = 0\\). We can show this by noting that\n\n\\[\\begin{align}\nCov(X, Y) & = E[(X - E[X])(Y - E[Y])] \\\\\n& = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\\\\n& = E[XY] - E[XE[Y]] - E[YE[X]] + E[X]E[Y] \\\\\n& =  2E[X]E[Y] - 2E[X]E[Y] \\\\\n& = 0\n\\end{align}\\]\n\nUsing MGFs to find moments\nRecall that the moment generating function of a random variable \\(X\\), denoted by \\(M_X(t)\\) is\n\\[\nM_X(t) = E[e^{tX}]\n\\]\nThen the \\(n\\)th moment of the probability distribution for \\(X\\) , \\(E[X^n]\\), is given by\n\\[\n\\frac{\\partial M_X}{\\partial t^n} \\Bigg|_{t = 0}\n\\]\nwhere the above reads as “the \\(n\\)th derivative of the moment generating function, evaluated at \\(t = 0\\).”\nUsing MGFs to identify pdfs\nMGFs uniquely identify probability density functions. If \\(X\\) and \\(Y\\) are two random variables where for all values of \\(t\\), \\(M_X(t) = M_Y(t)\\), then \\(F_X(x) = F_Y(y)\\).\nCentral Limit Theorem\nThe classical CLT states that for independent and identically distributed (iid) random variables \\(X_1, \\dots, X_n\\), with expected value \\(E[X_i] = \\mu\\) and \\(Var[X_i] = \\sigma^2 &lt; \\infty\\), the sample average (centered and standardized) converges in distribution to a standard normal distribution at a root-\\(n\\) rate. Notationally, this is written as\n\\[\n\\sqrt{n} (\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\n A fun aside: this is only one CLT, often referred to as the Levy CLT. There are other CLTs, such as the Lyapunov CLT and Lindeberg-Feller CLT!\n\n\n1.4.1 Transforming Continuous Random Variables\nWe will often take at face value previously proven relationships between random variables. What I mean by this, as an example, is that it is a nice (convenient) fact that a sum of two independent normal random variables is still normally distributed, with a nice form for the mean and variance. In particular, if \\(X \\sim N(\\mu, \\sigma^2)\\) and \\(Y \\sim N(\\theta, \\nu^2)\\), then \\(X + Y \\sim N(\\mu + \\theta, \\sigma^2 + \\nu^2)\\). Most frequently used examples of these sorts of relationships can be found in the ``Related Distributions” section of the Wikipedia page for a given probability distribution. Unless I explicitly ask you to derive/show how certain variables are related to each other, you can just state the known relationship, use it, and move on!\nIf I do ask you to derive/show these things, there a few different ways we can go about this. For this course, I only expect you to know the “CDF method” for one function of one random variable, as we’ll demonstrate below.\nTheorem. Let \\(X\\) be a continuous random variable with pdf \\(f_X(x)\\). Define a new random variable \\(Y = g(X)\\), for nice* functions \\(g\\). Then \\(f_Y(y) = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))}\\).\n *By nice functions we mean functions that are strictly increasing and smooth on the required range. As an example, \\(exp(x)\\) is a smooth, strictly increasing function; \\(|x|\\) is not on the whole real line, but is from \\((0, \\infty)\\) (where a lot of useful pdfs are defined). For the purposes of this class, every function that you will need to do this for will be “nice.” Note that there are also considerations that need to be taken regarding the range of continuous random variables when considering transforming them. We will mostly ignore these considerations in this class, but a technically complete derivation (or proof) must consider them.\nProof. We can write\n\\[\\begin{align*}\n    f_Y(y) & = \\frac{\\partial}{\\partial y} F_Y(y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(Y \\leq y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(g(X) \\leq y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(X \\leq g^{-1}(y)) \\\\\n    & = \\frac{\\partial}{\\partial y} F_X(g^{-1}(y)) \\\\\n    & = f_X(g^{-1}(y)) \\times \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align*}\\]\nwhere to obtain the last equality we use chain rule! Now we require some statistical trickery to continue… (note that this method is called the “CDF method” because we go through the CDF to derive the distribution for \\(Y\\))\nYou will especially see this in the Bayes chapter of our course notes, but it is often true that our lives are made easier as statisticians if we multiply things by one, or add zero. What exactly do I mean? Rearranging gross looking formulas into things we are familiar with (like pdfs, for example) often makes our lives easier and allows us to avoid dealing with such grossness. Here, the grossness is less obvious, but nonetheless relevant. Note that we can write\n\\[\\begin{align*}\n    y & = y \\\\\n    y & = g(g^{-1}(y)) \\\\\n    \\frac{\\partial}{\\partial y} y & = \\frac{\\partial}{\\partial y} g(g^{-1}(y)) \\\\\n    1 & = g'(g^{-1}(y)) \\frac{\\partial}{\\partial y} g^{-1}(y) \\hspace{1cm} \\text{(chain rule again!)} \\\\\n    \\frac{1}{g'(g^{-1}(y))} & = \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align*}\\]\nThe right-hand side should look familiar: it is exactly what we needed to “deal with” in our proof! Returning to that proof, we have\n\\[\\begin{align*}\n    f_Y(y) & = f_X(g^{-1}(y)) \\times \\frac{\\partial}{\\partial y} g^{-1}(y) \\\\\n    & = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))}\n\\end{align*}\\]\nas desired."
  },
  {
    "objectID": "probability.html#worked-examples",
    "href": "probability.html#worked-examples",
    "title": "1  Probability: A Brief Review",
    "section": "1.5 Worked Examples",
    "text": "1.5 Worked Examples\nProblem 1: Suppose \\(X \\sim Exponential(\\lambda)\\). Calculate \\(E[X]\\) and \\(Var[X]\\).\n\n\nSolution:\n\nWe know that \\(f(x) = \\lambda e^{-\\lambda x}\\). If we can calculate \\(E[X]\\) and \\(E[X^2]\\), then we’re basically done! We can write\n\\[\\begin{align*}    \nE[X] & = \\int_0^\\infty x \\lambda e^{-\\lambda x} dx \\\\    \n& = \\lambda \\int_0^\\infty x e^{-\\lambda x} dx\n\\end{align*}\\]\nAnd now we need integration by parts! Set \\(u = x\\), \\(dv = e^{-\\lambda x} dx\\). Then \\(du = 1dx\\) and \\(v = \\frac{-1}{\\lambda} e^{-\\lambda x}\\). Since \\(\\int u dv = uv - \\int vdu\\), we can continue\n\\[\\begin{align*}    \nE[X] & = \\lambda \\int_0^\\infty x e^{-\\lambda x} dx \\\\    \n& = \\lambda \\left( -\\frac{x}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  - \\int_0^\\infty \\frac{-1}{\\lambda} e^{-\\lambda x} dx \\right) \\\\    \n& = \\lambda \\left( - \\int_0^\\infty \\frac{-1}{\\lambda} e^{-\\lambda x} dx \\right) \\\\    \n& = \\lambda \\left( \\frac{-1}{\\lambda^2} e^{-\\lambda x}  \\bigg|_0^\\infty \\right) \\\\    \n& = \\frac{-1}{\\lambda} e^{-\\lambda x}  \\bigg|_0^\\infty \\\\    \n& = \\frac{1}{\\lambda} e^{-0} \\\\    \n& = \\frac{1}{\\lambda}\n\\end{align*}\\]\nWe can follow a similar process to get \\(E[X^2]\\) (using the law of the unconscious statistician!). We can write\n\\[\\begin{align*}\n    E[X^2] & = \\int_0^\\infty x^2 \\lambda e^{-\\lambda x} dx \\\\\n    & = \\lambda \\int_0^\\infty x^2 e^{-\\lambda x} dx\n\\end{align*}\\]\nAnd now we need integration by parts again! Set \\(u = x^2\\), \\(dv = e^{-\\lambda x} dx\\). Then \\(du = 2xdx\\) and \\(v = \\frac{-1}{\\lambda} e^{-\\lambda x}\\). Since \\(\\int u dv = uv - \\int vdu\\), we can continue\n\\[\\begin{align*}\n    E[X] & = \\lambda \\int_0^\\infty x^2 e^{-\\lambda x} dx \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  - \\int_0^\\infty \\frac{-2}{\\lambda} xe^{-\\lambda x} dx \\right) \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  + \\frac{2}{\\lambda} \\int_0^\\infty  xe^{-\\lambda x} dx \\right) \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  + \\frac{2}{\\lambda^3} \\right)\\\\\n    & = \\lambda \\left( 0  + \\frac{2}{\\lambda^3} \\right) \\\\\n    & = \\frac{2}{\\lambda^2}\n\\end{align*}\\]\nNow we can calculate \\(Var[X] = E[X^2] - E[X]^2\\) as \\[\nVar[X] = E[X^2] - E[X]^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\n\\] And so we have \\(E[X] = \\frac{1}{\\lambda}\\) and \\(Var[X] = \\frac{1}{\\lambda^2}\\).\n\nProblem 2: Show that an exponentially distributed random variable is “memoryless”, i.e. show that \\(\\Pr(X &gt; s + x \\mid X &gt; s) = \\Pr(X &gt; x)\\), \\(\\forall s\\).\n\n\nSolution:\n\nRecall that the CDF of an exponential distribution is given by \\(F(x) = 1-e^{-\\lambda x}\\). Thanks to Bayes rule, we can write\n\\[\\begin{align*}\n    \\Pr(X &gt; s + x \\mid X &gt; s) & = \\frac{\\Pr(X &gt; s + x , X &gt; s)}{\\Pr(X &gt; s)} \\\\\n    & = \\frac{\\Pr(X &gt; s + x)}{\\Pr(X &gt; s)} \\\\\n    & = \\frac{1 - \\Pr(X &lt; s + x)}{1 - \\Pr(X &lt; s)} \\\\\n    & = \\frac{1 - F(s + x)}{1 - F(s)}\n\\end{align*}\\]\nwhere the second equality is true because \\(x &gt; 0\\). Then we can write\n\\[\\begin{align*}\n    \\Pr(X &gt; s + x \\mid X &gt; s) & = \\frac{1 - F(s + x)}{1 - F(s)} \\\\\n    & = \\frac{1 - \\left(1 - e^{-\\lambda(s + x)}\\right)}{1 - \\left(1 - e^{-\\lambda s}\\right)} \\\\\n    & = \\frac{e^{-\\lambda(s + x)}}{e^{-\\lambda s}} \\\\\n    & = \\frac{e^{-\\lambda s - \\lambda x}}{e^{-\\lambda s}} \\\\\n    & = e^{-\\lambda x} \\\\\n    & = 1 - F(x) \\\\\n    & = \\Pr(X &gt; x)\n\\end{align*}\\]\nand we’re done!\n\nProblem 3: Suppose \\(X \\sim Exponential(1/\\lambda)\\), and \\(Y \\mid X \\sim Poisson(X)\\). Show that \\(Y \\sim Geometric(1/(1 + \\lambda))\\).\n\n\nSolution:\n\nNote that we can write \\(f(x, y) = f(y \\mid x) f(x)\\), and \\(f(y) = \\int f(x, y) dx\\). Then\n\\[\nf(x, y) = \\left( \\frac{1}{\\lambda} e^{-x/\\lambda} \\right) \\left( \\frac{x^y e^{-x}}{y!} \\right)\n\\] And so,\n\\[\\begin{align*}\n    f(y) & = \\int f(x, y) dx \\\\\n    & = \\int \\left( \\frac{1}{\\lambda} e^{-x/\\lambda} \\right) \\left( \\frac{x^y e^{-x}}{y!} \\right) dx \\\\\n    & = \\frac{1}{\\lambda y!} \\int x^y e^{-x(1 + \\lambda)/\\lambda} dx\n\\end{align*}\\]\nAnd we can again use integration by parts! Let \\(u = x^y\\) and \\(dv = e^{-x(1 + \\lambda)/\\lambda} dx\\). Then we have \\(du = yx^{y-1} dx\\) and \\(v = -\\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda}\\), and we can write\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda y!} \\int x^y e^{-x(1 + \\lambda)/\\lambda} dx \\\\\n    & = \\frac{1}{\\lambda y!} \\left( x^y \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} \\bigg|_{x = 0}^{x = \\infty}  + \\int \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} yx^{y-1} dx\\right) \\\\\n    & = \\frac{1}{\\lambda y!} \\left(  \\int \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} yx^{y-1} dx \\right) \\\\\n    & = \\frac{1}{\\lambda y!} \\left( \\frac{\\lambda }{1 + \\lambda} \\right) y \\left(  \\int e^{-x(1 + \\lambda)/\\lambda} x^{y-1} dx \\right)\n\\end{align*}\\]\nThis looks gross, but it’s actually not so bad. Note that, since \\(Y\\) is Poisson, it can only take integer values beginning at 1! Then we can repeat the process of integration by parts \\(y\\) times in order to get rid of \\(x^{y\\dots}\\) term on the inside of the integral. Specifically, each time we do this process we will pull out a \\(\\left( \\frac{\\lambda }{1 + \\lambda} \\right)\\), and a \\(y - i\\) for the \\(i\\)th integration by parts step (try this one or two steps for yourself to see how it will simplify if you find this unintuitive!). We end up with,\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda y!} \\left( \\frac{\\lambda }{1 + \\lambda} \\right)^y y! \\\\\n    & = \\frac{1}{\\lambda} \\left(\\frac{\\lambda}{1 + \\lambda}\\right)^y\n\\end{align*}\\]\nNow let \\(p = \\frac{1}{1 + \\lambda}\\). If we can show that \\(f(y) \\sim Geometric(p)\\) then we’re done. Note that \\(1 - p = \\lambda/(1 + \\lambda)\\). We have\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda} (1 - p)^y \\\\\n    & = \\frac{1}{\\lambda} (1 - p)^{y-1} (1-p) \\\\\n    & = (1 - p)^{y-1} \\frac{1}{\\lambda} \\left( \\frac{\\lambda}{1 + \\lambda} \\right) \\\\\n    & = (1 - p)^{y-1} \\left( \\frac{1}{1 + \\lambda} \\right) \\\\\n    & = (1 - p)^{y-1} p\n\\end{align*}\\]\nwhich is exactly the pdf of a geometric random variable with parameter \\(p\\) and trials that begin at 1.\nAn alternative solution (which perhaps embodies the phrase “work smarter, not harder”) actually doesn’t involve integration by parts at all! As statisticians, we typically like to avoid actually integrating anything whenever possible, and this is often achieved by manipulating algebra enough to essentially “create” a pdf out of what we see (since pdfs integrate to \\(1\\)!). Massive props to a student for solving this problem in a much “easier” way, answer below:\n\\[\\begin{align*}\n        f(y) &= \\int_{0}^{\\infty} f(y \\mid x) f(x) dx \\\\\n        &= \\int_{0}^{\\infty} (\\frac{1}{\\lambda}e^{-\\frac{x}{\\lambda}}) (\\frac{x^y}{y!} e^{-x}) dx \\\\\n        &= \\frac{1}{\\lambda y!} \\int_{0}^{\\infty} x^y e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx \\\\\n        &= \\frac{1}{\\lambda y!} \\int_{0}^{\\infty} \\frac{(\\frac{1+\\lambda}{\\lambda})^{y+1}}{(\\frac{1+\\lambda}{\\lambda})^{y+1}} \\frac{\\Gamma(y+1)}{\\Gamma(y+1)} x^{(y+1)-1} e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx\\\\\n        &= \\frac{\\Gamma(y+1)}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} \\int_{0}^{\\infty} \\frac{(\\frac{1+\\lambda}{\\lambda})^{y+1}}{\\Gamma(y+1)} x^{(y+1)-1} e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx\\\\\n        &= \\frac{\\Gamma(y+1)}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} (1)\\\\\n        &= \\frac{y!}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} \\\\\n        &= \\frac{\\lambda^{-1}}{(\\frac{1+\\lambda}{\\lambda})^{y+1}}\\\\\n        &= \\frac{\\lambda^y}{(1+\\lambda)^{y+1}}\\\\\n        &= \\frac{1}{(1+\\lambda)} \\frac{\\lambda^y}{(1+\\lambda)^y}\\\\\n        &= \\frac{1}{(1+\\lambda)} (1 - \\frac{1}{(1+\\lambda)})^y \\\\\n        &=p(1-p)^y\\qquad (\\text{where }p=\\frac{1}{1+\\lambda})\n    \\end{align*}\\]\nNote that we arrive at a slightly different answer with this approach. Specifically, we arrive at the pdf of a geometric random variable with parameter \\(p\\) and trials that begin at 0, as opposed to 1. There’s some subtlety here that we’re going to choose to ignore.\n\nProblem 4: Suppose that \\(X \\sim N(\\mu, \\sigma^2)\\), and let \\(Y = \\frac{X - \\mu}{\\sigma}\\). Find the distribution of \\(Y\\) (simplifying all of your math will be useful for this problem).\n\n\nSolution:\n\nTo solve this problem, we can use the theorem on transforming continuous random variables. We must first define our function \\(g\\) that relates \\(X\\) and \\(Y\\). In this case, we have \\(g(a) = \\frac{a - \\mu}{\\sigma}\\). Now all we need to do is collect the mathematical “pieces” we need to use theorem: \\(g^{-1}(a)\\), and \\(g'(a)\\), and finally, the pdf of a normal random variable. We have\n\\[\\begin{align*}\n    f_X(x) & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2) \\\\\n    g^{-1}(a) & = \\sigma a + \\mu \\\\\n    g'(a) & = \\frac{\\partial}{\\partial a} \\left(\\frac{a - \\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}\n\\end{align*}\\]\nPutting it all together, we have\n\\[\\begin{align*}\n    f_Y(y) & = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))} \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (\\sigma y + \\mu - \\mu)^2) \\times \\sigma \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp(-\\frac{1}{2\\sigma^2} (\\sigma y)^2) \\times \\sigma \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2\\sigma^2} \\sigma^2 y^2) \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2} y^2)\n\\end{align*}\\]\nand note that this is the pdf of a normally distributed random variable with mean \\(0\\) and variance \\(1\\)! Thus, we have shown that \\(\\frac{X - \\mu}{\\sigma} \\sim N(0,1)\\). Fun Fact: If this random variable reminds you of a Z-score, it should!\n\nProblem 5: Suppose the joint pdf of two random variables \\(X\\) and \\(Y\\) is given by \\(f_{X,Y}(x,y) = \\lambda \\beta e^{-x\\lambda - y\\beta}\\). Determine if \\(X\\) and \\(Y\\) are independent, showing why or why not.\n\n\nSolution:\n\nTo determine whether \\(X\\) and \\(Y\\) are independent (or not), we need to determine if their joint pdf is “separable.” Doing some algebra, we can see that\n\\[\\begin{align*}    \nf_{X,Y}(x,y) & = \\lambda \\beta e^{-x \\lambda - y\\beta} \\\\    \n& = \\lambda \\beta e^{-x \\lambda} e^{-y \\beta} \\\\   \n& = \\left( \\lambda  e^{-x \\lambda} \\right) \\left( \\beta e^{-y \\beta} \\right)\n\\end{align*}\\]\nand so since we can write the joint distribution as a function of \\(X\\) multiplied by a function of \\(Y\\), \\(X\\) and \\(Y\\) are independent (and in this case, both have exponential distributions).\n\nProblem 6: Suppose the joint pdf of two random variables \\(X\\) and \\(Y\\) is given by \\(f_{X,Y}(x,y) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\binom{n}{y} x^{y + \\alpha - 1} (1-x)^{n-y + \\beta - 1}\\). Determine if \\(X\\) and \\(Y\\) are independent, showing why or why not.\n\n\nSolution:\n\nTo determine whether \\(X\\) and \\(Y\\) are independent (or not), we need to determine if their joint pdf is “separable.” Right away, we should note that a piece of the pdf contains \\(x^y\\), and therefore we are never going to be able to fully separate out this joint pdf into a function of \\(x\\) times a function \\(y\\). Therefore, \\(X\\) and \\(Y\\) are not independent. In this case, we actually have \\(X \\sim Beta(\\alpha, \\beta)\\), and \\(Y \\mid X \\sim Binomial(n, y)\\) (we’ll return to this example in the Bayes chapter!)."
  },
  {
    "objectID": "mle.html#learning-objectives",
    "href": "mle.html#learning-objectives",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive maximum likelihood estimators for parameters of common probability density functions\nCalculate maximum likelihood estimators “by hand” for common probability density functions\nExplain (in plain English) why maximum likelihood estimation is an intuitive approach to estimating unknown parameters using a combination of (1) observed data, and (2) a distributional assumption"
  },
  {
    "objectID": "mle.html#reading-guide",
    "href": "mle.html#reading-guide",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.2 Reading Guide",
    "text": "2.2 Reading Guide\nAssociated Readings: Chapter 5 (Introduction through Example 5.2.5)\n\n2.2.1 Reading Questions\n\nWhat is the intuition behind the maximum likelihood estimation (MLE) approach?\nWhat are the typical steps to find a MLE? (see Ex 5.2.1, 5.2.2, and Case Study 5.2.1; work through at least one of these examples in detail, filling in any steps that the textbook left out)\nAre there ever situations when the typical steps to finding a MLE don’t work? If so, what can we do instead to find the MLE? (see Ex 5.2.3, 5.2.4)\nHow do the steps to finding a MLE change when we have more than one unknown parameter? (see Ex 5.2.5)"
  },
  {
    "objectID": "mle.html#definitions",
    "href": "mle.html#definitions",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.3 Definitions",
    "text": "2.3 Definitions\nYou are expected to know the following definitions:\nParameter\nIn a frequentist* framework, a parameter is a fixed, unknown truth (very philosophical). By fixed, I mean “not random”. We assume that there is some true unknown value, governing the generation of all possible random observations of all possible people and things in the whole world. We sometimes call this unknown governing process the “superpopulation” (think: all who ever have been, all who are, and all who ever will be).\nPractically speaking, parameters are things that we want to estimate, and we will estimate them using observed data!\n*Two main schools of thought in statistics are: (1) Frequentist (everything you’ve ever learned so far in statistics, realistically), and (2) Bayesian. We’ll cover the latter, and differences between the two, in a later chapter. There’s also technically Fiducial inference as a third school of thought, but that one’s never been widely accepted.\nStatistic/Estimator\nA statistic (or “estimator”) is a function of your data, used to “estimate” an unknown parameter. Often, statistics/estimators will be functions of means or averages, as we’ll see in the worked examples for this chapter!\nLikelihood Function\nLet \\(x_1, \\dots, x_n\\) be a sample of size \\(n\\) of independent observations from the probability density function \\(f_X(x \\mid \\boldsymbol{\\theta})\\), where \\(\\boldsymbol{\\theta}\\) is a set of unknown parameters that define the pdf. Then the likelihood function \\(L(\\boldsymbol{\\theta})\\) is the product of the pdf evaluated at each \\(x_i\\),\n\\[\nL(\\boldsymbol{\\theta}) = \\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta}).\n\\]\nNote that this looks exactly like the joint pdf for \\(n\\) independent random variables, but it is interpreted differently. A likelihood is a function of parameters, given a set of observations (random variables). A joint pdf is a function of random variables.\nNote: The likelihood function is one of the reasons why we like independent observations so much! If observations aren’t independent, we can’t simply multiply all of their pdfs together to get a likelihood function.\nMaximum Likelihood Estimate (MLE)\nLet \\(L(\\boldsymbol{\\theta}) = \\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta})\\) be the likelihood function corresponding to a random sample of observations \\(x_1, \\dots, x_n\\). If \\(\\boldsymbol{\\theta}_e\\) is such that \\(L(\\boldsymbol{\\theta}_e) \\geq L(\\boldsymbol{\\theta})\\) for all possible values \\(\\boldsymbol{\\theta}\\), then \\(\\boldsymbol{\\theta}_e\\) is called a maximum likelihood estimate for \\(\\boldsymbol{\\theta}\\).\nLog-likelihood\nIn statistics, when we say “log,” we essentially always mean “ln” (or, natural log). The log-likelihood is then, hopefully unsurprisingly, given by \\(\\log(L(\\boldsymbol{\\theta}))\\). One thing that’s useful to note (and will come in handy when calculating MLEs, is that the log of a product is equal to a sum of logs. For likelihoods, that means\n\\[\n\\log(L(\\boldsymbol{\\theta})) = \\log \\left(\\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta})\\right) = \\sum_{i = 1}^n \\log(f_X(x_i \\mid \\boldsymbol{\\theta}))\n\\]\nThis will end up making it much easier to take derivatives than needing to deal with products!\nOrder Statistic\nThe \\(k\\)th order statistic is equal to a sample’s \\(k\\)th smallest value. Practically speaking, there are essentially three order statistics we typically care about: the minimum, the median, and the maximum. We denote the minimum (or, first order statistic) in a sample of random variables \\(X_1, \\dots, X_n\\) as \\(X_{(1)}\\) , the maximum as \\(X_{(n)}\\), and the median \\(X_{(m+1)}\\) where \\(n = 2m + 1\\) when \\(n\\) is odd. Note that median is in fact not an order statistic if \\(n\\) is even (since the median is an average of two values, \\(X_{(m)}\\) and \\(X_{(m+1)}\\), in this case.\nSee Example 5.2.4 in the Textbook for an example of where order statistics occasionally come into play when calculating maximum likelihood estimates."
  },
  {
    "objectID": "mle.html#theorems",
    "href": "mle.html#theorems",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.4 Theorems",
    "text": "2.4 Theorems\nNone for this chapter!"
  },
  {
    "objectID": "mle.html#worked-examples",
    "href": "mle.html#worked-examples",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.5 Worked Examples",
    "text": "2.5 Worked Examples\nProblem 1: Suppose we observe \\(n\\) independent observations \\(X_1, \\dots, X_n \\sim Bernoulli(p)\\), where \\(f_X(x) = p^x(1-p)^{1-x}\\). Find the MLE of \\(p\\).\n\n\nSolution:\n\nWe can write the likelihood function as\n\\[\nL(p) = \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i}\n\\]\nThen the log-likelihood is given by\n\\[\\begin{align*}\n\\log(L(p)) & = \\log \\left[ \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\right] \\\\\n& = \\sum_{i = 1}^n \\log \\left[p^{x_i} (1-p)^{1 - x_i} \\right] \\\\\n& = \\sum_{i = 1}^n \\left[ \\log(p^{x_i}) + \\log((1-p)^{1-x_i}) \\right] \\\\\n& = \\sum_{i = 1}^n \\left[ x_i \\log(p) + (1 - x_i) \\log(1-p) \\right] \\\\\n& = \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p) \\sum_{i = 1}^n (1 - x_i)  \\\\\n& = \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p)  (n - \\sum_{i = 1}^n x_i)\n\\end{align*}\\]\nWe can take the derivative of the log-likelihood with respect to \\(p\\), and set it equal to zero…\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial p} \\log(L(p)) & = \\frac{\\partial}{\\partial p} \\left[ \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p)  (n - \\sum_{i = 1}^n x_i) \\right] \\\\\n& = \\frac{\\sum_{i = 1}^n x_i }{p} - \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n0 & \\equiv \\frac{\\sum_{i = 1}^n x_i }{p} - \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n\\frac{\\sum_{i = 1}^n x_i }{p}  & = \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n(1-p) \\sum_{i = 1}^n x_i & = p (n - \\sum_{i = 1}^n x_i) \\\\\n\\sum_{i = 1}^n x_i - p\\sum_{i = 1}^n x_i & = pn - p \\sum_{i = 1}^n x_i \\\\\n\\sum_{i = 1}^n x_i & = pn \\\\\n\\frac{1}{n} \\sum_{i = 1}^n x_i & = p\n\\end{align*}\\]\nand by solving for \\(p\\), we get that the MLE of \\(p\\) is equal to \\(\\frac{1}{n}\\sum_{i = 1}^n x_i\\). We will often see that the MLEs of parameters are functions of sample averages (in this case, just the identity function!).\n\nProblem 2: Suppose \\(X_1, X_2, \\dots, X_n\\) are a random sample from the Normal pdf with parameters \\(\\mu\\) and \\(\\sigma^2\\):\n\\[\nf_X(x ; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2},\n\\]\nfor \\(-\\infty &lt; x &lt; \\infty, \\ -\\infty &lt; \\mu &lt; \\infty,\\) and \\(\\sigma^2 &gt; 0\\). Find the MLEs of \\(\\mu\\) and \\(\\sigma^2\\). (Note that this is Question 5 on the MLE section of Problem Set 1! For your HW, try your best to do this problem from scratch, without looking at the course notes!)\n\n\nSolution:\n\nSince we are dealing with a likelihood with two parameters, we’ll need to solve a system of equations to obtain the MLEs for \\(\\mu\\) and \\(\\sigma^2\\).\n\\[\\begin{align*}\n    \\log(L(\\mu, \\sigma^2)) & = \\log( \\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x_i - \\mu)^2) ) \\\\\n    & = \\sum_{i = 1}^n \\left[ \\log(\\frac{1}{\\sqrt{2\\pi \\sigma^2}})  - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ -\\frac{1}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right] \\\\\n    & = \\frac{-n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (x_i - \\mu)^2\n\\end{align*}\\]\nNow we need to find \\(\\frac{\\partial}{\\partial \\sigma^2}\\log(L(\\mu, \\sigma^2))\\) and \\(\\frac{\\partial}{\\partial \\mu}\\log(L(\\mu, \\sigma^2))\\). Let’s make our lives a little bit easier by setting \\(\\sigma^2 \\equiv \\theta\\) (so we don’t trip ourselves up with the exponent). We get\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\theta}\\log(L(\\mu, \\theta)) & = \\frac{\\partial}{\\partial \\theta} \\left(\\frac{-n}{2} \\log(2\\pi \\theta) - \\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i - \\mu)^2 \\right)\\\\\n    & = \\frac{-2\\pi n}{4 \\pi \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2}\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\log(L(\\mu, \\theta)) & = \\frac{\\partial}{\\partial \\mu} \\left(\\frac{-n}{2} \\log(2\\pi \\theta) - \\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i - \\mu)^2 \\right)\\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i^2 - 2 \\mu x_i + \\mu^2)\\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} ( \\sum_{i = 1}^n x_i^2 - 2 \\mu \\sum_{i = 1}^n x_i + n\\mu^2 )\\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} (- 2 \\mu \\sum_{i = 1}^n x_i + n\\mu^2 ) \\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left(   \\frac{\\sum_{i = 1}^n x_i}{\\theta} \\mu - \\frac{n}{2\\theta}\\mu^2  \\right) \\\\\n    & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu\n\\end{align*}\\]\nWe now have the following system of equations to solve:\n\\[\\begin{align*}\n    0 & \\equiv \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    0 & \\equiv \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu\n\\end{align*}\\]\nTypically, we solve one of the equations for one of the parameters, plug that into the other equation, and then go from there. We’ll start by solving the second equation for \\(\\mu\\).\n\\[\\begin{align*}\n    0 & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu \\\\\n    \\frac{n}{\\theta} \\mu & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} \\\\\n    \\mu & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nWell that’s convenient! We already have the MLE for \\(\\mu\\) as being just the sample average. Plugging this into the first equation in our system we obtain\n\\[\\begin{align*}\n    0 & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    0 & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{2 \\theta^2} \\\\\n    \\frac{n}{2 \\theta} & = \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{2 \\theta^2} \\\\\n    n & = \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{\\theta} \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2] \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x} )^2\n\\end{align*}\\]\nwhere \\(\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i\\). And so finally, we have that the MLE for \\(\\sigma^2\\) is given by \\(\\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x} )^2\\), and the MLE for \\(\\mu\\) is given by \\(\\bar{x}\\)!"
  },
  {
    "objectID": "mom.html#learning-objectives",
    "href": "mom.html#learning-objectives",
    "title": "3  Method of Moments",
    "section": "3.1 Learning Objectives",
    "text": "3.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive method of moments estimators for parameters of common probability density functions\nExplain (in plain English) why method of moments estimation is an intuitive approach to estimating unknown parameters"
  },
  {
    "objectID": "mom.html#reading-guide",
    "href": "mom.html#reading-guide",
    "title": "3  Method of Moments",
    "section": "3.2 Reading Guide",
    "text": "3.2 Reading Guide\nAssociated Readings: Chapter 5 (“The Method of Moments” through Example 5.2.7)\n\n3.2.1 Reading Questions\n\nWhat is the intuition behind the method of moments (MOM) procedure for estimating unknown parameters?\nWhat are the typical steps to find a MOM estimator? (see Ex 5.2.6, 5.2.7, and Case Study 5.2.2; work through at least one of these examples in detail, filling in any steps that the textbook left out)\nWhat advantages does the MOM approach offer compared to MLE?\nDo the MOM and MLE approaches always yield the same estimate? (look through the examples in Section 5.2 and try using the other approach — do you always get the same answer?)"
  },
  {
    "objectID": "mom.html#definitions",
    "href": "mom.html#definitions",
    "title": "3  Method of Moments",
    "section": "3.3 Definitions",
    "text": "3.3 Definitions\nYou are expected to know the following definitions:\nTheoretical Moment\nThe \\(r^{th}\\) theoretical moment of a probability distribution is given by \\(E[X^r]\\). For example, when \\(r = 1\\), the \\(r^{th}\\) moment is just the expectation of the random variable \\(X\\).\nSample Moment\nThe \\(r^{th}\\) sample moment of a probability distribution is given by \\(\\frac{1}{n} \\sum_{i = 1}^n x_i^r\\), for a random sample of observations \\(x_1, \\dots, x_n\\).\nMethod of Moments Estimates\nLet \\(x_1, \\dots, x_n\\) be a random sample of observations from the pdf \\(f_X(x \\mid \\boldsymbol{\\theta})\\). The method of moments estimates are then the solutions to the set of \\(s\\) equations given by\n\\[\\begin{align*}\n  E[X] & = \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n  E[X^2] & = \\frac{1}{n} \\sum_{i = 1}^n x_i^2 \\\\\n  & \\vdots \\\\\n  E[X^s] & = \\frac{1}{n} \\sum_{i = 1}^n x_i^s\n\\end{align*}\\]\nIf our pdf depends on only a single unknown parameter, we only need to solve the first equation. If we have two unknown parameters, we need to solve the system of the first two equations. So on and so forth."
  },
  {
    "objectID": "mom.html#theorems",
    "href": "mom.html#theorems",
    "title": "3  Method of Moments",
    "section": "3.4 Theorems",
    "text": "3.4 Theorems\nNone for this chapter!"
  },
  {
    "objectID": "mom.html#worked-examples",
    "href": "mom.html#worked-examples",
    "title": "3  Method of Moments",
    "section": "3.5 Worked Examples",
    "text": "3.5 Worked Examples\nIn general (for these worked examples as well as the problem sets), I do not expect you to calculate theoretical moments by hand. We practiced that in the probability review chapter, and now we can use those known theoretical moments to make our lives easier.\nProblem 1: Suppose \\(X_1, \\dots, X_n \\sim Poisson(\\lambda)\\). Find the MLE of \\(\\lambda\\) and the MOM estimator of \\(\\lambda\\).\n\n\nSolution:\n\nTo obtain the MLE, note that we can write the likelihood as\n\\[\nL(\\lambda) = \\prod_{i = 1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\n\\]\nand the log-likelihood as\n\\[\n\\log(L(\\lambda)) = \\sum_{i = 1}^n \\left[ x_i\\log(\\lambda) - \\lambda - \\log(x_i!)\\right]\n\\]\nwhere I’ve used one “log rule” in the above to simplify: \\(\\log(a^b) = b \\times log(a)\\). Taking the derivative of the log-likelihood and setting it equal to zero, we obtain\n\\[\\begin{align*}\n  \\frac{\\partial}{\\partial \\lambda} \\log(L(\\lambda)) & = \\frac{1}{\\lambda} \\sum_{i = 1}^n x_i - n \\\\\n  0 & \\equiv \\frac{1}{\\lambda} \\sum_{i = 1}^n x_i - n \\\\\n  n & = \\frac{1}{\\lambda} \\sum_{i = 1}^n x_i \\\\\n  \\lambda & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nand so the MLE for \\(\\lambda\\) is the sample average. To obtain the MOM estimator for \\(\\lambda\\), first note that the pdf contains only one parameter. Therefore, we only need to set the first theoretical moment equal to the first sample moment, and solve. Note that the first theoretical moment of a Poisson distribution is \\(E[X] = \\lambda\\), and so equating this to the first sample moment, we obtain that the MOM estimator for \\(\\lambda\\) is again, just the sample average! Much “easier” to compute than the MLE, in this case.\n\nProblem 2: Suppose \\(X_1, \\dots, X_n \\sim Bernoulli(\\theta)\\). Find the MOM estimator for \\(\\theta\\).\n\n\nSolution:\n\nNote that our pdf contains only one parameter. Then we only need to solve a “system” of one equation. We have\n\\[\\begin{align*}\n  E[X] & = \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n  \\theta & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nand we’re done! The system is pretty easy to “solve” when the theoretical moment is exactly the parameter we’re interested in.\n\nProblem 3: Suppose \\(Y_1, \\dots, Y_n \\sim Uniform(0, \\theta)\\). Find the MOM estimator for \\(\\theta\\).\n\n\nSolution:\n\nNote that our pdf contains only one parameter. Then we only need to solve a “system” of one equation. We have\n\\[\\begin{align*}\n  E[Y] & = \\frac{1}{n} \\sum_{i = 1}^n y_i \\\\\n  \\frac{\\theta}{2} & = \\frac{1}{n} \\sum_{i = 1}^n y_i \\\\\n  \\theta & = 2 \\bar{y}\n\\end{align*}\\]\nAnd so the MOM estimator for \\(\\theta\\) is 2 times the sample mean. Note that this is an example where the MOM estimator and MLE are not the same (you derived the MLE on your first problem set)!"
  },
  {
    "objectID": "properties.html#learning-objectives",
    "href": "properties.html#learning-objectives",
    "title": "4  Properties of Estimators",
    "section": "4.1 Learning Objectives",
    "text": "4.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nCalculate bias and variance of various estimators for unknown parameters\nExplain the distinction between bias and variance colloquially in terms of precision and accuracy, and why these properties are important\nCompare estimators in terms of their relative efficiency\nJustify why there exists a bias-variance trade-off, and explain what consequences this may have when comparing estimators"
  },
  {
    "objectID": "properties.html#reading-guide",
    "href": "properties.html#reading-guide",
    "title": "4  Properties of Estimators",
    "section": "4.2 Reading Guide",
    "text": "4.2 Reading Guide\nAssociated Readings: Chapter 5, Section 5.4 (“Properties of Estimators”) & 5.5 (“Minimum-Variance Estimators: The Cramér-Rao Lower Bound”)\n\n4.2.1 Reading Questions\n\nIntuitively, what is the difference between bias and precision?\nWhat are the typical steps to checking if an estimator is unbiased? (see Examples 5.4.2, 5.4.3, and 5.4.4 in the textbook)\nHow can we construct unbiased estimators? (see comment in Example 5.4.2 and 5.4.4)\nIf an estimator is unbiased, is it also asymptotically unbiased? If an estimator is asymptotically unbiased, is it necessarily unbiased?\nWhen comparing estimators, how can we determine which estimator is more efficient? (see Examples 5.4.5 and 5.4.6)\nDescribe, in your own words, what the Cramér-Rao inequality tells us.\nWhat is the difference between a UMVUE and an efficient estimator? Does one imply the other? (see the Comment below Definition 5.5.2)"
  },
  {
    "objectID": "properties.html#definitions",
    "href": "properties.html#definitions",
    "title": "4  Properties of Estimators",
    "section": "4.3 Definitions",
    "text": "4.3 Definitions\nYou are expected to know the following definitions:\nUnbiased\nAn estimator \\(\\hat{\\theta} = g(X_1, \\dots, X_n)\\) is an unbiased estimator for \\(\\theta\\) if \\(E[\\hat{\\theta}] = \\theta\\), for all \\(\\theta\\).\nAsymptotically Unbiased\nAn estimator \\(\\hat{\\theta} = g(X_1, \\dots, X_n)\\) is an asymptotically unbiased estimator for \\(\\theta\\) if \\(\\underset{n \\to \\infty}{\\text{lim}} E[\\hat{\\theta}] = \\theta\\).\nPrecision\nThe precision of a random variable \\(X\\) is given by \\(\\frac{1}{Var(X)}\\).\nMean Squared Error (MSE)\nThe mean squared error of an estimator is given by\n\\[\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + Bias(\\hat{\\theta})^2\n\\]\nRelative Efficiency\nThe relative efficiency of an estimator \\(\\hat{\\theta}_1\\) with respect to an estimator \\(\\hat{\\theta}_2\\) is the ratio \\(Var(\\hat{\\theta}_2)/Var(\\hat{\\theta}_1)\\).\nUniformly Minimum-Variance Unbiased Estimator (UMVUE)\nAn estimator \\(\\hat{\\theta}^*\\) is the UMVUE if, for all estimators \\(\\hat{\\theta}\\) in the class of unbiased estimators \\(\\Theta\\),\n\\[\nVar(\\hat{\\theta}^*) \\leq Var(\\hat{\\theta})\n\\]\nScore\nThe score is defined as the first partial derivative with respect to \\(\\theta\\) of the log-likelihood function, given by\n\\[\n\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x)\n\\]\nInformation Matrix\nThe information matrix* \\(I(\\theta)\\) for a collection of iid random variables \\(X_1, \\dots, X_n\\) is the variance of the score, given by\n\\[\nI(\\theta) = E \\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x) \\right)^2\\right] = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x)\\right]\n\\]\nNote that the above formula is in fact the variance of the score, since we can show that the expectation of the score is 0 (under some regularity conditions). This is shown as part of the proof of the C-R lower bound in the Theorems section of this chapter.\nThe information matrix is sometimes written in terms of a pdf for a single random variable as opposed to a likelihood (this is what our textbook does, for example). In this case, we have \\(I(\\theta) = n I_1(\\theta)\\), where the \\(I_1(\\theta)\\) on the right-hand side is defined as \\(E \\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f_X(x \\mid \\theta) \\right)^2\\right]\\). Sometimes (as in the textbook) \\(I_1(\\theta)\\) is written without the subscript \\(1\\) which is a slight abuse of notation that is endlessly confusing (to me, at least). For this set of course notes, we’ll always specify the information matrix in terms of a pdf for a single random variable with the subscript \\(1\\), for clarity.\n*The information matrix is often referred to as the Fisher Information matrix, as it was developed by Sir Ronald Fisher. Fisher developed much of the core, statistical theory that we use today. He was also the founding chairman of the University of Cambridge Eugenics Society, and contributed to a large body of scientific work and public policy that promoted racist and classist ideals."
  },
  {
    "objectID": "properties.html#theorems",
    "href": "properties.html#theorems",
    "title": "4  Properties of Estimators",
    "section": "4.4 Theorems",
    "text": "4.4 Theorems\nCovariance Inequality (based on the Cauchy-Schwarz inequality)\nLet \\(X\\) and \\(Y\\) be random variables. Then,\n\\[\nVar(X) \\geq \\frac{Cov(X, Y)^2}{Var(Y)}\n\\]\nThe proof is quite clear on Wikipedia.\nCramér-Rao Lower Bound\nLet \\(f_Y(y \\mid \\theta)\\) be a pdf with nice* conditions, and let \\(Y_1, \\dots, Y_n\\) be a random sample from \\(f_Y(y \\mid \\theta)\\). Let \\(\\hat{\\theta}\\) be any unbiased estimator of \\(\\theta\\). Then\n\\[\\begin{align*}\nVar(\\hat{\\theta}) & \\geq \\left\\{ E\\left[ \\left( \\frac{\\partial \\log( L(\\theta \\mid y))}{\\partial \\theta}\\right)^2\\right]\\right\\}^{-1} \\\\\n& = -\\left\\{ E\\left[ \\frac{\\partial^2 \\log( L(\\theta \\mid y))}{\\partial \\theta^2} \\right] \\right\\}^{-1} \\\\\n& = \\frac{1}{I(\\theta)}\n\\end{align*}\\]\n*our nice conditions that we need are that \\(f_Y(y \\mid \\theta)\\) has continuous first- and second-order derivatives, which would quickly discover we need by looking at the form for the C-R lower bound, and that the set of values \\(y\\) where \\(f_Y(y \\mid \\theta) \\neq 0\\) does not depend on \\(\\theta\\). If you are familiar with the concept of the “support” of a function, that is where this second condition comes from. The key here is that this condition allows to interchange derivatives and integrals, in particular, \\(\\frac{\\partial}{\\partial \\theta} \\int f(x) dx = \\int \\frac{\\partial}{\\partial \\theta} f(x)dx\\), which we’ll need to complete the proof.\nProof.\nLet \\(X = \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta}\\). By the Covariance Inequality,\n\\[\nVar(\\hat{\\theta}) \\geq \\frac{Cov(\\hat{\\theta},X)^2}{Var(X)}\n\\]\nand so if we can show\n\\[\\begin{align*}\n\\frac{Cov(\\hat{\\theta},X)^2}{Var(X)} & = \\left\\{ E\\left[ \\left( \\frac{\\partial \\log( L(\\theta \\mid \\textbf{y}))}{\\partial \\theta}\\right)^2\\right]\\right\\}^{-1}  \\\\\n& = \\frac{1}{I(\\theta)}\n\\end{align*}\\]\nthen we’re done, as this is the C-R lower bound. Note first that\n\\[\\begin{align*}\nE[X] & = \\int x f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right)  f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\left( \\frac{\\partial \\log f_Y(\\textbf{y} \\mid \\theta)}{\\partial \\theta} \\right)  f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\frac{\\frac{\\partial}{\\partial \\theta} f_Y(\\textbf{y} \\mid \\theta)}{ f_Y(\\textbf{y} \\mid \\theta)} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\frac{\\partial}{\\partial \\theta} f_Y (\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\frac{\\partial}{\\partial \\theta} \\int f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\frac{\\partial}{\\partial \\theta} 1 \\\\\n& = 0\n\\end{align*}\\]\nThis means that\n\\[\\begin{align*}\n    Var[X] & = E[X^2] - E[X]^2 \\\\\n    & = E[X^2] \\\\\n    & = E \\left[ \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right)^2\\right ]\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n    Cov(\\hat{\\theta}, X) & = E[\\hat{\\theta} X] - E[\\hat{\\theta}] E[X] \\\\\n    & = E[\\hat{\\theta}X] \\\\\n    & = \\int \\hat{\\theta} x f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right) f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\left( \\frac{\\partial \\log f_Y(\\textbf{y} \\mid \\theta)}{\\partial \\theta} \\right) f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\frac{\\frac{\\partial}{\\partial \\theta} f_Y(\\textbf{y} \\mid \\theta)}{ f_Y(\\textbf{y} \\mid \\theta)} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\frac{\\partial}{\\partial \\theta} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y}  \\\\\n    & = \\frac{\\partial}{\\partial \\theta} \\int \\hat{\\theta} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y}  \\\\\n    & =  \\frac{\\partial}{\\partial \\theta} E[\\hat{\\theta}] \\\\\n    & = \\frac{\\partial}{\\partial \\theta} \\theta \\\\\n    & = 1\n\\end{align*}\\]\nwhere \\(E[\\hat{\\theta}] = \\theta\\) since our estimator is unbiased. Putting this all together, we have\n\\[\\begin{align*}\n    Var[\\hat{\\theta}] & \\geq \\frac{Cov(\\hat{\\theta},X)^2}{Var(X)} \\\\\n    & = \\frac{1^2}{E \\left[ \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right)^2\\right ]} \\\\\n    & = \\frac{1}{I(\\theta)}\n\\end{align*}\\]\nas desired.\nComment: Note that what the Cramér-Rao lower bound tells us is that, if the variance of an unbiased estimator is equal to the Cramér-Rao lower bound, then that estimator has the minimum possible variance among all unbiased estimators there could possibly be. This allows us to prove, for example, whether or not an unbiased estimator is the UMVUE: If an unbiased estimator’s variance achieves the C-R lower bound, then it is optimal according to the UMVUE criterion."
  },
  {
    "objectID": "properties.html#worked-examples",
    "href": "properties.html#worked-examples",
    "title": "4  Properties of Estimators",
    "section": "4.5 Worked Examples",
    "text": "4.5 Worked Examples\nProblem 1: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Exponential(1/\\theta)\\). Compute the MLE of \\(\\theta\\), and show that it is an unbiased estimator of \\(\\theta\\).\n\n\nSolution:\n\nNote that we can write\n\\[\\begin{align*}\n    L(\\theta) & = \\prod_{i = 1}^n \\frac{1}{\\theta} e^{-x_i / \\theta} \\\\\n    \\log L(\\theta) & = \\sum_{i = 1}^n \\log(\\frac{1}{\\theta} e^{-x_i / \\theta}) \\\\\n    & = \\sum_{i = 1}^n  \\log(\\frac{1}{\\theta}) - \\sum_{i = 1}^n x_i / \\theta \\\\\n    & = -n \\log(\\theta) - \\frac{1}{\\theta} \\sum_{i = 1}^n x_i \\\\\n    \\frac{\\partial}{\\partial \\theta} \\log L(\\theta) & = \\frac{\\partial}{\\partial \\theta}  \\left( -n \\log(\\theta) - \\frac{1}{\\theta} \\sum_{i = 1}^n x_i \\right) \\\\\n    & = -\\frac{n}{\\theta} + \\frac{\\sum_{i = 1}^n x_i }{\\theta^2}\n\\end{align*}\\]\nSetting this equal to \\(0\\) and solving for \\(\\theta\\) we obtain\n\\[\\begin{align*}\n    0 & \\equiv -\\frac{n}{\\theta} + \\frac{\\sum_{i = 1}^n x_i }{\\theta^2}  \\\\\n    \\frac{n}{\\theta} & = \\frac{\\sum_{i = 1}^n x_i }{\\theta^2} \\\\\n    n & = \\frac{\\sum_{i = 1}^n x_i }{\\theta} \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nand so the MLE for \\(\\theta\\) is the sample mean. To show that the MLE is unbiased, we note that\n\\[\\begin{align*}\n    E \\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\right] & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i] = \\frac{1}{n} \\sum_{i = 1}^n \\theta  = \\theta\n\\end{align*}\\]\nas desired.\n\nProblem 2: Suppose again that \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Exponential(1/\\theta)\\). Let \\(\\hat{\\theta}_2 = Y_1\\), and \\(\\hat{\\theta}_3 = nY_{(1)}\\). Show that \\(\\hat{\\theta}_2\\) and \\(\\hat{\\theta}_3\\) are unbiased estimators of \\(\\theta\\). Hint: use the fact that \\(Y_{(1)} \\sim Exponential(n/\\theta)\\)\n\n\nSolution:\n\nNote that the mean of a random variable \\(Y \\sim Exponential(\\lambda)\\) is given by \\(1/\\lambda\\). Then we can write\n\\[\nE[\\hat{\\theta}_2] = E[Y_1] = \\frac{1}{1/\\theta} = \\theta\n\\]\nand\n\\[\nE[\\hat{\\theta}_3] = E[nY_{(1)}] = \\frac{n}{n/\\theta} = \\theta\n\\] as desired.\n\nProblem 3: Compare the variance of the estimators from Problems 1 and 2. Which is most efficient?\n\n\nSolution:\n\nRecall that the variance of a random variable \\(Y \\sim Exponential(\\lambda)\\) is given by \\(1/\\lambda^2\\). Let the MLE from Problem 1 be denoted \\(\\hat{\\theta}_1 = \\bar{X}\\). Then we can write\n\\[\nVar\\left[\\hat{\\theta}_1\\right] = Var\\left[\\frac{1}{n} \\sum_{i = 1}^n X_i\\right] = \\frac{1}{n^2} \\sum_{i = 1}^n Var[X_i] = \\frac{1}{n^2} \\left( \\frac{n}{(1/\\theta)^2} \\right) = \\frac{\\theta^2}{n}\n\\]\nand\n\\[\nVar\\left[\\hat{\\theta}_2\\right] = Var[Y_1] = \\frac{1}{(1/\\theta)^2} = \\theta^2\n\\]\nand\n\\[\nVar\\left[\\hat{\\theta}_3\\right] = Var[nY_{(1)}] = n^2 Var[Y_{(1)}] = \\frac{n^2}{(n/\\theta)^2} = \\theta^2\n\\]\nThus, the variance of the MLE, \\(\\hat{\\theta}_1\\), is most efficient, and is \\(n\\) times smaller than the variance of both \\(\\hat{\\theta}_2\\) and \\(\\hat{\\theta}_3\\).\n\nProblem 4: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Show that the estimator \\(\\hat{\\mu} = \\frac{1}{n} \\sum_{i = 1}^n X_i\\) and the estimator \\(\\hat{\\mu}_w = \\sum_{i = 1}^n w_i X_i\\) are both unbiased estimators of \\(\\mu\\), where \\(\\sum_{i = 1}^n w_i = 1\\).\n\n\nSolution:\n\nWe can write\n\\[\nE[\\hat{\\mu}] = E\\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\right] = \\frac{1}{n}\\sum_{i = 1}^n E[X_i] = \\frac{1}{n}\\sum_{i = 1}^n \\mu = \\mu\n\\]\nand\n\\[\nE[\\hat{\\mu}_w] = E \\left[ \\sum_{i = 1}^n w_i X_i \\right] = \\sum_{i = 1}^n w_i E \\left[ X_i \\right] = \\sum_{i = 1}^n w_i \\mu = \\mu \\sum_{i = 1}^n w_i = \\mu\n\\]\nas desired.\n\nProblem 5: Determine whether the estimator \\(\\hat{\\mu}\\) or \\(\\hat{\\mu}_w\\) is more efficient, in Problem 5, if we additionally impose the constraint \\(w_i \\geq 0\\) \\(\\forall i\\). (Note that this is a more “general” example based on Example 5.4.5 in the course textbook) (Hint: use the Cauchy-Schwarz inequality)\n\n\nSolution:\n\nTo determine relative efficiency, we must compute the variance of each estimator. We have\n\\[\nVar[\\hat{\\mu}] = Var \\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\right] = \\frac{1}{n^2} \\sum_{i = 1}^n Var[X_i] = \\frac{1}{n^2} \\sum_{i = 1}^n \\sigma^2 = \\sigma^2 / n\n\\]\nand\n\\[\\begin{align*}\n    Var[\\hat{\\mu}_w] & =  Var \\left[ \\sum_{i = 1}^n w_i X_i \\right] \\\\\n    & = \\sum_{i = 1}^n Var[w_i X_i] \\\\\n    & = \\sum_{i = 1}^n w_i^2 Var[X_i] \\\\\n    & = \\sum_{i = 1}^n w_i^2  \\sigma^2 \\\\\n    & = \\sigma^2 \\sum_{i = 1}^n w_i^2\n\\end{align*}\\]\nAnd so to determine which estimator is more efficient, we need to determine if \\(\\frac{1}{n}\\) is less than \\(\\sum_{i = 1}^n w_i^2\\) (or not). The Cauchy-Schwarz inequality tells us that\n\\[\\begin{align*}\n    \\left( \\sum_{i = 1}^n w_i \\cdot 1\\right)^2 & \\leq \\left( \\sum_{i = 1}^n w_i^2 \\right) \\left( \\sum_{i = 1}^n 1^2 \\right) \\\\\n    \\left( \\sum_{i = 1}^n w_i \\right)^2 & \\leq \\left( \\sum_{i = 1}^n w_i^2 \\right) n \\\\\n    1 & \\leq \\left( \\sum_{i = 1}^n w_i^2 \\right) n  \\\\\n    \\frac{1}{n} & \\leq \\sum_{i = 1}^n w_i^2\n\\end{align*}\\]\nand therefore, \\(\\hat{\\mu}\\) is more efficient than \\(\\hat{\\mu}_w\\).\n\nProblem 6: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Show that the MLE for \\(\\sigma^2\\) is biased, and suggest a modified variance estimator for \\(\\sigma^2\\) that is unbiased. (Note that this is example 5.4.4 in our course textbook)\n\n\nSolution:\n\nRecall that the MLE for \\(\\sigma^2\\) is given by \\(\\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\). Then\n\\[\\begin{align*}\n    E\\left[ \\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\right] & = \\frac{1}{n} \\sum_{i = 1}^n E\\left[ (X_i - \\bar{X})^2\\right] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E\\left[ X_i^2 - 2X_i \\bar{X} + \\bar{X}^2\\right] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - 2 E\\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\bar{X} \\right] + E[\\bar{X}^2] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - 2 E\\left[ \\bar{X} \\frac{1}{n} \\sum_{i = 1}^n X_i  \\right] + E[\\bar{X}^2] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - 2 E\\left[ \\bar{X}^2  \\right] + E[\\bar{X}^2] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - E\\left[ \\bar{X}^2  \\right]\n\\end{align*}\\]\nRecall that since \\(X_i \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\), \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\), and that we can write \\(Var[Y] + E[Y]^2 = E[Y^2]\\) (definition of variance). Then we can write\n\\[\\begin{align*}\n    E\\left[ \\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2 \\right] & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - E\\left[ \\bar{X}^2  \\right] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n \\left( \\sigma^2 + \\mu^2 \\right) - \\left( \\frac{\\sigma^2}{n} + \\mu^2 \\right) \\\\\n    & = \\sigma^2 + \\mu^2 - \\frac{\\sigma^2}{n} - \\mu^2  \\\\\n    & = \\sigma^2 - \\frac{\\sigma^2}{n} \\\\\n    & = \\sigma^2 \\left( 1 - \\frac{1}{n} \\right) \\\\\n    & = \\sigma^2  \\left( \\frac{n-1}{n} \\right)\n\\end{align*}\\]\nTherefore, since \\(E[\\hat{\\sigma}^2_{MLE}] \\neq \\sigma^2\\), the MLE is unbiased. Note that\n\\[\\begin{align*}\n    E\\left[ \\left( \\frac{n}{n-1} \\right)\\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\right] & = \\left( \\frac{n}{n-1} \\right) \\left( \\frac{n-1}{n} \\right) \\sigma^2   \\\\\n    & = \\sigma^2\n\\end{align*}\\]\nand so the estimator \\(\\frac{1}{n-1} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\) is an unbiased estimator for \\(\\sigma^2\\). This estimator is often called the “sample variance”, and is denoted by \\(S^2\\)."
  },
  {
    "objectID": "consistency.html#learning-objectives",
    "href": "consistency.html#learning-objectives",
    "title": "5  Consistency",
    "section": "5.1 Learning Objectives",
    "text": "5.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDistinguish between finite sample properties and asymptotic properties of estimators\nProve (using Chebyshev’s inequality) whether or not an estimator is consistent"
  },
  {
    "objectID": "consistency.html#reading-guide",
    "href": "consistency.html#reading-guide",
    "title": "5  Consistency",
    "section": "5.2 Reading Guide",
    "text": "5.2 Reading Guide\nAssociated Readings: Chapter 5, Section 5.7\n\n5.2.1 Reading Questions\n\nWhat is the distinction between a fixed sample property and an asymptotic property of an estimator?\nDescribe, in your own words, what it means for an estimator to be consistent.\nHow can we use Chebyshev’s inequality to show that an estimator is consistent?\nWhich of the estimation techniques we’ve seen so far yield consistent estimators?"
  },
  {
    "objectID": "consistency.html#definitions",
    "href": "consistency.html#definitions",
    "title": "5  Consistency",
    "section": "5.3 Definitions",
    "text": "5.3 Definitions\nYou are expected to know the following definitions:\nAsymptotically Unbiased\nAn estimator \\(\\hat{\\theta} = g(X_1, \\dots, X_n)\\) is an asymptotically unbiased estimator for \\(\\theta\\) if \\(\\underset{n \\to \\infty}{\\text{lim}} E[\\hat{\\theta}] = \\theta\\).\nConsistent\nAn estimator \\(\\hat{\\theta}_n = h(X_1, \\dots, X_n)\\) is consistent for \\(\\theta\\) if it converges in probability to \\(\\theta\\). That is, for all \\(\\epsilon &gt; 0\\),\n\\[\n\\underset{n \\to \\infty}{\\text{lim}} \\Pr(| \\hat{\\theta}_n - \\theta | &lt; \\epsilon) = 1\n\\]\nNote that we write our estimator with a subscript \\(n\\) here to clarify that our estimator depends on our sample size. There is an alternative \\(\\epsilon-\\delta\\) definition of consistency in the textbook, but we won’t focus on it for this course.\nWeak Law of Large Numbers\nFor independent and identically distributed random variables \\(X_1, \\dots, X_n\\) with finite expectation \\(\\mu &lt; \\infty\\),\n\\[\n\\underset{n \\to \\infty}{\\text{lim}} \\Pr(| \\overline{X} - \\mu | &lt; \\epsilon) = 1\n\\]\nAlternatively, we can write that as \\(n \\to \\infty\\), \\(\\overline{X} \\overset{p}{\\to} \\mu\\), where “\\(\\overset{p}{\\to}\\)” denotes convergence in probability."
  },
  {
    "objectID": "consistency.html#theorems",
    "href": "consistency.html#theorems",
    "title": "5  Consistency",
    "section": "5.4 Theorems",
    "text": "5.4 Theorems\nChebyshev’s Inequality\nLet \\(W\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then for any \\(\\epsilon &gt; 0\\),\n\\[\n\\Pr(|W - \\mu| &lt; \\epsilon) \\geq 1 - \\frac{\\sigma^2}{\\epsilon^2},\n\\]\nor, equivalently,\n\\[\n\\Pr(|W - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}.\n\\]\nProof.\nLet \\(\\epsilon &gt; 0\\). Then\n\\[\\begin{align*}\n\\sigma^2 & = \\text{Var}(W) \\\\\n& = \\int_{-\\infty}^\\infty (w-\\mu)^2 f_W(w)dw \\\\\n& = \\int_{-\\infty}^{\\mu - \\epsilon} (w-\\mu)^2 f_W(w)dw + \\int_{\\mu - \\epsilon}^{\\mu + \\epsilon} (w-\\mu)^2 f_W(w)dw + \\int_{\\mu + \\epsilon}^\\infty (w-\\mu)^2 f_W(w)dw \\\\\n& \\ge \\int_{-\\infty}^{\\mu - \\epsilon} (w-\\mu)^2 f_W(w)dw + 0  + \\int_{\\mu + \\epsilon}^\\infty (w-\\mu)^2 f_W(w)dw \\\\\n&= \\int_{|w-\\mu|\\ge \\epsilon} (w-\\mu)^2 f_W(w)dw \\\\\n& \\ge \\int_{|w-\\mu|\\ge \\epsilon} \\epsilon^2 f_W(w)dw \\\\\n& = \\epsilon^2 \\int_{|w-\\mu|\\ge \\epsilon} f_W(w)dw\\\\\n& = \\epsilon^2 P(|W-\\mu| \\ge \\epsilon)\n\\end{align*}\\]\nand rearranging yields\n\\[\\begin{align*}\n\\sigma^2 & \\geq \\epsilon^2 P(|W-\\mu| \\ge \\epsilon) \\\\\nP(|W-\\mu| \\geq \\epsilon) & \\leq \\frac{\\sigma^2}{\\epsilon^2}\n\\end{align*}\\]\nas desired.\nCorollary 1: If \\(\\hat{\\theta}_n\\) is an unbiased estimator for \\(\\theta\\) and \\(\\underset{n \\to \\infty}{\\text{lim}} Var(\\hat{\\theta}_n) = 0\\), then \\(\\hat{\\theta}_n\\) is consistent for \\(\\theta\\). (You’ll prove this corollary on a problem set!)\nCorollary 2: If \\(\\hat{\\theta}_n\\) is an asymptotically unbiased estimator for \\(\\theta\\) and \\(\\underset{n \\to \\infty}{\\text{lim}} Var(\\hat{\\theta}_n) = 0\\), then \\(\\hat{\\theta}_n\\) is consistent for \\(\\theta\\).\nNote that the second corollary is a bit stronger than the first one, in that the first corollary actually implies the second. If an estimator is unbiased, then it is certainly asymptotically unbiased as well."
  },
  {
    "objectID": "consistency.html#worked-examples",
    "href": "consistency.html#worked-examples",
    "title": "5  Consistency",
    "section": "5.5 Worked Examples",
    "text": "5.5 Worked Examples\nProblem 1: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Show that the MLE for \\(\\sigma^2\\) is asymptotically unbiased.\n\n\nSolution:\n\nThe MLE for \\(\\sigma^2\\) is given by \\(\\frac{1}{n} \\sum_{i = 1}^n (X_i - \\overline{X})^2\\) (see the MLE section of the course notes, worked example problem 2), and has expectation \\(\\left( \\frac{n-1}{n} \\right)\\sigma^2\\) (see the Properties section of the course notes, worked example problem 6). To show that this estimator is asymptotically unbiased, note that we have\n\\[\\begin{align*}\n    \\underset{n\\to \\infty}{\\text{lim}} E[\\hat{\\sigma^2}_{MLE}] & = \\underset{n\\to \\infty}{\\text{lim}} \\left( \\frac{n-1}{n} \\right) \\sigma^2 \\\\\n    & = \\left( 1 \\right) \\sigma^2 \\\\\n    & = \\sigma^2\n\\end{align*}\\]\nand therefore, the MLE for \\(\\sigma^2\\) is asymptotically unbiased.\n\nProblem 2: Suppose \\(Y_1, \\dots, Y_n \\overset{iid}{\\sim} Uniform(0, \\theta)\\), and recall that \\(\\hat{\\theta}_{MLE} = Y_{(n)}\\) with \\(f_{Y_{(n)}}(y \\mid \\theta) = \\frac{n}{\\theta^n} y^{n-1}\\), \\(0 \\leq y \\leq \\theta\\). Prove that \\(\\hat{\\theta}_{MLE}\\) is a consistent estimator for \\(\\theta\\).\n\n\nSolution:\n\nTo prove that \\(\\hat{\\theta}_{MLE}\\) is consistent, we must first show that \\(\\hat{\\theta}_{MLE}\\) is (either) unbiased or asymptotically unbiased, and then we must show that the variance of \\(\\hat{\\theta}_{MLE}\\) tends to zero as \\(n \\to \\infty\\). To begin, note that\n\\[\\begin{align*}\n    E\\left[\\hat{\\theta}_{MLE}\\right] & = \\int_{0}^\\theta y f_{Y_{(n)}} (y \\mid \\theta) dy \\\\\n    & = \\int_{0}^\\theta y \\left( \\frac{n}{\\theta^n} y^{n-1} \\right) dy \\\\\n    & = \\frac{n}{\\theta^n} \\int_0^\\theta y^n dy \\\\\n    & = \\frac{n}{\\theta^n} \\left( \\frac{y^{n + 1}}{n + 1} \\bigg|_0^\\theta \\right) \\\\\n    & = \\frac{n}{\\theta^n} \\left( \\frac{\\theta^{n + 1}}{n + 1} \\right) \\\\\n    & = \\left( \\frac{n}{n + 1} \\right) \\theta\n\\end{align*}\\]\nand so \\(\\hat{\\theta}_{MLE}\\) is biased. It is, however, asymptotically unbiased. Note that \\(\\left( \\frac{n}{n + 1} \\right) \\overset{n \\to \\infty}{\\to} 1\\), and therefore \\(E\\left[\\hat{\\theta}_{MLE}\\right] \\overset{n \\to \\infty}{\\to} \\theta\\).\nAll that’s left is to show that \\(Var\\left[\\hat{\\theta}_{MLE}\\right] \\overset{n \\to \\infty}{\\to} 0\\). We can write\n\\[\\begin{align*}\n        E\\left[Y_{(n)}^2\\right] & = \\int_0^\\theta y^2 \\frac{ny^{n-1}}{\\theta^n} dy \\\\\n        & = \\frac{n}{\\theta^n} \\int_0^\\theta y^{n + 1} dy \\\\\n        & = \\frac{n}{\\theta^n} \\left( \\frac{y^{n + 2}}{n + 2} \\bigg|_0^\\theta \\right) \\\\\n        & = \\frac{n}{\\theta^n} \\left( \\frac{\\theta^{n + 2}}{n + 2}\\right) \\\\\n        & = \\left( \\frac{n}{n + 2} \\right) \\theta^2\n    \\end{align*}\\]\nand therefore\n\\[\\begin{align*}\n    \\underset{n \\to \\infty}{\\text{lim}} Var\\left[\\hat{\\theta}_{MLE}\\right] & = \\underset{n \\to \\infty}{\\text{lim}} \\left[ E\\left[Y_{(n)}^2\\right] - E\\left[Y_{(n)}\\right]^2 \\right]\\\\\n    & = \\underset{n \\to \\infty}{\\text{lim}} \\left[ \\left( \\frac{n}{n + 2} \\right) \\theta^2 - \\left( \\frac{n}{n + 1} \\right)^2 \\theta^2 \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[ \\left( \\frac{n}{n + 2} \\right)  - \\left( \\frac{n}{n + 1} \\right)^2\\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n}{n + 2}   - \\frac{n^2}{(n + 1)^2} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n(n + 1)^2 - n^2 (n + 2)}{(n + 2)(n + 1)^2} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n(n^2 + 2n + 1) - n^3 -2n^2}{(n + 2)(n^2 + 2n + 1)} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n^3 + 2n^2 + n - n^3 -2n^2}{n^3 + 2n^2 + 2n^2 + 5n + 2} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n}{n^3 + 4n^2  + 5n + 2} \\right] \\\\\n    & = 0\n\\end{align*}\\]\nwhere the last term goes to zero because \\(\\frac{n}{n^3} \\to 0\\) as \\(n \\to \\infty\\). Therefore, \\(\\hat{\\theta}_{MLE}\\) is a consistent estimator for \\(\\theta\\)."
  },
  {
    "objectID": "asymptotics.html#learning-objectives",
    "href": "asymptotics.html#learning-objectives",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.1 Learning Objectives",
    "text": "6.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nExplain the usefulness of the Central Limit Theorem for Frequentist statistical theory\nManipulate asymptotic distributions to remove their dependence on unknown parameters using the delta-method and Slutsky’s theorem\n\n…and explain why such manipulation is important for confidence interval construction\n\nDerive confidence intervals for unknown parameters based on asymptotic or exact distributions"
  },
  {
    "objectID": "asymptotics.html#reading-guide",
    "href": "asymptotics.html#reading-guide",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.2 Reading Guide",
    "text": "6.2 Reading Guide\nAssociated Readings: Chapter 4, Section 5.3\n\n6.2.1 Reading Questions\n\nWhat feature of a confidence interval tells us about the precision of our estimator?\nWhy is “removing” unknown parameters from the asymptotic distribution of our estimators important when constructing confidence intervals?"
  },
  {
    "objectID": "asymptotics.html#definitions",
    "href": "asymptotics.html#definitions",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.3 Definitions",
    "text": "6.3 Definitions\nAsymptotic Normality\nAn estimator \\(\\hat{\\theta}_n\\) is asymptotically normal if \\(\\hat{\\theta}_n\\) converges in distribution to a normally distributed random variable.\nAsymptotic Efficiency\nAn estimator \\(\\hat{\\theta}_n\\) is asymptotically efficient if it’s asymptotic variance attains the C-R Lower Bound. Note that this is the C-R Lower Bound for a single observation, and therefore the asymptotic distribution of an MLE looks something like this:\n\\[\n\\sqrt{n} (\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N\\left( 0, \\frac{1}{I_1(\\theta)}\\right)\n\\]\nConfidence Interval\nA 100(1 - \\(\\alpha\\))% confidence interval for a parameter \\(\\theta\\) is given by \\((a, b)\\), where \\(\\Pr(a \\leq \\theta \\leq b) = 1 - \\alpha\\)."
  },
  {
    "objectID": "asymptotics.html#theorems",
    "href": "asymptotics.html#theorems",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.4 Theorems",
    "text": "6.4 Theorems\nCentral Limit Theorem (CLT)\nFor iid random variables \\(X_1, \\dots, X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[\n\\sqrt{n} (\\overbracket[0.65pt][-1]{X_n} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\nwhere “\\(\\overset{d}{\\to}\\)” denotes convergence in distribution.\nProof.\nNote that this proof is not completely rigorous, in that we will use the following theorem (without proof) in order to prove the CLT:\nTheorem: Let \\(W_1, \\dots, W_n\\) be a sequence of random variables with MGF of the sequence \\(W_n\\) given by \\(M_{W_n}(t)\\). Also, let \\(V\\) denote another random variable with MGF \\(M_V(t)\\). Then if \\(\\underset{n \\to \\infty}{\\text{lim}} M_{W_n}(t) = M_V(t)\\), for all values of \\(t\\) in some interval around \\(t = 0\\), then the sequence \\(W_1, \\dots, W_n\\) converges in distribution to \\(V\\).\nSuppose \\(X_1, \\dots, X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and let \\(Y_i = (X_i - \\mu)/\\sigma\\). Then \\(E[Y_i] = 0\\), and \\(Var[Y_i] = 1\\) since\n\\[\nE[Y_i] = E \\left[ (X_i - \\mu)/\\sigma\\right] = \\frac{1}{\\sigma} (E[X_i] - \\mu) = \\frac{1}{\\sigma} (\\mu - \\mu) = 0\n\\]\nand\n\\[\nVar[Y_i] = Var\\left[ (X_i - \\mu)/\\sigma\\right] = \\frac{1}{\\sigma^2} Var[X_i - \\mu] = \\frac{1}{\\sigma^2} Var[X_i] = \\frac{\\sigma^2}{\\sigma^2} = 1\n\\]\nFurther, let\n\\[\nZ_n = \\frac{\\sqrt{n}(\\overbracket[0.65pt][-1]{X} - \\mu)}{\\sigma} = \\frac{1}{\\sqrt{n}} \\sum_{i = 1}^n Y_i\n\\]\nwhere the last two terms are equal since\n\\[\\begin{align*}\n    \\frac{1}{\\sqrt{n}} \\sum_{i = 1}^n Y_i & = \\frac{1}{\\sqrt{n}} \\sum_{i = 1}^n \\left( \\frac{X_i - \\mu}{\\sigma}\\right)\\\\\n    & = \\frac{1}{\\sigma\\sqrt{n}} \\sum_{i = 1}^n (X_i - \\mu) \\\\\n    & = \\frac{1}{\\sigma\\sqrt{n}}  (n\\overbracket[0.65pt][-1]{X} - n\\mu) \\\\\n    & = \\frac{\\sqrt{n}(\\overbracket[0.65pt][-1]{X} - \\mu)}{\\sigma}\n\\end{align*}\\]\nWe’ll show that \\(Z_n \\overset{d}{\\to} N(0,1)\\) by showing that the MGF of \\(Z_n\\) converges to the MGF of a standard normal distribution. Let \\(M_Y(t)\\) denote the MGF of each \\(Y_i\\). Then the MGF of \\(\\sum_{i = 1}^n Y_i\\) is given by\n\\[\nE[e^{t\\sum_{i = 1}^n Y_i}] = E[e^{tY_1}e^{tY_2} \\dots e^{tY_n}] = E[e^{tY_1}]E[e^{tY_2}] \\dots E[e^{tY_n}] = M_Y(t)^n\n\\]\nand the MGF of \\(Z_n\\) is\n\\[\nM_{Z_n}(t) = E[e^{tZ_n}] = E[e^{t\\frac{1}{\\sqrt{n}}\\sum_{i = 1}^n Y_i}] = M_Y\\left(\\frac{t}{\\sqrt{n}}\\right)^n\n\\]\nNow note that the Taylor expansion of the function \\(e^{tY}\\) about \\(0\\) is given by\n\\[\ne^{tY} = 1 + tY + \\frac{t^2Y^2}{2!} + \\frac{t^3Y^3}{3!} + \\dots\n\\]\nTaking the expectation of both sides, we obtain\n\\[\nE[e^{tY}] = 1 + tE[Y] + \\frac{t^2E[Y^2]}{2!} + \\frac{t^3E[Y^3]}{3!} + \\dots\n\\]\nand note now that the left hand side is the MGF for \\(Y\\). Recalling that \\(E[Y] = 0\\) and \\(Var[Y] = 1\\), we have\n\\[\nE[e^{tY}] = 1  + \\frac{t^2}{2!} + \\frac{t^3E[Y^3]}{3!} + \\dots\n\\]\nAnd therefore\n\\[\nE[e^{tZ_n}] = \\left[1  + \\frac{t^2}{2n} + \\frac{t^3E[Y^3]}{3!n^{3/2}} + \\dots \\right]^n\n\\]\nWe’ll now make use of a theorem regarding sequences of real numbers (without proof): Let \\(a_n\\) and \\(c_n\\) be sequences of real numbers such that \\(a_n \\overset{n \\to \\infty}{\\to} 0\\) and \\(c_na_n^2 \\overset{n \\to \\infty}{\\to} 0\\). Then if \\(a_nc_n \\overset{n \\to \\infty}{\\to} b\\), \\((1 + a_n)^{c_n} \\overset{n \\to \\infty}{\\to} e^b\\).\nLet \\(a_n = \\frac{t^2}{2n} + \\frac{t^3E[Y^3]}{3!n^{3/2}} + \\dots\\) and \\(c_n = n\\). Note that both \\(a_n \\overset{n \\to \\infty}{\\to} 0\\) and \\(c_na_n^2 \\overset{n \\to \\infty}{\\to} 0\\). Then\n\\[\n\\underset{n \\to \\infty}{\\text{lim}} a_n c_n = \\underset{n \\to \\infty}{\\text{lim}} \\left[ \\frac{t^2}{2} + \\frac{t^3E[Y^3]}{3!n^{1/2}} + \\dots\\right] = \\frac{t^2}{2}\n\\]\nand therefore\n\\[\nM_{Z_n}(t) = (1 + a_n)^{c_n} \\overset{n \\to \\infty}{\\to} e^{t^2/2}\n\\]\nwhere we note that the right hand side is the MGF of a standard normal distribution. Then finally, we have proved that\n\\[\n\\sqrt{n}(\\overbracket[0.65pt][-1]{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\nas desired.\nContinuous Mapping Theorem\nIf \\(X_n \\overset{p}{\\to} X\\), and \\(g\\) is a continuous function, then \\(g(X_n) \\overset{p}{\\to} g(X)\\). Similarly for convergence almost surely and convergence in distribution.\nProof. Left to the reader, but also on Wikipedia.\nSlutsky’s Theorem\nIf \\(g(X, Y)\\) is a jointly continuous function at every point \\((X, a)\\) for some fixed \\(a\\), and if \\(X_n \\overset{d}{\\to} X\\) and \\(Y_n \\overset{p}{\\to} a\\), then \\(g(X_n, Y_n) \\overset{d}{\\to} g(X, a)\\).\nProof. Beyond the scope of the course, unfortunately, but here’s a link to the Wikipedia page if you want to go down that rabbit hole in your spare time.\nDelta-method\nLet \\(\\sqrt{n} (Y - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\\). If \\(g(Y)\\) is differentiable at \\(\\mu\\) and \\(g'(\\mu) \\neq 0\\), then\n\\[\n\\sqrt{n} \\left( g(Y) - g(\\mu)\\right) \\overset{d}{\\to} N(0, [g'(\\mu)]^2 \\sigma^2)\n\\]\nProof.\nSince \\(g\\) is differentiable at \\(\\mu\\), it’s first-order Taylor expansion is given by\n\\[\ng(Y) = g(\\mu) + (Y - \\mu)g'(\\mu) + O(| Y - \\mu |^2)\n\\]\nwhere \\(O(f(x))\\), referred to as “Big O,” describes the limiting behavior of the function \\(f(x)\\). In this case, we use it to note that every term in the Taylor expansion after the first derivative evaluated at \\(\\mu\\) is growing no faster than \\(|Y - \\mu|^2\\) as \\(n \\to \\infty\\).\nRearranging, note that\n\\[\ng(Y) - g(\\mu) =  (Y - \\mu)g'(\\mu) + O(| Y - \\mu |^2)\n\\]\nand so\n\\[\\begin{align*}\n    \\sqrt{n}\\left( g(Y) - g(\\mu) \\right) = \\sqrt{n}(Y - \\mu) g'(\\mu) + O(\\sqrt{n} |Y - \\mu|^2)\n\\end{align*}\\]\nThen note that \\(\\sqrt{n}(Y - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\\), \\(g'(\\mu) \\overset{p}{\\to} g'(\\mu)\\) since it’s just a constant, and \\(O(\\sqrt{n} |Y - \\mu|^2) \\overset{p}{\\to} 0\\) (due to the \\(\\sqrt{n}\\) term). Then using two applications of Slutsky’s theorem, we can write that\n\\[\n\\sqrt{n}(Y - \\mu)g'(\\mu) \\overset{d}{\\to} N(0, \\sigma^2)g'(\\mu) = \\overset{d}{\\to} N(0, [g'(\\mu)]^2\\sigma^2)\n\\] and\n\\[\\begin{align*}\n    \\sqrt{n}(Y - \\mu)g'(\\mu) + O(\\sqrt{n} |Y - \\mu|^2) & \\overset{d}{\\to} N(0, \\sigma^2)g'(\\mu) + 0\\\\\n    & \\overset{d}{=} N(0, [g'(\\mu)]^2\\sigma^2)\n\\end{align*}\\]\nand so finally we have shown that \\[\n\\sqrt{n}\\left( g(Y) - g(\\mu) \\right) \\overset{d}{\\to}  N(0, [g'(\\mu)]^2\\sigma^2)\n\\] as desired."
  },
  {
    "objectID": "asymptotics.html#worked-examples",
    "href": "asymptotics.html#worked-examples",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.5 Worked Examples",
    "text": "6.5 Worked Examples\nProblem 1: Suppose \\(\\sqrt{n}(Y_n - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\\). Find the asymptotic distribution of \\(\\sqrt{n}(Y_n^2 - \\mu^2)\\) when \\(\\mu \\neq 0\\).\n\n\nSolution:\n\nWe can apply the delta-method with the function \\(g(x) = x^2\\). Note that \\(g'(x) = 2x\\), and therefore we can write\n\\[\\begin{align*}\n    \\sqrt{n}(Y_n - \\mu) & \\overset{d}{\\to} N(0, \\sigma^2) \\\\\n    \\sqrt{n}(g(Y_n) - g(\\mu)) & \\overset{d}{\\to} N(0, [g'(\\mu)]^2\\sigma^2) \\\\\n    \\sqrt{n}(Y_n^2 - \\mu^2) & \\overset{d}{\\to} N(0, [2\\mu]^2\\sigma^2) \\\\\n    \\sqrt{n}(Y_n^2 - \\mu^2) & \\overset{d}{\\to} N(0, 4 \\mu^2\\sigma^2)\n\\end{align*}\\]\n\nProblem 2: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Bernoulli(p)\\), and recall that the MLE for \\(p\\) is given by \\(\\hat{p}_{MLE} = \\frac{1}{n} \\sum_{i = 1}^n X_i\\). Find the asymptotic distribution of \\(\\hat{p}_{MLE}\\) using the CLT and known properties of the Bernoulli distribution (expectation and variance, for example), and construct a 95% confidence interval for \\(p\\) based on this asymptotic distribution.\n\n\nSolution:\n\nWe know that \\(E[X_i] = p\\) and \\(Var[X_i] = p(1-p)\\). Then the CLT tell us that\n\\[\n\\sqrt{n}(\\hat{p}_{MLE} - p) \\overset{d}{\\to} N(0, p(1-p))\n\\]\nThe WLLN gives us that \\(\\hat{p}_{MLE} \\overset{p}{\\to} p\\), since \\(\\hat{p}_{MLE}\\) is a sample mean. We can then use the continuous mapping theorem to show that \\(\\frac{1}{\\sqrt{\\hat{p}_{MLE}(1-\\hat{p}_{MLE})}} \\overset{p}{\\to} \\frac{1}{\\sqrt{p(1 - p)}}\\). Applying Slutsky’s theorem, we then have\n\\[\n\\sqrt{n}\\left(\\frac{\\hat{p}_{MLE} - p}{\\sqrt{\\hat{p}_{MLE}(1-\\hat{p}_{MLE})}}\\right) \\overset{d}{\\to} N(0, 1)\n\\]\nand finally, (letting \\(\\hat{p} = \\hat{p}_{MLE}\\) for ease of notation)\n\\[\\begin{align*}\n    0.95 & = \\Pr\\left(-1.96 &lt; \\frac{\\hat{p} - p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}  &lt; 1.96\\right)  \\\\\n    & = \\Pr\\left(-1.96\\sqrt{\\hat{p}(1-\\hat{p})/n} &lt; \\hat{p} - p  &lt; 1.96\\sqrt{\\hat{p}(1-\\hat{p})/n}\\right) \\\\\n    & = \\Pr\\left(\\hat{p} -1.96\\sqrt{\\hat{p}(1-\\hat{p})/n} &lt;  p  &lt; \\hat{p} + 1.96\\sqrt{\\hat{p}(1-\\hat{p})/n}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "hypothesis.html#learning-objectives",
    "href": "hypothesis.html#learning-objectives",
    "title": "7  Hypothesis Testing",
    "section": "7.1 Learning Objectives",
    "text": "7.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive and implement a hypothesis test using each of the three classical test statistics to distinguish between two conflicting hypotheses\nDescribe the differences and relationships between Type I Error, Type II Error, and power, as well as the factors that influence each of them\nCalculate the power or Type II error for a given hypothesis test"
  },
  {
    "objectID": "hypothesis.html#reading-guide",
    "href": "hypothesis.html#reading-guide",
    "title": "7  Hypothesis Testing",
    "section": "7.2 Reading Guide",
    "text": "7.2 Reading Guide\nAssociated Readings: Chapter 6, Sections 6.1-6.5*\n*Note: I (Taylor) don’t love the way that the textbook discusses hypothesis testing. This reading will not be required, but is included in the course notes as a reference if you’d like to read about hypothesis testing from a different perspective.\n\n7.2.1 Reading Questions\n\nWhat is the goal of hypothesis testing?\nWhat are the typical steps to deriving a hypothesis test?\nWhat is the difference between a one-sided and a two-sided alternative hypothesis? How does this impact our hypothesis testing procedure? How does this impact our p-value?\nHow are test statistics and p-values related?\nHow is type I error related to the choice of significance level?\nWhat are the typical steps to calculating the probability of a type II error?\nHow is type II error related to the power of a hypothesis test?\nWhat factors influence the power of a test? In practice, which of these factors can we control?"
  },
  {
    "objectID": "hypothesis.html#definitions",
    "href": "hypothesis.html#definitions",
    "title": "7  Hypothesis Testing",
    "section": "7.3 Definitions",
    "text": "7.3 Definitions\nWald Test Statistic\nThe Wald test statistic \\(\\lambda_W\\) for testing the hypothesis \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta \\neq \\theta_0\\) is given by\n\\[\n\\lambda_W = \\left(\\frac{\\hat{\\theta}_{MLE} - \\theta_0}{se(\\hat{\\theta}_{MLE})}\\right)^2,\n\\]\nwhere \\(\\hat{\\theta}_{MLE}\\) is a maximum likelihood estimator.\nLikelihood Ratio Test (LRT) Statistic\nThe likelihood ratio test statistic \\(\\lambda_{LRT}\\) for testing the hypothesis \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta \\neq \\theta_0\\) is given by\n\\[\n\\lambda_{LRT} = -2 \\log\\left(\\frac{\\underset{\\theta = \\theta_0}{\\text{sup}} \\hspace{1mm} L(\\theta)}{\\underset{\\theta \\in \\Theta}{\\text{sup}} \\hspace{1mm} L(\\theta)}\\right),\n\\]\nwhere we note that the denominator, \\(\\underset{\\theta \\in \\Theta}{\\text{sup}} \\hspace{1mm} L(\\theta)\\), is the likelihood evaluated at the maximum likelihood estimator.\nScore Test Statistic\nThe score test statistic \\(\\lambda_S\\) for testing the hypothesis \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta \\neq \\theta_0\\) is given by\n\\[\n\\lambda_S = \\frac{\\left( \\frac{\\partial}{\\partial \\theta_0} \\log L(\\theta_0 \\mid x) \\right)^2}{I(\\theta_0)}.\n\\]\nPower\nPower is the probability that we correctly reject the null hypothesis; aka, the probability that we reject the null hypothesis, when the null hypothesis is actually false. As a conditional probability statement: \\(\\Pr(\\text{Reject }H_0 \\mid H_0 \\text{ False})\\). Note that\n\\[\n\\text{Power} = 1 - \\text{Type II Error}\n\\]\nType I Error (“False positive”)\nType I Error is the probability that the null hypothesis is rejected, when the null hypothesis is actually true. As a conditional probability statement: \\(\\Pr(\\text{Reject }H_0 \\mid H_0 \\text{ True})\\)\nType II Error (“False negative”)\nType II Error is the probability that we fail to reject the null hypothesis, given that the null hypothesis is actually false. As a conditional probability statement: \\(\\Pr(\\text{Fail to reject }H_0 \\mid H_0 \\text{ False})\\)\nCritical Region / Rejection Region\nThe critical/rejection region is defined as the set of values for which the null hypothesis would be rejected. This set is often denoted with a capital \\(R\\).\nCritical Value\nThe critical value is the point that separates the rejection region from the “acceptance” region (i.e., the value at which the decision for your hypothesis test would change). Acceptance is in quotes because we should never “accept” the null hypothesis… but we still call the “fail-to-reject” region the acceptance region for short.\nSignificance Level\nThe significance level, denoted \\(\\alpha\\), is the probability that, under the null hypothesis, the test statistic lies in the critical/rejection region.\nP-value\nThe p-value associated with a test statistic is the probability of obtaining a value as or more extreme than the observed test statistic, under the null hypothesis.\nUniformly Most Powerful (UMP) Test\nA “most powerful” test is a hypothesis test that has the greatest power among all possible tests of a given significance threshold \\(\\alpha\\). A uniformly most powerful (UMP) test is a test that is most powerful for all possible values of parameters in the restricted parameter space, \\(\\Theta_0\\).\nMore formally, let the set \\(R\\) denote the rejection region of a hypothesis test. Let\n\\[\n\\phi(x) = \\begin{cases} 1 & \\quad \\text{if } x \\in R \\\\ 0 & \\quad \\text{if } x \\in R^c \\end{cases}\n\\]\nThen \\(\\phi(x)\\) is an indicator function. Recalling that expectations of indicator functions are probabilities, note that \\(E[\\phi(x)] = \\Pr(\\text{Reject } H_0)\\). \\(\\phi(x)\\) then represents our hypothesis test. A hypothesis test \\(\\phi(x)\\) is UMP of size \\(\\alpha\\) if, for any other hypothesis test \\(\\phi'(x)\\) of size (at most) \\(\\alpha\\),\n\\[\n\\underset{\\theta \\in \\Theta_0}{\\text{sup}} E[\\phi'(X) \\mid \\theta] \\leq \\underset{\\theta \\in \\Theta_0}{\\text{sup}} E[\\phi(X) \\mid \\theta]\n\\]\nwe have that \\(\\forall \\theta \\in \\Theta_1\\),\n\\[\nE[\\phi'(X) \\mid \\theta] \\leq E[\\phi(X) \\mid \\theta],\n\\]\nwhere \\(\\Theta_0\\) is the set of all values for \\(\\theta\\) that align with the null hypothesis (sometimes just a single point, sometimes a region), and \\(\\Theta_1\\) is the set of all values for \\(\\theta\\) that align with the alternative hypothesis (sometimes just a single point, sometimes a region). Note: In general, UMP tests do not exist for two-sided alternative hypotheses. The Neyman-Pearson lemma tells us about UMP tests for simple null and alternative hypotheses, and the Karlin-Rubin theorem extends this to one-sided null and alternative hypotheses."
  },
  {
    "objectID": "hypothesis.html#theorems",
    "href": "hypothesis.html#theorems",
    "title": "7  Hypothesis Testing",
    "section": "7.4 Theorems",
    "text": "7.4 Theorems\nNeyman-Pearson Lemma\nConsider a hypothesis test with \\(H_0: \\theta = \\theta_0\\) and \\(H_1: \\theta = \\theta_1\\). Let \\(\\phi\\) be a likelihood ratio test of level \\(\\alpha\\), where \\(\\alpha = E[\\phi(X) \\mid \\theta_0]\\). Then \\(\\phi\\) is a UMP level \\(\\alpha\\) test for thee hypotheses \\(H_0: \\theta = \\theta_0\\) and \\(H_1: \\theta = \\theta_1\\).\nProof.\nLet \\(\\alpha = E[\\phi(X) \\mid \\theta_0]\\). Note that the LRT statistic is simplified in the case of these simple hypotheses, and can be written just as \\(\\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)}\\).* If the likelihood under the alternative is greater than some constant \\(c\\) (which depends on \\(\\alpha\\)), then we reject the null in favor of the alternative, and vice versa. Then the hypothesis testing function \\(\\phi\\) can be written as\n\\[\n\\phi(x) = \\begin{cases} 0 & \\quad \\text{if } \\lambda_{LRT} = \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &lt; c\\\\\n1 & \\quad \\text{if } \\lambda_{LRT} = \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &gt; c\\\\\n\\text{Flip a coin} & \\quad \\text{if } \\lambda_{LRT} = \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} = c\n\\end{cases}\n\\]Suppose \\(\\phi'\\) is any other test such that \\(E[\\phi'(X) \\mid \\theta_0] \\leq \\alpha\\) (another level \\(\\alpha\\) test). Then we must show that \\(E[\\phi'(X) \\mid \\theta_1] \\leq E[\\phi(X) \\mid \\theta_1]\\).\nBy assumption, we have\n\\[\\begin{align*}\n    E[\\phi(X) \\mid \\theta_0] &= \\int \\phi(x) f_X(x \\mid \\theta_0) dx = \\alpha \\\\\n    E[\\phi'(X) \\mid \\theta_0] &= \\int \\phi'(x) f_X(x \\mid \\theta_0) dx \\leq \\alpha\n\\end{align*}\\]\nTherefore we can write\n\\[\\begin{align*}\n    E[\\phi(X) & \\mid \\theta_1] - E[\\phi'(X) \\mid \\theta_1] \\\\\n    & = \\int \\phi(x) f_X(x \\mid \\theta_1) dx - \\int \\phi'(x) f_X(x \\mid \\theta_1) dx \\\\\n    & = \\int [\\phi(x) - \\phi'(x)] f_X(x \\mid \\theta_1) dx \\\\\n    & = \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &gt; c \\right\\}} \\underbrace{[\\phi(x) - \\phi'(x)]}_{\\geq 0} f_X(x \\mid \\theta_1) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &lt; c \\right\\}} \\underbrace{[\\phi(x) - \\phi'(x)]}_{\\leq 0} f_X(x \\mid \\theta_1) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} = c \\right\\}} [\\phi(x) - \\phi'(x)] f_X(x \\mid \\theta_1) dx \\\\\n    & \\geq \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &gt; c \\right\\}} [\\phi(x) - \\phi'(x)] cf_X(x \\mid \\theta_0) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &lt; c \\right\\}} [\\phi(x) - \\phi'(x)] cf_X(x \\mid \\theta_0) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} = c \\right\\}} [\\phi(x) - \\phi'(x)] cf_X(x \\mid \\theta_0) dx \\\\\n    & = c \\int [\\phi(x) - \\phi'(x)] f_X(x \\mid \\theta_0) dx \\\\\n    & = c \\int \\phi(x) f_X(x \\mid \\theta_0) dx - c \\int \\phi'(x) f_X(x \\mid \\theta_0) dx \\\\\n    & \\geq c(\\alpha - \\alpha) \\\\\n    & = 0\n\\end{align*}\\]\nAnd rearranging yields\n\\[\\begin{align*}\nE[\\phi(X)  \\mid \\theta_1] - E[\\phi'(X) \\mid \\theta_1] & \\geq 0 \\\\\nE[\\phi(X) \\mid \\theta_1] & \\geq E[\\phi'(X) \\mid \\theta_1]\n\\end{align*}\\]\nas desired.\n*Note: The \\(-2\\log(\\dots)\\) piece comes into play for the LRT statistic to ensure that the test statistic converges in distribution to a \\(\\chi^2\\) random variable. When we’re just comparing the LRT statistic to another LRT test statistic, we can (more simply) just compare the ratio of likelihoods. Think: comparing \\(X\\) vs. \\(Y\\) is equivalent to comparing \\(\\log(X)\\) vs. \\(\\log(Y)\\) if we are only interested in the direction of the difference between them, since \\(\\log\\) is a monotone function."
  },
  {
    "objectID": "hypothesis.html#worked-examples",
    "href": "hypothesis.html#worked-examples",
    "title": "7  Hypothesis Testing",
    "section": "7.5 Worked Examples",
    "text": "7.5 Worked Examples\nProblem 1: Let \\(Y_i \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\), where \\(\\sigma^2 = 25\\) is known. Suppose we want to test the hypotheses \\(H_0: \\mu = 8\\) vs. \\(H_1: \\mu \\neq 8\\) and we observe \\(\\overline{Y} = 10\\) across \\(n = 64\\) observations. Can we reject \\(H_0\\), with a significance threshold of \\(\\alpha = 0.05\\)? (Use a Wald test statistic)\n\n\nSolution:\n\nOur hypotheses are already stated in the problem set-up. The next thing we should do is derive a Wald test statistic. We know that the MLE for \\(\\mu\\) is given by \\(\\hat{\\mu}_{MLE} = \\overline{Y}\\) (we have shown this is previous problem sets/worked examples). Then the Wald test statistic can be written as\n\\[\n\\lambda_W = \\left( \\frac{\\hat{\\mu}_{MLE} - \\mu_0}{\\sigma/\\sqrt{n}}\\right)^2 = \\left( \\frac{10 - 8}{5/\\sqrt{64}}\\right)^2 = 10.24\n\\]\nWe can compare this test statistic to the critical value from a \\(\\chi^2_1\\) distribution since, by properties of normal distributions and recalling that standard normal distributions squared are \\(\\chi^2_1\\),\n\\[\\begin{align*}\n    \\overline{Y} & \\sim N(\\mu, \\sigma^2/n) \\\\\n    \\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}} & \\sim N(0,1) \\\\\n    \\left( \\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}  \\right)^2 & \\sim \\chi^2_1\n\\end{align*}\\]\nTo calculate the critical value when \\(\\alpha = 0.05\\), we turn to R.\n\n# The quantile function for a given distribution gives us the value at which\n# a given percentage of the distributions lies ahead of that value, which\n# is exactly what we want in this case!\n\nqchisq(1 - 0.05, df = 1)\n\n[1] 3.841459\n\n\nFinally, noting that our test statistic is greater than the critical value, we reject \\(H_0\\).\n\nProblem 2: Suppose we wanted to use a different significance level \\(\\alpha\\). How would the procedure in Problem 1 change if we let \\(\\alpha = 0.001\\)? How would the procedure in Problem 1 change if we let \\(\\alpha = 0.1\\)?\n\n\nSolution:\n\nChanging the significance level changes the critical value, and may change whether or not we reject \\(H_0\\), depending on the difference between our critical value and the test statistic. We can calculate what the critical value would be if we let \\(\\alpha = 0.01\\) and \\(\\alpha = 0.1\\) again in R:\n\n# alpha = 0.01\nqchisq(1 - 0.001, df = 1)\n\n[1] 10.82757\n\n# alpha = 0.1\nqchisq(1 - 0.1, df = 1)\n\n[1] 2.705543\n\n\nNote that when \\(\\alpha = 0.1\\), we still reject \\(H_0\\). This should make intuitive sense, since increasing \\(\\alpha\\) only can only increase our rejection region. However, when \\(\\alpha = 0.001\\), we would fail to reject \\(H_0\\), as our test statistic is not “more extreme” (greater) than the critical value.\n\nProblem 3: Suppose we have a random sample \\(X_1, \\dots, X_n \\sim Bernoulli(p)\\), and we want to test the hypotheses \\(H_0:p = 0.5\\), \\(H_1:p \\neq 0.5\\). Suppose we calculate an estimator for \\(p\\) as \\(\\hat{p} = \\frac{1}{n} \\sum_{i = 1}^n X_i\\). Derive a Wald test statistic for this hypothesis testing scenario (simplifying as much as you can).\n\n\nSolution:\n\nRecall that \\(\\hat{p}\\) as defined in the problem set-up is the MLE for \\(p\\). Then the Wald test statistic can be written as\n\\[\n\\lambda_W = \\left( \\frac{\\hat{p} - p_0}{se(\\hat{p})}\\right)^2.\n\\]\nWe can simplify a little further by calculating \\(se(\\hat{p})\\) and plugging in \\(p_0\\). Recall from the CLT (and Slutsky) that we have\n\\[\n\\left( \\frac{\\hat{p} - p_0}{\\sqrt{\\hat{p}(1 - \\hat{p})/n}} \\right) \\overset{d}{\\to} N(0,1)\n\\]\nThen the standard error of \\(\\hat{p}\\) is given by \\(\\sqrt{\\hat{p}(1 - \\hat{p})/n}\\), and our Walt test statistic simplifies to\n\\[\n\\lambda_W = \\left( \\frac{\\hat{p} - 0.5}{\\sqrt{\\hat{p}(1 - \\hat{p})/n}}\\right)^2.\n\\]\n(Note that this is as “simplified” as we can get without knowing \\(\\hat{p}\\) or \\(n\\))\n\nProblem 4: Derive a LRT statistic for the hypothesis testing scenario described in Problem 3 (simplifying as much as you can).\n\n\nSolution:\n\nThe LRT statistic is given by\n\\[\n\\lambda_{LRT} = -2 \\log\\left(\\frac{\\underset{p = p_0}{\\text{sup}} \\hspace{1mm} L(p)}{\\underset{p \\in \\Theta}{\\text{sup}} \\hspace{1mm} L(p)} \\right)= -2 \\log\\left(\\frac{ L(0.5)} {L(\\hat{p}_{MLE})} \\right)\n\\]\nThe likelihood for our observations can be written as\n\\[\nL(p) = \\prod_{i = 1}^n p^{x_i} (1 - p)^{(1 - x_i)}\n\\]\nAnd so our LRT statistic simplifies to\n\\[\\begin{align*}\n\\lambda_{LRT} & = -2 \\log\\left(\\frac{ L(0.5)} {L(\\hat{p})} \\right) \\\\\n& = -2 \\left[\\log L(0.5) - \\log L(\\hat{p}) \\right] \\\\\n& = -2 \\left[ \\log(0.5) \\sum_{i = 1}^n X_i + \\log(1 - 0.5)\\sum_{i = 1}^n(1 - X_i) - \\log(\\hat{p}) \\sum_{i = 1}^n X_i - \\log(1 - \\hat{p})\\sum_{i = 1}^n(1 - X_i)\\right] \\\\\n& = -2 \\left[ \\log(0.5) \\left( \\sum_{i = 1}^n X_i + \\sum_{i = 1}^n(1 - X_i)\\right) - \\log(\\hat{p}) \\sum_{i = 1}^n X_i - \\log(1 - \\hat{p})\\sum_{i = 1}^n(1 - X_i)\\right] \\\\\n& = -2 \\left[ n\\log(0.5) - \\log(\\hat{p}) \\sum_{i = 1}^n X_i - \\log(1 - \\hat{p})\\sum_{i = 1}^n(1 - X_i)\\right] \\\\\n& = -2 \\left[ n\\log(0.5) - \\log(\\hat{p}) n \\overline{X} - \\log(1 - \\hat{p}) (n - n \\overline{X})\\right] \\\\\n& = -2 n \\left[ \\log(0.5) - \\log(\\hat{p}) \\hat{p} - \\log(1 - \\hat{p}) (1 -  \\hat{p})\\right]\n\\end{align*}\\]\n(Note that this is as “simplified” as we can get without knowing \\(\\hat{p}\\) or \\(n\\))\n\nProblem 5: Derive a score test statistic for the hypothesis testing scenario described in Problem 3 (simplifying as much as you can).\n\n\nSolution:\n\nThe score test statistic is given by\n\\[\n\\lambda_S = \\frac{\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) \\right)^2}{I(p_0)}.\n\\]\nWe can simplify by deriving the score and information matrix, and then plugging in \\(p_0 = 0.5\\). We have,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) & = \\frac{\\partial}{\\partial p_0} \\left[ \\log(p_0) \\sum_{i = 1}^n X_i + \\log(1 - p_0) \\sum_{i = 1}^n (1 - X_i) \\right] \\\\\n& = \\frac{\\sum_{i = 1}^n X_i}{p_0} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - p_0} \\\\\n\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) \\right)^2 & = \\left(\\frac{\\sum_{i = 1}^n X_i}{p_0} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - p_0} \\right)^2\n\\end{align*}\\]\nand plugging in \\(p_0 = 0.5\\), we have,\n\\[\n\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x)\\right)^2 = \\left(\\frac{\\sum_{i = 1}^n X_i}{0.5} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - 0.5} \\right)^2 = \\left( \\frac{-n + 2\\sum_{i = 1}^n X_i}{0.5}\\right)^2 = \\left( -2n + 4\\sum_{i = 1}^n X_i\\right)^2.\n\\]\nThe information matrix is given by \\(-E\\left[ \\frac{\\partial^2}{\\partial p_0^2} \\log L(p_0 \\mid x)\\right]\\). Piecing this together,\n\\[\\begin{align*}\n\\frac{\\partial^2}{\\partial p_0^2} \\log L(p_0 \\mid x)\n& =  \\frac{\\partial}{\\partial p_0} \\left[ \\frac{\\sum_{i = 1}^n X_i}{p_0} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - p_0} \\right] \\\\\n& = \\frac{-\\sum_{i = 1}^n X_i}{p_0^2} - \\frac{n - \\sum_{i = 1}^n  X_i}{(1 - p_0)^2}\n\\end{align*}\\]\nAnd to get \\(I(p_0)\\), we take the negative expectation of the above quantity under the null hypothesis (that is, where \\(E[X] = p_0\\)) to obtain\n\\[\\begin{align*}\n    I(p_0) & = -E \\left[ \\frac{-\\sum_{i = 1}^n X_i}{p_0^2} - \\frac{n - \\sum_{i = 1}^n  X_i}{(1 - p_0)^2} \\right] \\\\\n    & = \\frac{1}{p_0^2} \\sum_{i = 1}^n E[X_i] + \\frac{1}{(1 - p_0)^2} \\left( n - \\sum_{i = 1}^n E[X_i]\\right) \\\\\n    & = \\frac{1}{p_0^2} \\sum_{i = 1}^n p_0 + \\frac{1}{(1 - p_0)^2} \\left( n - \\sum_{i = 1}^n p_0\\right) \\\\\n    & = \\frac{n}{p_0}  + \\frac{n}{(1 - p_0)^2} \\left( 1 - p_0\\right) \\\\\n    & = \\frac{n}{p_0}  + \\frac{n}{(1 - p_0)}\n\\end{align*}\\]\nAnd plugging in \\(p_0 = 0.5\\) we have\n\\[\nI(0.5) = \\frac{n}{0.5}  + \\frac{n}{(1 - 0.5)} = 2n + 2n = 4n.\n\\]\nThen, finally, the score test statistic (simplified as much as possible) is given by\n\\[\\begin{align*}\n    \\lambda_S & = \\frac{\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) \\right)^2}{I(p_0)} \\\\\n    & = \\frac{\\left( -2n + 4\\sum_{i = 1}^n X_i\\right)^2}{4n} \\\\\n    & = \\frac{\\left( -2n + 4n \\hat{p}\\right)^2}{4n} \\\\\n    & = \\frac{4n^2\\left( -1 + 2 \\hat{p}\\right)^2}{4n} \\\\\n    & = n\\left( -1 + 2 \\hat{p}\\right)^2\n\\end{align*}\\]\n\nProblem 6: For each of Problems 3, 4, and 5, calculate the p-values from each test when \\(\\hat{p} = 0.4\\) and \\(n = 300\\).\n\n\nSolution:\n\nWe’ll again use R to obtain the critical values for these hypothesis tests, noting that in each case, the test statistic follows a \\(\\chi^2_1\\) distribution asymptotically:\n\np_hat &lt;- 0.4\nn &lt;- 300\n\n# Wald test statistic\nlambda_w &lt;- ((p_hat - 0.5)/(sqrt(p_hat * (1 - p_hat) / n)))^2\n\n# LRT statistic\nlambda_lrt &lt;- -2 * n * (log(0.5) - log(p_hat) * p_hat - log(1 - p_hat) * (1 - p_hat))\n\n# Score test statistic\nlambda_s &lt;- n * (-1 + 2 * p_hat)^2\n\n# Compare statistics\nlambda_w\n\n[1] 12.5\n\nlambda_lrt\n\n[1] 12.08131\n\nlambda_s\n\n[1] 12\n\n# Calculate p-values\n# Recall: probability that we observe something *as or more extreme*\n1 - pchisq(lambda_w, df = 1)\n\n[1] 0.000406952\n\n1 - pchisq(lambda_lrt, df = 1)\n\n[1] 0.0005092985\n\n1 - pchisq(lambda_s, df = 1)\n\n[1] 0.0005320055\n\n\nThings to note:\n\nWhen \\(n\\) is large (300, in this case), each of the three classical test statistics are approximately equal! This makes sense, as they all converge in distribution to the same random variable, asymptotically.\nP-values are the probability that we would observe something as or more extreme than what we actually did observe, under the null hypothesis. In R, we can use the p function (for a given pdf) to calculate this.\n\n\nProblem 7: Repeat Problem 6 but with \\(\\hat{p} = 0.4\\) and \\(n = 95\\). If your significance threshold were \\(\\alpha = 0.05\\), would your conclusion to the hypothesis test be the same regardless of which test statistic you chose?\n\n\nSolution:\n\nTo answer this question, we can again calculate p-values, and compare them to 0.05 (note that we could have also calculated a critical value, and compared our test statistics to the critical value, as these are equivalent).\n\np_hat &lt;- 0.4\nn &lt;- 95\n\n# Wald test statistic\nlambda_w &lt;- ((p_hat - 0.5)/(sqrt(p_hat * (1 - p_hat) / n)))^2\n\n# LRT statistic\nlambda_lrt &lt;- -2 * n * (log(0.5) - log(p_hat) * p_hat - log(1 - p_hat) * (1 - p_hat))\n\n# Score test statistic\nlambda_s &lt;- n * (-1 + 2 * p_hat)^2\n\n# Compare statistics\nlambda_w\n\n[1] 3.958333\n\nlambda_lrt\n\n[1] 3.825748\n\nlambda_s\n\n[1] 3.8\n\n# Calculate p-values\n# Recall: probability that we observe something *as or more extreme*\n1 - pchisq(lambda_w, df = 1)\n\n[1] 0.04663986\n\n1 - pchisq(lambda_lrt, df = 1)\n\n[1] 0.05047083\n\n1 - pchisq(lambda_s, df = 1)\n\n[1] 0.05125258\n\n\nIn this case, we would reject \\(H_0\\) using the Wald test statistic, but fail to reject using the LRT statistic and score test statistic, since the only p-value that was below our significance threshold was the one calculated from the Wald test statistic. Finite-sample distributions of the three classical test statistics are generally unknown; only asymptotically have they been shown to be equivalent, and therefore, can provide different answers to hypothesis tests when sample sizes are relatively small."
  },
  {
    "objectID": "bayes.html#learning-objectives",
    "href": "bayes.html#learning-objectives",
    "title": "8  Bayesian Statistics",
    "section": "8.1 Learning Objectives",
    "text": "8.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nArticulate the differences in Frequentist and Bayesian philosophy\nDerive the posterior distribution for an unknown parameter based on a specified prior and likelihood\nEvaluate the properties of posterior means, medians, etc.\nArticulate the impact of the choice of prior distribution on Bayesian estimation"
  },
  {
    "objectID": "bayes.html#reading-guide",
    "href": "bayes.html#reading-guide",
    "title": "8  Bayesian Statistics",
    "section": "8.2 Reading Guide",
    "text": "8.2 Reading Guide\nAssociated Readings: Chapter 5, Section 5.8 (through Example 5.8.4)\n\n8.2.1 Reading Questions\n\nWhat is the difference between the Bayesian and Frequentist philosophies?\nWhat are the typical steps to deriving a posterior distribution?\nHow is the posterior distribution impacted by the observed data and our choice of prior? What sorts of considerations should we keep in mind in choosing a prior?\nHow are Bayes and maximum likelihood estimators typically related?\nWhat are typical Frequentist properties (e.g., bias, asymptotic bias, consistency) of Bayesian estimators (posterior means, for example)?"
  },
  {
    "objectID": "bayes.html#definitions",
    "href": "bayes.html#definitions",
    "title": "8  Bayesian Statistics",
    "section": "8.3 Definitions",
    "text": "8.3 Definitions\nBayes’ Theorem, Prior distribution, Posterior distribution\nFor two random variables \\(\\theta\\) and \\(\\textbf{X}\\), Bayes’ theorem states that,\n\\[\n\\pi(\\theta \\mid \\textbf{X}) = \\frac{\\pi(\\textbf{X} \\mid \\theta)\\pi(\\theta)}{\\pi(\\textbf{X})},\n\\]\nwhere \\(\\pi(\\theta)\\) denotes the prior distribution of \\(\\theta\\), \\(\\pi(\\textbf{X} \\mid \\theta)\\) denotes the likelihood, \\(\\pi(\\theta \\mid \\textbf{X})\\) denotes the posterior distribution of \\(\\theta\\), and \\(\\pi(\\textbf{X})\\) denotes the normalizing constant.\nImproper prior\nAn improper prior is a prior distribution that does not integrate to 1. This means that the prior is not a probability density function, since all pdfs must integrate to 1. In practice, some improper priors can still lead to proper posterior distributions, and as such, they are occasionally used as one type of non-informative prior. The most commonly used improper proper is the uniform distribution from \\(-\\infty\\) to \\(\\infty\\).\nConjugate prior\nA conjugate prior is a prior distribution that is in the same probability density family as the posterior distribution. Conjugate priors primarily used for computational convenience (as the posterior distributions then have closed form solutions), or when conjugacy makes sense in the context of the modeling problem. For examples of conjugate priors, the Wikipedia page linked here is quite complete.\nPosterior mode\nThe posterior mode is, as the name implies, the mode of the posterior distribution. In math, the posterior mode is the estimate \\(\\hat{\\theta}\\) that satisfies,\n\\[\n\\frac{\\partial}{\\partial \\theta}\\pi(\\theta \\mid \\textbf{X}) = 0.\n\\]\nPosterior median\nThe posterior median is, as the name implies, the median of the posterior distribution. In math, the posterior median is the estimate \\(\\hat{\\theta}\\) that satisfies,\n\\[\n\\int_{-\\infty}^{\\hat{\\theta}} \\pi(\\theta \\mid \\textbf{X}) d\\theta = 0.5\n\\]\nPosterior mean\nThe posterior mean is, as the name implies, the mean of the posterior distribution. In math, the posterior mean is the estimate\n\\[\n\\hat{\\theta} = E[\\theta \\mid \\textbf{X}] = \\int \\theta \\pi(\\theta \\mid \\textbf{X}) d\\theta\n\\]\nCredible interval\nA 100(1 - \\(\\alpha\\))% credible interval is an interval \\((\\Phi_{\\alpha/2}, \\Phi_{1 - \\alpha/2})\\) for a parameter \\(\\theta\\) is given by\n\\[\n\\int_{\\Phi_{\\alpha/2}}^{\\Phi_{1 - \\alpha/2}} \\pi(\\theta \\mid \\textbf{X}) d\\theta = 1 - \\alpha,\n\\]\nwhere \\(\\Phi_p\\) denotes the \\(p\\)th quantile of the posterior distribution."
  },
  {
    "objectID": "bayes.html#theorems",
    "href": "bayes.html#theorems",
    "title": "8  Bayesian Statistics",
    "section": "8.4 Theorems",
    "text": "8.4 Theorems\nNone for this chapter, other than Bayes’ theorem, which doesn’t really count as a theorem cause it’s just a probability rule!"
  },
  {
    "objectID": "bayes.html#worked-examples",
    "href": "bayes.html#worked-examples",
    "title": "8  Bayesian Statistics",
    "section": "8.5 Worked Examples",
    "text": "8.5 Worked Examples\nProblem 1: Suppose we have a random sample \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Bernoulli(\\theta)\\), and choose a \\(Beta(\\alpha, \\beta)\\) prior for \\(\\theta\\). Derive the posterior distribution, \\(\\pi(\\theta \\mid X_1, \\dots, X_n)\\).\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\pi(\\theta \\mid X_1, \\dots, X_n) & \\propto \\left( \\prod_{i = 1}^n f(x_i) \\right) \\pi(\\theta) \\\\\n    & = \\left( \\prod_{i = 1}^n \\theta^{x_i} (1 - \\theta)^{1 - x_i} \\right) \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\\n    & = \\theta^{\\sum_{i = 1}^n x_i} (1 - \\theta)^{n - \\sum_{i = 1}^n x_i}  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\\n    & \\propto \\theta^{\\sum_{i = 1}^n x_i + \\alpha - 1} (1 - \\theta)^{n - \\sum_{i = 1}^n x_i + \\beta - 1}\n\\end{align*}\\]\nwhere we recognize the kernel of a \\(Beta(\\sum_{i = 1}^n X_i + \\alpha, n - \\sum_{i = 1}^n X_i + \\beta)\\) distribution, and therefore this is the posterior distribution for \\(\\theta\\).\n\nProblem 2: Derive the posterior mean for \\(\\theta\\) in Problem 1.\n\n\nSolution:\n\nWe know that the expectation of a \\(Beta(a, b)\\) distribution is given by \\(\\frac{a}{a + b}\\), and so we have\n\\[\n\\hat{\\theta} = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{\\sum_{i = 1}^n X_i + \\alpha + n - \\sum_{i = 1}^n X_i + \\beta} = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{\\alpha + \\beta + n}\n\\]\n\nProblem 3: Write the posterior mean from Problem 2 as a function of the MLE, \\(\\hat{\\theta}_{MLE} = \\overline{X}\\), and the prior mean for \\(\\theta\\). What do you notice?\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\hat{\\theta} & = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{\\alpha + \\beta + n} \\\\\n    & = \\frac{\\sum_{i = 1}^n X_i }{\\alpha + \\beta + n} + \\frac{\\alpha}{\\alpha + \\beta + n} \\\\\n    & = \\frac{\\frac{n}{n} \\sum_{i = 1}^n X_i}{\\alpha + \\beta + n} + \\frac{\\frac{\\alpha (\\alpha + \\beta)}{\\alpha + \\beta}}{\\alpha + \\beta + n} \\\\\n    & = \\left( \\frac{n}{\\alpha + \\beta + n} \\right) \\overline{X} + \\left( \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\right) \\left( \\frac{\\alpha}{\\alpha + \\beta}\\right)\n\\end{align*}\\]\nand so we can see that the posterior mean is a weighted average of the prior mean and the MLE (in this case, the sample mean)!\n\nProblem 4: Suppose we have a random sample \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Poisson(\\lambda)\\), and choose a \\(Gamma(\\alpha, \\beta)\\) prior for \\(\\lambda\\). Derive the posterior distribution, \\(\\pi(\\lambda \\mid X_1, \\dots, X_n)\\).\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\pi(\\lambda \\mid X_1, \\dots, X_n) \\\\\n    & \\propto \\left( \\prod_{i = 1}^n f(x_i) \\right) \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\\\\n    & = \\left( \\prod_{i = 1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!} \\right) \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\\\\n    & = \\frac{\\lambda^{\\sum_{i = 1}^n x_i}e^{-n\\lambda}}{\\prod_{i = 1}^n x_i!} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\\\\n    & \\propto \\lambda^{\\sum_{i = 1}^n x_i + \\alpha - 1} e^{-n\\lambda - \\beta \\lambda} \\\\\n    & = \\lambda^{\\sum_{i = 1}^n x_i + \\alpha - 1} e^{-(n + \\beta)\\lambda}\n\\end{align*}\\]\nwhere we recognize the kernel of a \\(Gamma(\\sum_{i = 1}^n X_i + \\alpha, n + \\beta)\\) distribution, and therefore this is the posterior distribution for \\(\\lambda\\).\n\nProblem 5: Derive the posterior mean for \\(\\lambda\\) in Problem 4.\n\n\nSolution:\n\nWe know that the expectation of a \\(Gamma(a, b)\\) distribution is given by \\(\\frac{a}{b}\\), and so we have\n\\[\\begin{align*}\n    \\hat{\\lambda} = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{n + \\beta}\n\\end{align*}\\]\n\nProblem 6: Write the posterior mean from Problem 5 as a function of the MLE, \\(\\hat{\\lambda}_{MLE} = \\overline{X}\\), and the prior mean for \\(\\lambda\\). What do you notice?\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\hat{\\lambda} & = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{n + \\beta} \\\\\n    & = \\frac{\\sum_{i = 1}^n X_i}{n + \\beta} + \\frac{\\alpha}{n + \\beta} \\\\\n    & = \\frac{n \\overline{X}}{n + \\beta} + \\frac{\\frac{\\beta\\alpha}{\\beta}}{n + \\beta} \\\\\n    & = \\left( \\frac{n}{n + \\beta}  \\right) \\overline{X} + \\left( \\frac{\\beta}{n + \\beta} \\right) \\frac{\\alpha}{\\beta}\n\\end{align*}\\]\nand so we can see (again) that the posterior mean is a weighted average of the prior mean and the MLE (in this case, the sample mean)!\n\nProblem 7: What is the asymptotic behavior of the posterior means calculated in Problems 2 and 5?\n\n\nSolution:\n\nIn both cases, as \\(n \\to \\infty\\), the posterior mean will approach the MLE! This is easiest to note after we observe that the posterior mean is a weighted average of the MLE and the prior mean. The weight on the prior mean will approach zero, as the weight on the MLE will approach 1, as \\(n\\) goes to infinity."
  },
  {
    "objectID": "computation.html",
    "href": "computation.html",
    "title": "10  Computational Optimization",
    "section": "",
    "text": "Under development…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH/STAT 455: Mathematical Statistics",
    "section": "",
    "text": "Welcome to Mathematical Statistics!\nThis book contains the course notes for MATH/STAT 455: Mathematical Statistics at Macalester College, as taught by Prof. Taylor Okonek. These notes draw from course notes created by Prof. Kelsey Grinde, and from the course textbook, An Introduction to Mathematical Statistics and Its Applications by Richard Larsen and Morris Marx (6th Edition). Each chapter will contain (at a minimum):\n\nLearning Objectives\nReading Guide\nDefinitions\nTheorems\nWorked Examples\n\nI will be editing and adding to these notes throughout Spring 2024, so please check consistently for updates!\nIf you find any typos or have other questions, please email tokonek@macalester.edu."
  },
  {
    "objectID": "computation.html#newton-raphson",
    "href": "computation.html#newton-raphson",
    "title": "10  Computational Optimization",
    "section": "10.1 Newton-Raphson",
    "text": "10.1 Newton-Raphson\nRecall from the second chapter of the course notes the typical procedure for finding an MLE:\n\nFind the log likelihood\nTake a derivative with respect to the unknown parameter(s)\nSet it equal to zero, and solve\n\nWe previously saw that sometimes this procedure doesn’t work, in particular, when the support of the density function depends on our unknown parameters. In these cases, we noted that the MLE would be an order statistic. There are other situations, however, where neither the MLE is neither readily found analytically nor is it an order statistic. In these cases, we turn to computational techniques, such as Newton-Raphson.\nNewton-Raphson is a root-finding algorithm, and hence useful when trying to maximize a function (or a likelihood!). Suppose we want to find a root (i.e., the value of \\(x\\) such that \\(f(x) = 0\\)) of the function \\(f\\) with derivative denoted \\(f'\\). Newton-Raphson takes the following steps:\n\nStart with an initial guess \\(x_0\\)\nUpdate your guess according to \\(x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\\)\nRepeat step 2 according to \\(x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}\\) until your guesses have “converged” (i.e. are very very similar)\n\nA maximum likelihood estimator is the root of the first derivative of the log-likelihood (a.k.a. the value at which the derivative of the log-likelihood crosses zero). This means that, for finding MLEs, the Newton-Raphson algorithm replaces \\(f = \\frac{\\partial}{\\partial \\theta} \\log L(\\theta)\\).\nWe can visualize this process as follows:\n\n\n\n\n\n\n\n\n\nThe equation of the tangent line to the curve \\(y = f(x)\\) at a point \\(x = x_n\\) is \\[y = f'(x_n)(x-x_n) + f(x_n)\\]\n\n\n\n\n\n\n\n\n\nThe root of this tangent line (i.e., the place where it crosses the x-axis) is easy to find: \\[0 = f'(x_n)(x-x_n) + f(x_n) \\iff x = x_n - f(x_n)/f'(x_n)\\]\nTake this root of the tangent line as our next guess, then repeat…\n\n\n\n\n\n\n\n\n\n…and repeat…\n\n\n\n\n\n\n\n\n\n…and repeat…\n\n\n\n\n\n\n\n\n\n…and repeat…\n\n\n\n\n\n\n\n\n\n…and repeat…\n\n\n\n\n\n\n\n\n\n…and keep repeating until you’ve converged!\nThe multivariate version of Newton-Raphson is called the Scoring algorithm (also sometimes called Fisher’s scoring), and is used in \\(\\texttt{R}\\) to obtain estimates of logistic regression coefficients.\n\nMotivating Example: Logistic Regression\nSuppose that we observe data \\((y_i, x_i)\\) where the outcome \\(y\\) is binary. A natural model for these data is to assume the statistical model\n\\[\\begin{align*}\ny_i & \\sim Bernoulli(p_i), \\\\\n\\text{log} \\left( \\frac{p_i}{1 - p_i} \\right) & = \\beta_0 + \\beta_1 x_i.\n\\end{align*}\\]\nThis is a simple logistic regression model, with unknown parameters given by the logistic regression coefficients \\(\\beta_0, \\beta_1\\). Let’s attempt to find MLEs for \\(\\beta_0\\) and \\(\\beta_1\\) analytically.\nNote that \\(p_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\). Then the likelihood of our Bernoulli observations \\(y_i\\) can be written as\n\\[\nL(\\beta_0, \\beta_1) = \\prod_{i = 1}^n \\left( \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right)^{y_i} \\left( \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right)^{1 - y_i}\n\\]\nFollowing the typical procedure, we log the likelihood…\n\\[\\begin{align*}\n    \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ y_i \\log(\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}) + (1 - y_i) \\log(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}) \\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ y_i (\\beta_0 + \\beta_1 x_i) - y_i \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) - \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) + y_i \\log(1 + e^{\\beta_0 + \\beta_1 x_i})\\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ y_i (\\beta_0 + \\beta_1 x_i)  - \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) \\right]\n\\end{align*}\\]\n…taking the partial derivatives with respect to \\(\\beta_0\\) and \\(\\beta_1\\) we get…\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\beta_0} \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}  \\right]\\\\\n    \\frac{\\partial}{\\partial \\beta_1} \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ x_i \\left( y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right) \\right]\n\\end{align*}\\]\n…and if you try to solve the system of equations given by\n\\[\\begin{align*}\n    0 & \\equiv \\sum_{i = 1}^n \\left[ y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}  \\right]\\\\\n    0 & \\equiv \\sum_{i = 1}^n \\left[ x_i \\left( y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right) \\right]\n\\end{align*}\\]\nyou’ll get nowhere! There is no analytical (sometimes called “closed-form”) solution. In this case, we’d need to use the Scoring algorithm to solve for the regression coefficient estimates, since we have more than one unknown parameter.\n\n\nWhy do anything analytically, if Newton-Raphson exists?\nYou may be wondering why you’ve been doing calculus/algebra the entire semester, when such an algorithm exists. The answer is two-fold.\n\nGoing through the steps of finding an MLE analytically helps build intuition. We saw that in the vast majority of cases, maximum likelihood estimators are functions of sample means. This is less obvious when doing everything numerically (using an algorithm). In addition to gaining insight from finding MLEs by hand, this practice also gave you the opportunity to learn/use common “tricks” in statistics, that will find their way into problems you complete down the road or research you may eventually conduct.\nNumerical optimization is slow. For simple cases like the ones we’ve seen in class, numerical optimization would techniques like Newton-Raphson would run relatively quickly. However, for more complex likelihoods with many unknown parameters, various optimization techniques can be so slow as to be computationally prohibitive. Even with continual improvements in computational power (and improvements in the algorithms themselves), computational speed is an important consideration when conducting statistical research or developing new methodology. If it takes someone two weeks to fit their regression model using numerical optimization, for example, that person may never fit a regression model ever again, or give up entirely. Especially when considering who has access to computational power, this can become an equity issue. If you can solve something analytically, do it. It’s significantly faster in the long-term, even it takes you some time to do the calculus/algebra."
  },
  {
    "objectID": "computation.html#simulation-studies",
    "href": "computation.html#simulation-studies",
    "title": "10  Computational Optimization",
    "section": "10.2 Simulation Studies",
    "text": "10.2 Simulation Studies\nSometimes proofs are hard. In such cases (and more generally), it can often be useful to “test” or observe properties of estimators in a computational setting, rather than in a rigorous mathematical context. This is where simulation studies come into play, and if you eventually find yourself conducting statistical research, knowing how to conduct a well-designed, reproducible, simulation study is an incredibly important skill.\nThe general idea of simulation study is to generate realistic settings (data) that could be observed in the real world, in order to compare properties of various estimators and their behavior in scenarios where the “truth” is known (because you generated the truth!). Steps include:\n\nDetermine your simulation settings (different parameter values, sample sizes, etc.)\nGenerate many data sets for each setting\nCompute your estimator / implement your method for each data set\nRecord the relevant property of that estimator / method for each data set\nSummarize your results across data sets and simulation settings\n\nThis can be a great way to get a feel for how certain estimators/methods behave in different settings without needing to rigorously prove something. Additionally, it can be used to inform more rigorous proofs down the line; if we can better understand how estimators/methods behave, we may be able to relate that behavior to existing proofs and build upon them!"
  },
  {
    "objectID": "computation.html#gibbs-samplers",
    "href": "computation.html#gibbs-samplers",
    "title": "10  Computational Optimization",
    "section": "10.3 Gibbs Samplers",
    "text": "10.3 Gibbs Samplers\nNot everything is conjugate."
  }
]