[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH/STAT 355: Statistical Theory",
    "section": "",
    "text": "Welcome to Statistical Theory!\nThis book contains the course notes for MATH/STAT 355: Statistical Theory* at Macalester College, as taught by Prof. Taylor Okonek. These notes draw from reading guides created by Prof. Kelsey Grinde, a little bit from the textbook, An Introduction to Mathematical Statistics and Its Applications by Richard Larsen and Morris Marx (6th Edition), and the STAT 512/513 Course Notes developed by Dr. Michael Perlman at the University of Washington. As of Spring 2025, this course no longer requires a textbook, and relies heavily on these course notes instead.\nEach chapter of the course notes will contain (at a minimum):\n\nTopic Introduction\nLearning Objectives\nConcept Questions\nDefinitions\nTheorems\nWorked Examples\n\nI will be editing and adding to these notes throughout Spring 2025, so please check consistently for updates!\nIf you find any typos or have other questions, please email tokonek@macalester.edu.\n* MATH/STAT 355: Statistical Theory went under the title MATH/STAT 455: Mathematical Statistics prior to Spring 2025. The course content is largely similar, the differences primarily being the structure of the course and not the content itself.",
    "crumbs": [
      "Welcome to Statistical Theory!"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "1  Probability: A Brief Review",
    "section": "",
    "text": "1.1 Learning Objectives\nMATH/STAT 355 builds directly on topics covered in MATH/STAT 354: Probability. You’re not expected to perfectly remember everything from Probability, but you will need to have sufficient facility with the following topics covered in this review Chapter in order to grasp the majority of concepts covered in MATH/STAT 355.\nBy the end of this chapter, you should be able to…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability: A Brief Review</span>"
    ]
  },
  {
    "objectID": "probability.html#learning-objectives",
    "href": "probability.html#learning-objectives",
    "title": "1  Probability: A Brief Review",
    "section": "",
    "text": "Distinguish between important probability models (e.g., Normal, Binomial)\nDerive the expectation and variance of a single random variable or a sum of random variables\nDefine the moment generating function and use it to find moments or identify pdfs\nDerive pdfs of transformations of continuous random variables",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability: A Brief Review</span>"
    ]
  },
  {
    "objectID": "probability.html#concept-questions",
    "href": "probability.html#concept-questions",
    "title": "1  Probability: A Brief Review",
    "section": "1.2 Concept Questions",
    "text": "1.2 Concept Questions\n\nWhich probability distributions are appropriate for quantitative (continuous) random variables?\nWhich probability distributions are appropriate for categorical random variables?\nIndependently and Identically Distributed (iid) random variables are an incredibly important assumption involved in many statistical methods. Why do you think it might be important/useful for random variables to have this property?\nWhy might we want to be able to derive distribution functions for transformations of random variables? In what scenarios can you imagine this being useful?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability: A Brief Review</span>"
    ]
  },
  {
    "objectID": "probability.html#definitions",
    "href": "probability.html#definitions",
    "title": "1  Probability: A Brief Review",
    "section": "1.3 Definitions",
    "text": "1.3 Definitions\nYou are expected to know the following definitions:\nRandom Variable\nA random variable is a function that takes inputs from a sample space of all possible outcomes, and outputs real values or probabilities. As an example, consider a coin flip. The sample space of all possible outcomes consists of “heads” and “tails”, and each outcome is associated with a probability (50% each, for a fair coin). For our purposes, you should know that random variables have probability density (or mass) functions, and are either discrete or continuous based on the number of possible outcomes a random variable may take. Random variables are often denoted with capital Roman letters, like \\(X\\), \\(Y\\), \\(Z\\), etc.\nProbability density function (discrete, continuous)\n\nNote: I don’t care if you call a pmf a pdf… I will probably do this continuously throughout the semester. We don’t need to be picky about this in MATH/STAT 355.\n\nThere are many different accepted ways to write the notation for a pdf of a random variables. Any of the following are perfectly appropriate for this class: \\(f(x)\\), \\(\\pi(x)\\), \\(p(x)\\), \\(f_X(x)\\). I typically use either \\(\\pi\\) or \\(p\\), but might mix it up occasionally.\nKey things I want you to know about probability density functions:\n\n\\(\\pi(x) \\geq 0\\), everywhere. This should make sense (hopefully) because probabilities cannot be negative!\n\\(\\int_{-\\infty}^\\infty \\pi(x) = 1\\). This should also (hopefully) makes sense. Probabilities can’t be greater than one, and the probability of event occurring at all (ever) should be equal to one, if the event \\(x\\) is a random variable.\n\nCumulative distribution function (discrete, continuous)\nCumulative distribution functions we’ll typically write as \\(F_X(x)\\). or \\(F(x)\\), for short. It is important to know that\n\\[\nF_X(x) = \\Pr(X \\leq x),\n\\]\nor in words, “the cumulative distribution function is the probability that a random variable lies before \\(x\\).” If you write \\(\\Pr(X &lt; x)\\) instead of \\(\\leq\\), you’re fine. The probability that a random variable is exactly one number (for an RV with a continuous pdf) is zero anyway, so these are the same thing. Key things I want you to know about cumulative distribution functions:\n\n\\(F(x)\\) is non-decreasing. This is in part where the “cumulative” piece comes in to play. Recall that probabilities are basically integrals or sums. If we’re integrating over something positive, and our upper bound for our integral increases, the area under the curve (cumulative probability) will increase as well.\n\\(0 \\leq F(x) \\leq 1\\) (since probabilities have to be between zero and one!)\n\\(\\Pr(a &lt; X \\leq b) = F(a) - F(b)\\) (because algebra)\n\nJoint probability density function\nA joint probability density function is a probability distribution defined for more than one random variable at a time. For two random variables, \\(X\\) and \\(Z\\), we could write their joint density function as \\(f_{X,Z}(x, z)\\) , or \\(f(x,z)\\) for short. The joint density function encodes all sorts of fun information, including marginal distributions for \\(X\\) and \\(Z\\), and conditional distributions (see next bold definition). We can think of the joint pdf as listing all possible pairs of outputs from the density function \\(f(x,z)\\), for varying values of \\(x\\) and \\(z\\). Key things I want you to know about joint pdfs:\n\nHow to get a marginal pdf from a joint pdf:\nSuppose I want to know \\(f_X(x)\\), and I know \\(f_{X,Z}(x,z)\\). Then I can integrate or “average over” \\(Z\\) to get\n\\[\nf_X(x) = \\int f_{X,Z}(x,z)dz\n\\]\nThe relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule\nHow to obtain a joint pdf for independent random variables: just multiply their marginal pdfs together! This is how we will (typically) think about likelihoods!\nHow to obtain a marginal pdf from a joint pdf when random variables are independent without integrating (think, “separability”)\n\nConditional probability density function\nA conditional pdf denotes the probability distribution for a (set of) random variable(s), given that the value for another (set of) random variable(s) is known. For two random variables, \\(X\\) and \\(Z\\), we could write the conditional distribution of \\(X\\) “given” \\(Z\\) as \\(f_{X \\mid Z}(x \\mid z)\\) , where the “conditioning” is denoted by a vertical bar (in LaTeX, this is typeset using “\\mid”). Key things I want you to know about conditional pdfs:\n\nThe relationship between conditional pdfs, marginal pdfs, joint pdfs, and Bayes’ theorem/rule\nHow to obtain a conditional pdf from a joint pdf (again, think Bayes’ rule)\nRelationship between conditional pdfs and independence (see next bold definition)\n\nIndependence\nTwo random variables \\(X\\) and \\(Z\\) are independent if and only if:\n\n\\(f_{X,Z}(x,z) = f_X(x) f_Z(z)\\) (their joint pdf is “separable”)\n\\(f_{X\\mid Z}(x\\mid z) = f_X(x)\\) (the pdf for \\(X\\) does not depend on \\(Z\\) in any way)\nNote that the “opposite” is also true: \\(f_{Z\\mid X}(z\\mid x) = f_Z(z)\\)\n\nIn notation, we denote that two variables are independent as \\(X \\perp\\!\\!\\!\\perp Z\\), or \\(X \\perp Z\\). In LaTeX, the latter is typeset as “\\perp”, and the former is typeset as “\\perp\\!\\!\\!\\perp”. As a matter of personal preference, I (Taylor) prefer \\(\\perp\\!\\!\\!\\perp\\), but I don’t like typing it out every time. Consider using the “\\newcommand” functionality in LaTeX to create a shorthand for this for your documents!\nJacobian Matrix\nLet \\(f\\) be a 1-1 and onto function, where \\(f(x_i) = y_i\\) for \\(i = 1, \\dots, n\\). Then the Jacobian matrix of \\(f\\) is the matrix of partial derivatives,\n\\[\nJ_f(x) = \\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_n}{\\partial x_1} \\\\\n\\vdots & & \\vdots \\\\\n\\frac{\\partial y_1}{\\partial x_n} & \\dots & \\frac{\\partial y_n}{\\partial x_n}\n\\end{pmatrix}\n\\]\nThe Jacobian matrix is sometimes simply referred to as the “Jacobian”, but be careful when simply calling it the Jacobian, since this can sometimes refer to the determinant of the Jacobian matrix as well. For this course, we’ll always refer to the Jacobian matrix as a matrix, and the “Jacobian” as its determinant.\nJacobian\nThe Jacobian is the determinant of the Jacobian matrix, denoted by \\(| J_f(x) |\\). Recall from linear algebra that \\(Det(A) = Det(A^\\top)\\). This is convenient, because it means we won’t have worry too much about remembering which order our partial derivatives go in our matrix, for 2x2 matrices (which is all we’ll be working with for this course).\nExpected Value / Expectation\nThe expectation (or expected value) of a random variable is defined as:\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\nExpected value is a weighted average, where the average is over all possible values a random variable can take, weighted by the probability that those values occur. Key things I want you to know about expectation:\n\nThe relationship between expectation, variance, and moments (specifically, that \\(E[X]\\) is the 1st moment!)\nThe “law of the unconscious statistician” (see the Theorems section of this chapter)\nExpectation of linear transformations of random variables (see Theorems section of this chapter)\n\nVariance\nThe variance of a random variable is defined as:\n\\[\nVar[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nIn words, we can read this as “the expected value of the squared deviation from the mean” of a random variable \\(X\\). Key things I want you to know about variance:\n\nThe relationship between expectation, variance, and moments (hopefully clear, given the formula for variance)\nThe relationship between variance and standard deviation: \\(Var(X) = sd(X)^2\\)\nThe relationship between variance and covariance: \\(Var(X) = Cov(X, X)\\)\n\\(Var(X) \\geq 0\\). This should make sense, given that we’re taking the expectation of something “squared” in order to calculate it!\n\\(Var(c) = 0\\) for any constant, \\(c\\).\nVariance of linear transformations of random variables (see Theorems section of this chapter)\n\n\\(r^{th}\\) moment\nThe \\(r^{th}\\) moment of a probability distribution is given by \\(E[X^r]\\). For example, when \\(r = 1\\), the \\(r^{th}\\) moment is just the expectation of the random variable \\(X\\). Key things I want you to know about moments:\n\nThe relationship between moments, expectation, and variance\n\nFor example, if you know the first and second moments of a distribution, you should be able to calculate the variance of a random variable with that distribution!\n\nThe relationship between moments and moment generating functions (see Theorems section of this chapter)\n\nCovariance\nThe covariance of two random variables is a measure of their joint variability. We denote the covariance of two random variables \\(X\\) and \\(Z\\) as \\(Cov(X,Z)\\), and\n\\[\nCov(X, Z) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\n\\]\nSome things I want you to know about covariance:\n\n\\(Cov(X, X) = Var(X)\\)\n\\(Cov(X, Y) = Cov(Y, X)\\) (order doesn’t matter)\n\nMoment Generating Function (MGF)\nThe moment generating function of a random variable \\(X\\) is defined as\n\\[\nM_X(t) = E[e^{tX}]\n\\]\nA few things to note:\n\n\\(M_X(0) = 1\\), always.\nIf two random variables have the same MGF, they have the same probability distribution!\nMGFs are sometimes useful for showing how different random variables are related to each other\n\n\n1.3.1 Distributions Table\nYou are also expected to know the probability distributions contained in Table 1, below. Note that you do not need to memorize the pdfs for these distributions, but you should be familiar with what types of random variables (continuous/quantitative, categorical, integer-valued, etc.) may take on different distributions. The more familiar you are with the forms of the pdfs, the easier/faster it will be to work through problem sets and quizzes.\n\nTable 1. Table of main probability distributions we will work with for MATH/STAT 355.\n\n\n\n\n\n\n\n\nDistribution\nPDF/PMF\nParameters\nSupport\n\n\n\n\nUniform\n\\(\\pi(x) = \\frac{1}{\\beta - \\alpha}\\)\n\\(\\alpha \\in \\mathbb{R}\\), \\(\\beta\\in \\mathbb{R}\\)\n\\(x \\in [\\alpha, \\beta]\\)\n\n\nNormal\n\\(\\pi(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2)\\)\n\\(\\mu \\in \\mathbb{R}\\), \\(\\sigma &gt; 0\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nMultivariate Normal\n\\(\\pi(\\textbf{x}) = (2\\pi)^{-k/2} |\\Sigma|^{-1/2} \\exp(-\\frac{1}{2}(\\textbf{x} - \\mu)^\\top \\Sigma^{-1}(\\textbf{x} - \\mu)))\\)\n\\(\\mu \\in \\mathbb{R}^k\\), \\(\\Sigma \\in \\mathbb{R}^{k\\times k}\\) , positive semi-definite (in practice, almost always positive definite)\n\\(x \\in \\mathbb{R}^{k}\\)\n\n\nGamma\n\\(\\pi(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\\)\n\\(\\alpha \\text{ (shape)}, \\beta \\text{ (rate)} &gt; 0\\)\n\\(x \\in (0,\\infty)\\)\n\n\nChi-squared\n\\(\\pi(x) = \\frac{2^{-\\nu/2}}{\\Gamma(\\nu/2)} x^{\\nu/2 - 1}e^{-x/2}\\)\n\\(\\nu &gt; 0\\)\n\\(x \\in [0, \\infty)\\)\n\n\n\\(F\\)\n\\(\\pi(x) = \\frac{\\Gamma(\\frac{\\nu_1 + \\nu_2}{2})}{\\Gamma(\\frac{\\nu_1}{2}) \\Gamma(\\frac{\\nu_2}{2})} \\left( \\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2} \\left( \\frac{x^{(\\nu_1 - 2)/2}}{\\left( 1 + \\left( \\frac{\\nu_1}{\\nu_2}\\right)x\\right)^{(\\nu_1 + \\nu_2)/2}}\\right)\\)\n\\(\\nu_1, \\nu_2 &gt; 0\\)\n\\(x \\in [0, \\infty)\\)\n\n\nExponential\n\\(\\pi(x) = \\beta e^{-\\beta x}\\)\n\\(\\beta &gt; 0\\)\n\\(x \\in [0, \\infty)\\)\n\n\nLaplace (Double Exponential)\n\\(\\pi(x) = \\frac{1}{2b} \\exp( - \\frac{|x - \\mu|}{b})\\)\n\\(\\mu \\in \\mathbb{R}\\), \\(b &gt; 0\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nStudent-\\(t\\)\n\\(\\pi(x) = \\frac{\\Gamma((\\nu + 1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu \\pi}} (1 + \\frac{x^2}{\\nu})^{-(\\nu + 1)/2}\\)\n\\(\\nu &gt; 0\\)\n\\(x \\in \\mathbb{R}\\)\n\n\nBeta\n\\(\\pi(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha - 1}(1 - x)^{\\beta - 1}\\)\n\\(\\alpha, \\beta &gt; 0\\)\n\\(x \\in [0,1]\\)\n\n\nPoisson\n\\(\\pi(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\n\\(\\lambda &gt; 0\\)\n\\(x \\in \\mathbb{N}\\)\n\n\nBinomial\n\\(\\pi(x) = \\binom{n}{x} p^{x} (1 - p)^{n - x}\\)\n\\(p \\in [0,1], n = \\{0, 1, 2, \\dots\\}\\)\n\\(x \\in \\{0, 1, \\dots, n\\}\\)\n\n\nMultinomial\n\\(\\pi(\\textbf{x}) = \\frac{n!}{x_1! \\dots x_k!} p_1^{x_1} \\dots p_k^{x_k}\\)\n\\(p_i &gt; 0\\), \\(p_1 + \\dots + p_k = 1\\), \\(n = \\{0, 1, 2, \\dots \\}\\)\n\\(\\{ x_1, \\dots, x_k \\mid \\sum_{i = 1}^k x_i = n, x_i \\geq 0 (i = 1, \\dots, k)\\}\\)\n\n\nNegative Binomial\n\\(\\pi(x) = \\binom{x + r - 1}{x} (1-p)^x p^r\\)\n\\(r &gt; 0\\), \\(p \\in [0,1]\\)\n\\(x \\in \\{0, 1, \\dots\\}\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability: A Brief Review</span>"
    ]
  },
  {
    "objectID": "probability.html#theorems",
    "href": "probability.html#theorems",
    "title": "1  Probability: A Brief Review",
    "section": "1.4 Theorems",
    "text": "1.4 Theorems\n\nLaw of Total Probability\n\\[\nP(A) = \\sum_n P(A \\cap B_n),\n\\]or\n\\[\nP(A) = \\sum_n P(A \\mid B_n) P(B_n)\n\\]\nBayes’ Theorem\n\\[\n\\pi(A \\mid B) = \\frac{\\pi(B \\mid A) \\pi(A)}{\\pi(B)}\n\\]\nRelationship between pdf and cdf\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t)dt\n\\]\n\\[\n\\frac{\\partial}{\\partial y}F_Y(y) = f_Y(y)\n\\]\nExpectation of random variables\n\\[\nE[X] = \\int_{-\\infty}^\\infty x f(x) dx\n\\]\n\\[\nE[X^2] = \\int_{-\\infty}^\\infty x^2 f(x) dx\n\\]\n\n“Law of the Unconscious Statistician”\n\\[\nE[g(X)] = \\int_{-\\infty}^\\infty g(x)f(x)dx\n\\]\n\nExpectation and variance of linear transformations of random variables\n\\[\nE[cX + b] = c E[X] + b\n\\]\n\\[\nVar[cX + b] = c^2 Var[X]\n\\]\nRelationship between mean and variance\n\\[\nVar[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n\\]\nAlso, recall that \\(Cov[X, X] = Var[X]\\).\nIterated Things\n\n\\[\nE[X] = E[E[X \\mid Y]]\n\\] and \\[\nVar(X) = E[Var(X \\mid Y)] + Var(E[X \\mid Y])\n\\]\n\nFinding a marginal pdf from a joint pdf\n\\[\nf_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(x, y) dy\n\\]\nIndependence of random variables and joint pdfs\nIf two random variables are independent, their joint pdf will be separable. For example, if \\(X\\) and \\(Y\\) are independent, we could write\n\\[\nf_{X,Y}(x, y) = f_{X}(x)f_Y(y)\n\\]\nExpected value of a product of independent random variables\nSuppose random variables \\(X_1, \\dots, X_n\\) are independent. Then we can write,\n\\[\nE\\left[\\prod_{i = 1}^n X_i\\right] = \\prod_{i = 1}^n E[X_i]\n\\]\nCovariance of independent random variables\nIf \\(X\\) and \\(Y\\) are independent, then \\(Cov(X, Y) = 0\\). We can show this by noting that\n\n\\[\\begin{align}\nCov(X, Y) & = E[(X - E[X])(Y - E[Y])] \\\\\n& = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\\\\n& = E[XY] - E[XE[Y]] - E[YE[X]] + E[X]E[Y] \\\\\n& =  2E[X]E[Y] - 2E[X]E[Y] \\\\\n& = 0\n\\end{align}\\]\n\nUsing MGFs to find moments\nRecall that the moment generating function of a random variable \\(X\\), denoted by \\(M_X(t)\\) is\n\\[\nM_X(t) = E[e^{tX}]\n\\]\nThen the \\(n\\)th moment of the probability distribution for \\(X\\) , \\(E[X^n]\\), is given by\n\\[\n\\frac{\\partial M_X}{\\partial t^n} \\Bigg|_{t = 0}\n\\]\nwhere the above reads as “the \\(n\\)th derivative of the moment generating function, evaluated at \\(t = 0\\).”\nUsing MGFs to identify pdfs\nMGFs uniquely identify probability density functions. If \\(X\\) and \\(Y\\) are two random variables where for all values of \\(t\\), \\(M_X(t) = M_Y(t)\\), then \\(F_X(x) = F_Y(y)\\).\nCentral Limit Theorem\nThe classical CLT states that for independent and identically distributed (iid) random variables \\(X_1, \\dots, X_n\\), with expected value \\(E[X_i] = \\mu\\) and \\(Var[X_i] = \\sigma^2 &lt; \\infty\\), the sample average (centered and standardized) converges in distribution to a standard normal distribution at a root-\\(n\\) rate. Notationally, this is written as\n\\[\n\\sqrt{n} (\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\n A fun aside: this is only one CLT, often referred to as the Levy CLT. There are other CLTs, such as the Lyapunov CLT and Lindeberg-Feller CLT!\n\n\n1.4.1 Transforming Continuous Random Variables\nWe will often take at face value previously proven relationships between random variables. What I mean by this, as an example, is that it is a nice (convenient) fact that a sum of two independent normal random variables is still normally distributed, with a nice form for the mean and variance. In particular, if \\(X \\sim N(\\mu, \\sigma^2)\\) and \\(Y \\sim N(\\theta, \\nu^2)\\), then \\(X + Y \\sim N(\\mu + \\theta, \\sigma^2 + \\nu^2)\\). Most frequently used examples of these sorts of relationships can be found in the “Related Distributions” section of the Wikipedia page for a given probability distribution. Unless I explicitly ask you to derive/show how certain variables are related to each other, you can just state the known relationship, use it, and move on!\nIf I do ask you to derive/show these things, there a few different ways we can go about this. For this course, I expect you to know the “CDF method” for one function of one random variable, and the “Jacobian method” for a function of more than one random variable.\n\n1.4.1.1 CDF Method: One random variable\nTheorem. Let \\(X\\) be a continuous random variable with pdf \\(f_X(x)\\). Define a new random variable \\(Y = g(X)\\), for nice* functions \\(g\\). Then \\(f_Y(y) = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))}\\).\n *By nice functions we mean functions that are strictly increasing and smooth on the required range. As an example, \\(exp(x)\\) is a smooth, strictly increasing function; \\(|x|\\) is not on the whole real line, but is from \\((0, \\infty)\\) (where a lot of useful pdfs are defined). For the purposes of this class, every function that you will need to do this for will be “nice.” Note that there are also considerations that need to be taken regarding the range of continuous random variables when considering transforming them. We will mostly ignore these considerations in this class, but a technically complete derivation (or proof) must consider them.\n\n\nProof.\n\nWe can write\n\\[\\begin{align*}\n    f_Y(y) & = \\frac{\\partial}{\\partial y} F_Y(y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(Y \\leq y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(g(X) \\leq y) \\\\\n    & = \\frac{\\partial}{\\partial y} \\Pr(X \\leq g^{-1}(y)) \\\\\n    & = \\frac{\\partial}{\\partial y} F_X(g^{-1}(y)) \\\\\n    & = f_X(g^{-1}(y)) \\times \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align*}\\]\nwhere to obtain the last equality we use chain rule! Now we require some statistical trickery to continue… (note that this method is called the “CDF method” because we go through the CDF to derive the distribution for \\(Y\\))\nYou will especially see this in the Bayes chapter of our course notes, but it is often true that our lives are made easier as statisticians if we multiply things by one, or add zero. What exactly do I mean? Rearranging gross looking formulas into things we are familiar with (like pdfs, for example) often makes our lives easier and allows us to avoid dealing with such grossness. Here, the grossness is less obvious, but nonetheless relevant. Note that we can write\n\\[\\begin{align*}\n    y & = y \\\\\n    y & = g(g^{-1}(y)) \\\\\n    \\frac{\\partial}{\\partial y} y & = \\frac{\\partial}{\\partial y} g(g^{-1}(y)) \\\\\n    1 & = g'(g^{-1}(y)) \\frac{\\partial}{\\partial y} g^{-1}(y) \\hspace{1cm} \\text{(chain rule again!)} \\\\\n    \\frac{1}{g'(g^{-1}(y))} & = \\frac{\\partial}{\\partial y} g^{-1}(y)\n\\end{align*}\\]\nThe right-hand side should look familiar: it is exactly what we needed to “deal with” in our proof! Returning to that proof, we have\n\\[\\begin{align*}\n    f_Y(y) & = f_X(g^{-1}(y)) \\times \\frac{\\partial}{\\partial y} g^{-1}(y) \\\\\n    & = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))}\n\\end{align*}\\]\nas desired.\n\n\n\n1.4.1.2 Jacobian Method: More than one random variable\nTransformations of single random variables are great, but we’ll need to work with transformations of more than one random variable if we want to be able to manipulate joint pdfs. Suppose, for example, we have \\(X \\sim Gamma(\\alpha, \\lambda)\\) and \\(Y \\sim Gamma(\\beta, \\lambda)\\), where \\(X \\perp\\!\\!\\!\\perp Y\\). Let \\(U = X + Y\\) and \\(W = \\frac{X}{X + Y}\\). How do we show that \\(U\\) and \\(W\\) are independent? Through finding the joint pdf! Which means we need a method for transforming more than one, continuous random variable. Enter the Jacobian Method.\nTheorem. Let \\(X = (x_1, \\dots, x_n)\\) be a vector of random variables (a random vector) with pdf \\(\\pi_{X}(x)\\). Suppose that \\(f(x) = y\\) is a smooth, 1-1 and onto function, and that \\(|J_f(x)| &gt; 0\\) almost everywhere. Then \\(\\pi_Y(y)\\) is given by\n\\[\n\\pi_Y(y) = \\pi_{X}(f^{-1}(y)) \\times \\bigg| \\frac{\\partial x}{\\partial y} \\bigg| \\times I_Y\\{y\\}\n\\] where \\(I_Y \\{y \\}\\) denotes the support of \\(Y\\).\nNOTE: \\(| J_f(y) | = | \\frac{\\partial x}{\\partial y} |\\), above, where \\(f(x) = y\\). This means that the numerators and denominators in the Jacobian Matrix in the definition in the Course Notes are flipped here. The reason for this becomes clear in the proof sketched below, noting that \\(| \\frac{\\partial x}{\\partial y} |^{-1} = | \\frac{\\partial y}{\\partial x} |\\), for the supposed functions \\(f\\).\n\n\nProof.\n\nNote that if \\(f\\) is both 1-1 and onto, then \\(f\\) is either monotone increasing or monotone decreasing. We’ll prove the case where \\(f\\) is increasing, and we’ll note (but not show) how the decreasing case follows directly.\nLet \\(f\\) be a smooth, monotone increasing function. For some subset \\(C \\subseteq \\mathbb{R}^n\\),\n\\[\\begin{align*}\n\\int_C \\pi_Y(y) dy & = \\Pr(Y \\in C) \\\\\n& = \\Pr(f(X) \\in C) \\\\\n& = \\Pr(X \\in f^{-1}(C)) \\\\\n& = \\int_{f^{-1}(C)} \\pi_X(x) dx\n\\end{align*}\\]\nNow we’ll use a (convoluted) \\(u\\)-substitution to make this look like what we want it to look like. Let \\(u = f^{-1}(y)\\). Note that this also means \\(u = x\\). Then \\(du = (f^{-1}(y))' dy = dx\\). Proceeding with \\(u\\)-substitution, we have\n\\[\\begin{align*}\n\\int_C \\pi_Y(y) dy & = \\int_{f^{-1}(C)} \\pi_X(x) dx \\\\\n& = \\int_C \\pi_X(u) du \\\\\n& = \\int_C \\pi_X(f^{-1}(y)) (f^{-1}(y))' dy \\\\\n& = \\int_C \\pi_X(f^{-1}(y))\n\\bigg| \\frac{dx}{dy} \\bigg| dy\n\\end{align*}\\] which implies \\(\\pi_Y(y) = \\pi_X(f^{-1}(y))\n\\bigg| \\frac{dx}{dy} \\bigg|\\), as desired. The absolute value signs (the Jacobian piece) come into play to help us deal with the decreasing case.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability: A Brief Review</span>"
    ]
  },
  {
    "objectID": "probability.html#worked-examples",
    "href": "probability.html#worked-examples",
    "title": "1  Probability: A Brief Review",
    "section": "1.5 Worked Examples",
    "text": "1.5 Worked Examples\nProblem 1: Suppose \\(X \\sim Exponential(\\lambda)\\). Calculate \\(E[X]\\) and \\(Var[X]\\).\n\n\nSolution:\n\nWe know that \\(f(x) = \\lambda e^{-\\lambda x}\\). If we can calculate \\(E[X]\\) and \\(E[X^2]\\), then we’re basically done! We can write\n\\[\\begin{align*}    \nE[X] & = \\int_0^\\infty x \\lambda e^{-\\lambda x} dx \\\\    \n& = \\lambda \\int_0^\\infty x e^{-\\lambda x} dx\n\\end{align*}\\]\nAnd now we need integration by parts! Set \\(u = x\\), \\(dv = e^{-\\lambda x} dx\\). Then \\(du = 1dx\\) and \\(v = \\frac{-1}{\\lambda} e^{-\\lambda x}\\). Since \\(\\int u dv = uv - \\int vdu\\), we can continue\n\\[\\begin{align*}    \nE[X] & = \\lambda \\int_0^\\infty x e^{-\\lambda x} dx \\\\    \n& = \\lambda \\left( -\\frac{x}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  - \\int_0^\\infty \\frac{-1}{\\lambda} e^{-\\lambda x} dx \\right) \\\\    \n& = \\lambda \\left( - \\int_0^\\infty \\frac{-1}{\\lambda} e^{-\\lambda x} dx \\right) \\\\    \n& = \\lambda \\left( \\frac{-1}{\\lambda^2} e^{-\\lambda x}  \\bigg|_0^\\infty \\right) \\\\    \n& = \\frac{-1}{\\lambda} e^{-\\lambda x}  \\bigg|_0^\\infty \\\\    \n& = \\frac{1}{\\lambda} e^{-0} \\\\    \n& = \\frac{1}{\\lambda}\n\\end{align*}\\]\nWe can follow a similar process to get \\(E[X^2]\\) (using the law of the unconscious statistician!). We can write\n\\[\\begin{align*}\n    E[X^2] & = \\int_0^\\infty x^2 \\lambda e^{-\\lambda x} dx \\\\\n    & = \\lambda \\int_0^\\infty x^2 e^{-\\lambda x} dx\n\\end{align*}\\]\nAnd now we need integration by parts again! Set \\(u = x^2\\), \\(dv = e^{-\\lambda x} dx\\). Then \\(du = 2xdx\\) and \\(v = \\frac{-1}{\\lambda} e^{-\\lambda x}\\). Since \\(\\int u dv = uv - \\int vdu\\), we can continue\n\\[\\begin{align*}\n    E[X] & = \\lambda \\int_0^\\infty x^2 e^{-\\lambda x} dx \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  - \\int_0^\\infty \\frac{-2}{\\lambda} xe^{-\\lambda x} dx \\right) \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  + \\frac{2}{\\lambda} \\int_0^\\infty  xe^{-\\lambda x} dx \\right) \\\\\n    & = \\lambda \\left( -\\frac{x^2}{\\lambda} e^{-\\lambda x} \\bigg|_0^\\infty  + \\frac{2}{\\lambda^3} \\right)\\\\\n    & = \\lambda \\left( 0  + \\frac{2}{\\lambda^3} \\right) \\\\\n    & = \\frac{2}{\\lambda^2}\n\\end{align*}\\]\nNow we can calculate \\(Var[X] = E[X^2] - E[X]^2\\) as \\[\nVar[X] = E[X^2] - E[X]^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\n\\] And so we have \\(E[X] = \\frac{1}{\\lambda}\\) and \\(Var[X] = \\frac{1}{\\lambda^2}\\).\n\nProblem 2: Show that an exponentially distributed random variable is “memoryless”, i.e. show that \\(\\Pr(X &gt; s + x \\mid X &gt; s) = \\Pr(X &gt; x)\\), \\(\\forall s\\).\n\n\nSolution:\n\nRecall that the CDF of an exponential distribution is given by \\(F(x) = 1-e^{-\\lambda x}\\). Thanks to Bayes rule, we can write\n\\[\\begin{align*}\n    \\Pr(X &gt; s + x \\mid X &gt; s) & = \\frac{\\Pr(X &gt; s + x , X &gt; s)}{\\Pr(X &gt; s)} \\\\\n    & = \\frac{\\Pr(X &gt; s + x)}{\\Pr(X &gt; s)} \\\\\n    & = \\frac{1 - \\Pr(X &lt; s + x)}{1 - \\Pr(X &lt; s)} \\\\\n    & = \\frac{1 - F(s + x)}{1 - F(s)}\n\\end{align*}\\]\nwhere the second equality is true because \\(x &gt; 0\\). Then we can write\n\\[\\begin{align*}\n    \\Pr(X &gt; s + x \\mid X &gt; s) & = \\frac{1 - F(s + x)}{1 - F(s)} \\\\\n    & = \\frac{1 - \\left(1 - e^{-\\lambda(s + x)}\\right)}{1 - \\left(1 - e^{-\\lambda s}\\right)} \\\\\n    & = \\frac{e^{-\\lambda(s + x)}}{e^{-\\lambda s}} \\\\\n    & = \\frac{e^{-\\lambda s - \\lambda x}}{e^{-\\lambda s}} \\\\\n    & = e^{-\\lambda x} \\\\\n    & = 1 - F(x) \\\\\n    & = \\Pr(X &gt; x)\n\\end{align*}\\]\nand we’re done!\n\nProblem 3: Suppose \\(X \\sim Exponential(1/\\lambda)\\), and \\(Y \\mid X \\sim Poisson(X)\\). Show that \\(Y \\sim Geometric(1/(1 + \\lambda))\\).\n\n\nSolution:\n\nNote that we can write \\(f(x, y) = f(y \\mid x) f(x)\\), and \\(f(y) = \\int f(x, y) dx\\). Then\n\\[\nf(x, y) = \\left( \\frac{1}{\\lambda} e^{-x/\\lambda} \\right) \\left( \\frac{x^y e^{-x}}{y!} \\right)\n\\] And so,\n\\[\\begin{align*}\n    f(y) & = \\int f(x, y) dx \\\\\n    & = \\int \\left( \\frac{1}{\\lambda} e^{-x/\\lambda} \\right) \\left( \\frac{x^y e^{-x}}{y!} \\right) dx \\\\\n    & = \\frac{1}{\\lambda y!} \\int x^y e^{-x(1 + \\lambda)/\\lambda} dx\n\\end{align*}\\]\nAnd we can again use integration by parts! Let \\(u = x^y\\) and \\(dv = e^{-x(1 + \\lambda)/\\lambda} dx\\). Then we have \\(du = yx^{y-1} dx\\) and \\(v = -\\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda}\\), and we can write\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda y!} \\int x^y e^{-x(1 + \\lambda)/\\lambda} dx \\\\\n    & = \\frac{1}{\\lambda y!} \\left( -x^y \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} \\bigg|_{x = 0}^{x = \\infty}  + \\int \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} yx^{y-1} dx\\right) \\\\\n    & = \\frac{1}{\\lambda y!} \\left(  \\int \\frac{\\lambda}{1 + \\lambda}e^{-x(1 + \\lambda)/\\lambda} yx^{y-1} dx \\right) \\\\\n    & = \\frac{1}{\\lambda y!} \\left( \\frac{\\lambda }{1 + \\lambda} \\right) y \\left(  \\int e^{-x(1 + \\lambda)/\\lambda} x^{y-1} dx \\right)\n\\end{align*}\\]\nThis looks gross, but it’s actually not so bad. Note that, since \\(Y\\) is Poisson, it can only take integer values beginning at 1! Then we can repeat the process of integration by parts \\(y\\) times in order to get rid of \\(x^{y\\dots}\\) term on the inside of the integral. Specifically, each time we do this process we will pull out a \\(\\left( \\frac{\\lambda }{1 + \\lambda} \\right)\\), and a \\(y - i\\) for the \\(i\\)th integration by parts step (try this one or two steps for yourself to see how it will simplify if you find this unintuitive!). We end up with,\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda y!} \\left( \\frac{\\lambda }{1 + \\lambda} \\right)^y y! \\\\\n    & = \\frac{1}{\\lambda} \\left(\\frac{\\lambda}{1 + \\lambda}\\right)^y\n\\end{align*}\\]\nNow let \\(p = \\frac{1}{1 + \\lambda}\\). If we can show that \\(f(y) \\sim Geometric(p)\\) then we’re done. Note that \\(1 - p = \\lambda/(1 + \\lambda)\\). We have\n\\[\\begin{align*}\n    f(y) & = \\frac{1}{\\lambda} (1 - p)^y \\\\\n    & = \\frac{1}{\\lambda} (1 - p)^{y-1} (1-p) \\\\\n    & = (1 - p)^{y-1} \\frac{1}{\\lambda} \\left( \\frac{\\lambda}{1 + \\lambda} \\right) \\\\\n    & = (1 - p)^{y-1} \\left( \\frac{1}{1 + \\lambda} \\right) \\\\\n    & = (1 - p)^{y-1} p\n\\end{align*}\\]\nwhich is exactly the pdf of a geometric random variable with parameter \\(p\\) and trials that begin at 1.\nAn alternative solution (which perhaps embodies the phrase “work smarter, not harder”) actually doesn’t involve integration by parts at all! As statisticians, we typically like to avoid actually integrating anything whenever possible, and this is often achieved by manipulating algebra enough to essentially “create” a pdf out of what we see (since pdfs integrate to \\(1\\)!). Massive props to a student for solving this problem in a much “easier” way, answer below:\n\\[\\begin{align*}\n        f(y) &= \\int_{0}^{\\infty} f(y \\mid x) f(x) dx \\\\\n        &= \\int_{0}^{\\infty} (\\frac{1}{\\lambda}e^{-\\frac{x}{\\lambda}}) (\\frac{x^y}{y!} e^{-x}) dx \\\\\n        &= \\frac{1}{\\lambda y!} \\int_{0}^{\\infty} x^y e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx \\\\\n        &= \\frac{1}{\\lambda y!} \\int_{0}^{\\infty} \\frac{(\\frac{1+\\lambda}{\\lambda})^{y+1}}{(\\frac{1+\\lambda}{\\lambda})^{y+1}} \\frac{\\Gamma(y+1)}{\\Gamma(y+1)} x^{(y+1)-1} e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx\\\\\n        &= \\frac{\\Gamma(y+1)}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} \\int_{0}^{\\infty} \\frac{(\\frac{1+\\lambda}{\\lambda})^{y+1}}{\\Gamma(y+1)} x^{(y+1)-1} e^{-\\frac{x}{\\lambda}(1 + \\lambda)} dx\\\\\n        &= \\frac{\\Gamma(y+1)}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} (1)\\\\\n        &= \\frac{y!}{\\lambda y! (\\frac{1+\\lambda}{\\lambda})^{y+1}} \\\\\n        &= \\frac{\\lambda^{-1}}{(\\frac{1+\\lambda}{\\lambda})^{y+1}}\\\\\n        &= \\frac{\\lambda^y}{(1+\\lambda)^{y+1}}\\\\\n        &= \\frac{1}{(1+\\lambda)} \\frac{\\lambda^y}{(1+\\lambda)^y}\\\\\n        &= \\frac{1}{(1+\\lambda)} (1 - \\frac{1}{(1+\\lambda)})^y \\\\\n        &=p(1-p)^y\\qquad (\\text{where }p=\\frac{1}{1+\\lambda})\n    \\end{align*}\\]\nNote that we arrive at a slightly different answer with this approach. Specifically, we arrive at the pdf of a geometric random variable with parameter \\(p\\) and trials that begin at 0, as opposed to 1. There’s some subtlety here that we’re going to choose to ignore.\n\nProblem 4: Suppose that \\(X \\sim N(\\mu, \\sigma^2)\\), and let \\(Y = \\frac{X - \\mu}{\\sigma}\\). Find the distribution of \\(Y\\) (simplifying all of your math will be useful for this problem).\n\n\nSolution:\n\nTo solve this problem, we can use the theorem on transforming continuous random variables. We must first define our function \\(g\\) that relates \\(X\\) and \\(Y\\). In this case, we have \\(g(a) = \\frac{a - \\mu}{\\sigma}\\). Now all we need to do is collect the mathematical “pieces” we need to use theorem: \\(g^{-1}(a)\\), and \\(g'(a)\\), and finally, the pdf of a normal random variable. We have\n\\[\\begin{align*}\n    f_X(x) & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2) \\\\\n    g^{-1}(a) & = \\sigma a + \\mu \\\\\n    g'(a) & = \\frac{\\partial}{\\partial a} \\left(\\frac{a - \\mu}{\\sigma}\\right) = \\frac{1}{\\sigma}\n\\end{align*}\\]\nPutting it all together, we have\n\\[\\begin{align*}\n    f_Y(y) & = f_X(g^{-1}(y)) \\times \\frac{1}{g'(g^{-1}(y))} \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (\\sigma y + \\mu - \\mu)^2) \\times \\sigma \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp(-\\frac{1}{2\\sigma^2} (\\sigma y)^2) \\times \\sigma \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2\\sigma^2} \\sigma^2 y^2) \\\\\n    & = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2} y^2)\n\\end{align*}\\]\nand note that this is the pdf of a normally distributed random variable with mean \\(0\\) and variance \\(1\\)! Thus, we have shown that \\(\\frac{X - \\mu}{\\sigma} \\sim N(0,1)\\). Fun Fact: If this random variable reminds you of a Z-score, it should!\n\nProblem 5: Suppose the joint pdf of two random variables \\(X\\) and \\(Y\\) is given by \\(f_{X,Y}(x,y) = \\lambda \\beta e^{-x\\lambda - y\\beta}\\). Determine if \\(X\\) and \\(Y\\) are independent, showing why or why not.\n\n\nSolution:\n\nTo determine whether \\(X\\) and \\(Y\\) are independent (or not), we need to determine if their joint pdf is “separable.” Doing some algebra, we can see that\n\\[\\begin{align*}    \nf_{X,Y}(x,y) & = \\lambda \\beta e^{-x \\lambda - y\\beta} \\\\    \n& = \\lambda \\beta e^{-x \\lambda} e^{-y \\beta} \\\\   \n& = \\left( \\lambda  e^{-x \\lambda} \\right) \\left( \\beta e^{-y \\beta} \\right)\n\\end{align*}\\]\nand so since we can write the joint distribution as a function of \\(X\\) multiplied by a function of \\(Y\\), \\(X\\) and \\(Y\\) are independent (and in this case, both have exponential distributions).\n\nProblem 6: Suppose the joint pdf of two random variables \\(X\\) and \\(Y\\) is given by \\(f_{X,Y}(x,y) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\binom{n}{y} x^{y + \\alpha - 1} (1-x)^{n-y + \\beta - 1}\\). Determine if \\(X\\) and \\(Y\\) are independent, showing why or why not.\n\n\nSolution:\n\nTo determine whether \\(X\\) and \\(Y\\) are independent (or not), we need to determine if their joint pdf is “separable.” Right away, we should note that a piece of the pdf contains \\(x^y\\), and therefore we are never going to be able to fully separate out this joint pdf into a function of \\(x\\) times a function \\(y\\). Therefore, \\(X\\) and \\(Y\\) are not independent. In this case, we actually have \\(X \\sim Beta(\\alpha, \\beta)\\), and \\(Y \\mid X \\sim Binomial(n, y)\\) (we’ll return to this example in the Bayes chapter!).\n\nProblem 7: Let \\(X\\) and \\(Y\\) be independent random variables with \\(X \\sim Exponential(1)\\) and \\(Y \\sim Exponential(1)\\). Find the joint distribution of \\(Z = X - Y\\) and \\(W = X + Y\\), and use this joint distribution to show that \\(Z \\sim Laplace(0, 1)\\).\n\n\nSolution:\n\nLet \\(X\\) and \\(Y\\) be independent random variables with \\(X \\sim Exponential(1)\\) and \\(Y \\sim Exponential(1)\\). Find the joint distribution of \\(Z = X - Y\\) and \\(W = X + Y\\), and use this joint distribution to show that \\(Z \\sim Laplace(0, 1)\\).\nWe’ll need a couple things before we can directly apply the Jacobian method:\n\nThe joint distribution, \\(\\pi_{X,Y}(x,y)\\)\nOur function \\(f(x,y)\\) and its inverse\nOur Jacobian matrix, given by \\(J_{f}(z,w) = \\begin{pmatrix} \\frac{\\partial x}{\\partial z} & \\frac{\\partial x}{\\partial w} \\\\\\frac{\\partial y}{\\partial z} & \\frac{\\partial y}{\\partial w} \\end{pmatrix}\\)\nOur Jacobian, given by \\(\\bigg| J_{f}(z,w) \\bigg|\\)\nThe support of the joint distribution \\(\\pi_{Z,W}(z,w)\\) (we’ll do this step at the end).\n\nSince \\(X\\) and \\(Y\\) are independent, we have \\[\n\\pi_{X,Y}(x,y) = e^{-x}e^{-y} = e^{-x - y}\n\\] Now we must determine what our function \\(f\\) is, and its inverse. From the problem set-up, we have \\(f(x, y) \\longmapsto (x - y, x + y)\\). To find the inverse function, we can rearrange these outputs to define \\(x\\) and \\(y\\) solely in terms of \\(z\\) and \\(w\\). Some algebra included below: \\[\\begin{align*}\n    x & = z + y \\\\\n    y & = w - x \\\\\n    x & = z + w - x \\\\\n    2x & = z + w \\\\\n    x & = \\frac{z + w}{2} \\quad \\text{(Our first equation)!} \\\\\n    y & = w - \\frac{z + w}{2} \\\\\n    y & = \\frac{2w - z - w}{2} \\\\\n    y & = \\frac{w - z}{2} \\quad \\text{(Our second equation!)}\n\\end{align*}\\] Which gives us \\(f^{-1}(z,w) \\longmapsto (\\frac{z + w}{2}, \\frac{w - z}{2})\\). The Jacobian matrix is then given by \\[\nJ_f(z,w) = \\begin{pmatrix}\n    \\frac{\\partial (\\frac{z + w}{2})}{\\partial z} & \\frac{\\partial (\\frac{z + w}{2})}{\\partial w} \\\\\n     \\frac{\\partial (\\frac{w - z}{2})}{\\partial z} & \\frac{\\partial (\\frac{w - z}{2})}{\\partial w}\n\\end{pmatrix} = \\begin{pmatrix}\n     \\frac{1}{2} & \\frac{1}{2} \\\\\n     \\frac{-1}{2} & \\frac{1}{2}\n     \\end{pmatrix}\n\\] and the Jacobian is given by \\(|J_f(z,w)| = (1/2)(1/2) - (1/2)(-1/2) = 1/2\\).\nNow we determine the support of the distribution \\(\\pi_{Z,W} (z,w)\\). Since \\(X\\) and \\(Y\\) are exponential, we know that \\[\\begin{align*}\n    & 0 \\leq x &lt; \\infty \\\\\n    & 0 \\leq y &lt; \\infty\n\\end{align*}\\] Plugging in some things and rearranging, we get \\[\\begin{align*}\n    & 0 \\leq \\frac{z + w}{2} &lt; \\infty \\\\\n    & 0 \\leq z + w &lt; \\infty \\\\\n    & -z \\leq w &lt; \\infty \\quad \\text{(or)} -w \\leq z &lt; \\infty\n\\end{align*}\\] and\n\\[\\begin{align*}\n    & 0 \\leq \\frac{w - z}{2} &lt; \\infty \\\\\n    & 0 \\leq w - z &lt; \\infty \\\\\n    & z \\leq w &lt; \\infty\n\\end{align*}\\]\nPutting these together, we have \\(-w \\leq z \\leq w &lt; \\infty\\), so the support for \\(z\\), marginally is given by \\([-w, w]\\). The support for \\(w\\), marginally, is given by \\([0, \\infty)\\), since it is a sum of two random variables that have that same support. Note that this therefore means \\(z\\) can range from \\((-\\infty, \\infty)\\), depending on the value of \\(w\\).\nWe can now finally apply the Jacobian method to obtain \\(\\pi_{Z,W}(z,w)\\) using the separate pieces we have calculated, obtaining: \\[\\begin{align*}\n    \\pi_{Z,W}(z,w) & = \\pi_{X,Y}(f^{-1}(z,w)) \\times | J_f(z,w) |  \\times I \\{ -w \\leq z \\leq w, 0 \\leq w &lt; \\infty \\}\\\\\n    & = \\exp(-\\frac{z + w}{2} - \\frac{w-z}{2}) \\times \\frac{1}{2} \\times I \\{ -w \\leq z \\leq w, 0 \\leq w &lt; \\infty \\}\\\\\n    & = \\frac{1}{2} \\exp(\\frac{-z - w - w + z}{2}) \\times I \\{ -w \\leq z \\leq w, 0 \\leq w &lt; \\infty \\}\\\\\n    & = \\frac{1}{2} e^{-w} \\times I \\{ -w \\leq z \\leq w, 0 \\leq w &lt; \\infty \\}\n\\end{align*}\\] Now that we have the joint distribution \\(\\pi_{Z,W}(z,w)\\), we must integrate with respect to \\(W\\) to get the marginal distribution of \\(Z\\). Recall that we have both \\(-z \\leq w &lt; \\infty\\) and \\(z \\leq w &lt; \\infty\\), so we consider these two cases separately. We have \\[\n\\pi_Z(z) = \\begin{cases}\n    \\int_z^\\infty \\frac{1}{2} e^{-w} dw = - \\frac{1}{2} e^{-w} \\big|_z^\\infty = \\frac{1}{2}e^{-z} & 0 \\leq z &lt; \\infty\\\\\n    \\int_{-z}^\\infty \\frac{1}{2} e^{-w} dw = - \\frac{1}{2} e^{-w} \\big|_{-z}^\\infty = \\frac{1}{2}e^{z} & -\\infty &lt; z \\leq 0\n\\end{cases}\n\\] which can equivalently be written as \\[\n\\pi_Z(z) = \\frac{1}{2} e^{-|z|} \\quad -\\infty &lt; z &lt; \\infty\n\\] which implies \\(Z \\sim Laplace(0,1)\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Probability: A Brief Review</span>"
    ]
  },
  {
    "objectID": "mle.html",
    "href": "mle.html",
    "title": "2  Maximum Likelihood Estimation",
    "section": "",
    "text": "Introduction to MLE\nIn Probability, you calculated probabilities of events by assuming a probability model for data and then assuming you knew the value of the parameters in that model. In Mathematical Statistics, we will similarly write down a probability model but then we will use observed data to estimate the value of the parameters in that model.\nThere is more than one technique that you can use to estimate the value of an unknown parameter. You’re already familiar with one technique—least squares estimation—from STAT 155. We’ll review the ideas behind that approach later in the course. To start, we’ll explore two other widely used estimation techniques: maximum likelihood estimation (this chapter) and the method of moments (next chapter).\nTo understand maximum likelihood estimation, we can first break down each individual word in that phrase: (1) maximum, (2) likelihood, (3) estimation. We’ll start in reverse order.\nRecall from your introductory statistics course that we are (often) interested in estimating true, unknown parameters in statistics, using some data. Our best guess at the truth, based on the data we observe / sample that we have, is an estimate of the truth (given some modeling assumptions). This is all the “estimation” piece is getting at here. We’re going to be learning about a method that produces estimates!\nThe likelihood piece may be less familiar to you. A likelihood is essentially a fancy form of a function (see the Definitions section for an exact definition), that combines an assumed probability distribution for your data, with some unknown parameters.* The key here is that a likelihood is a function. It may look more complicated than a function like \\(y = mx + b\\), but we can often manipulate them in a similar fashion, which comes in handy when trying to find the…\nMaximum! We’ve maximized functions before, and we can do it again! There are ways to maximize functions numerically (using certain algorithms, such as Newton-Raphson for example, which we’ll cover in a later chapter), but we will primarily focus on maximizing likelihoods analytically in this course to help us build intuition.\nRecall from calculus: To maximize a function we…\nThe last two steps we’ll often skip in this class, since things have a tendency to work out nicely with most likelihood functions. If we are trying to maximize a likelihood with multiple parameters, there a few different ways we can go about this. One way (which is nice for distributions like the multivariate normal) is to place all of the parameters in a vector, write the distribution in terms of matrices and vectors, and then use matrix algebra to obtain all of the MLEs for each parameter at once! An alternative way is to take partial derivatives of the likelihood function with respect to each parameter, and solve a system of equations to obtain MLEs for each parameter. We’ll see an example of this in Problem Set 1 as well as Worked Example 2!\nOne final thing to note (before checking out worked examples and making sure you have a grasp on definitions and theorems) is that it is often easier to maximize the log-likelihood as opposed to the the likelihood… un-logged. This is for a variety of reasons, one of which is that many common probability density functions contain some sort of \\(e^x\\) term, and logging (natural logging) simplifies that for us. Another one is that log rules sometimes make taking derivatives easier. The value of a parameter that maximizes the log-likelihood is the same value that maximizes the likelihood, un-logged (since log is a monotone, increasing function). This is truly just a convenience thing!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#learning-objectives",
    "href": "mle.html#learning-objectives",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive maximum likelihood estimators for parameters of common probability density functions\nCalculate maximum likelihood estimators “by hand” for common probability density functions\nExplain (in plain English) why maximum likelihood estimation is an intuitive approach to estimating unknown parameters using a combination of (1) observed data, and (2) a distributional assumption",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#concept-questions",
    "href": "mle.html#concept-questions",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.2 Concept Questions",
    "text": "2.2 Concept Questions\n\nWhat is the intuition behind the maximum likelihood estimation (MLE) approach?\nWhat are the typical steps to find a MLE?\nAre there ever situations when the typical steps to finding a MLE don’t work? If so, what can we do instead to find the MLE?\nHow do the steps to finding a MLE change when we have more than one unknown parameter?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#definitions",
    "href": "mle.html#definitions",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.3 Definitions",
    "text": "2.3 Definitions\nYou are expected to know the following definitions:\nParameter\nIn a frequentist* framework, a parameter is a fixed, unknown truth (very philosophical). By fixed, I mean “not random”. We assume that there is some true unknown value, governing the generation of all possible random observations of all possible people and things in the whole world. We sometimes call this unknown governing process the “superpopulation” (think: all who ever have been, all who are, and all who ever will be).\nPractically speaking, parameters are things that we want to estimate, and we will estimate them using observed data!\n*Two main schools of thought in statistics are: (1) Frequentist (everything you’ve ever learned so far in statistics, realistically), and (2) Bayesian. We’ll cover the latter, and differences between the two, in a later chapter. There’s also technically Fiducial inference as a third school of thought, but that one’s never been widely accepted.\nStatistic/Estimator\nA statistic (or “estimator”) is a function of your data, used to “estimate” an unknown parameter. Often, statistics/estimators will be functions of means or averages, as we’ll see in the worked examples for this chapter!\nLikelihood Function\nLet \\(x_1, \\dots, x_n\\) be a sample of size \\(n\\) of independent observations from the probability density function \\(f_X(x \\mid \\boldsymbol{\\theta})\\), where \\(\\boldsymbol{\\theta}\\) is a set of unknown parameters that define the pdf. Then the likelihood function \\(L(\\boldsymbol{\\theta})\\) is the product of the pdf evaluated at each \\(x_i\\),\n\\[\nL(\\boldsymbol{\\theta}) = \\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta}).\n\\]\nNote that this looks exactly like the joint pdf for \\(n\\) independent random variables, but it is interpreted differently. A likelihood is a function of parameters, given a set of observations (random variables). A joint pdf is a function of random variables.\nNote: The likelihood function is one of the reasons why we like independent observations so much! If observations aren’t independent, we can’t simply multiply all of their pdfs together to get a likelihood function.\nMaximum Likelihood Estimate (MLE)\nLet \\(L(\\boldsymbol{\\theta}) = \\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta})\\) be the likelihood function corresponding to a random sample of observations \\(x_1, \\dots, x_n\\). If \\(\\boldsymbol{\\theta}_e\\) is such that \\(L(\\boldsymbol{\\theta}_e) \\geq L(\\boldsymbol{\\theta})\\) for all possible values \\(\\boldsymbol{\\theta}\\), then \\(\\boldsymbol{\\theta}_e\\) is called a maximum likelihood estimate for \\(\\boldsymbol{\\theta}\\).\nLog-likelihood\nIn statistics, when we say “log,” we essentially always mean “ln” (or, natural log). The log-likelihood is then, hopefully unsurprisingly, given by \\(\\log(L(\\boldsymbol{\\theta}))\\). One thing that’s useful to note (and will come in handy when calculating MLEs, is that the log of a product is equal to a sum of logs. For likelihoods, that means\n\\[\n\\log(L(\\boldsymbol{\\theta})) = \\log \\left(\\prod_{i = 1}^n f_X(x_i \\mid \\boldsymbol{\\theta})\\right) = \\sum_{i = 1}^n \\log(f_X(x_i \\mid \\boldsymbol{\\theta}))\n\\]\nThis will end up making it much easier to take derivatives than needing to deal with products!\nOrder Statistic\nThe \\(k\\)th order statistic is equal to a sample’s \\(k\\)th smallest value. Practically speaking, there are essentially three order statistics we typically care about: the minimum, the median, and the maximum. We denote the minimum (or, first order statistic) in a sample of random variables \\(X_1, \\dots, X_n\\) as \\(X_{(1)}\\) , the maximum as \\(X_{(n)}\\), and the median \\(X_{(m+1)}\\) where \\(n = 2m + 1\\) when \\(n\\) is odd. Note that median is in fact not an order statistic if \\(n\\) is even (since the median is an average of two values, \\(X_{(m)}\\) and \\(X_{(m+1)}\\), in this case.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#theorems",
    "href": "mle.html#theorems",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.4 Theorems",
    "text": "2.4 Theorems\nNone for this chapter!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mle.html#worked-examples",
    "href": "mle.html#worked-examples",
    "title": "2  Maximum Likelihood Estimation",
    "section": "2.5 Worked Examples",
    "text": "2.5 Worked Examples\nProblem 1: Suppose we observe \\(n\\) independent observations \\(X_1, \\dots, X_n \\sim Bernoulli(p)\\), where \\(f_X(x) = p^x(1-p)^{1-x}\\). Find the MLE of \\(p\\).\n\n\nSolution:\n\nWe can write the likelihood function as\n\\[\nL(p) = \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i}\n\\]\nThen the log-likelihood is given by\n\\[\\begin{align*}\n\\log(L(p)) & = \\log \\left[ \\prod_{i = 1}^n p^{x_i} (1-p)^{1 - x_i} \\right] \\\\\n& = \\sum_{i = 1}^n \\log \\left[p^{x_i} (1-p)^{1 - x_i} \\right] \\\\\n& = \\sum_{i = 1}^n \\left[ \\log(p^{x_i}) + \\log((1-p)^{1-x_i}) \\right] \\\\\n& = \\sum_{i = 1}^n \\left[ x_i \\log(p) + (1 - x_i) \\log(1-p) \\right] \\\\\n& = \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p) \\sum_{i = 1}^n (1 - x_i)  \\\\\n& = \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p)  (n - \\sum_{i = 1}^n x_i)\n\\end{align*}\\]\nWe can take the derivative of the log-likelihood with respect to \\(p\\), and set it equal to zero…\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial p} \\log(L(p)) & = \\frac{\\partial}{\\partial p} \\left[ \\log(p)\\sum_{i = 1}^n x_i  + \\log(1-p)  (n - \\sum_{i = 1}^n x_i) \\right] \\\\\n& = \\frac{\\sum_{i = 1}^n x_i }{p} - \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n0 & \\equiv \\frac{\\sum_{i = 1}^n x_i }{p} - \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n\\frac{\\sum_{i = 1}^n x_i }{p}  & = \\frac{n - \\sum_{i = 1}^n x_i}{1-p} \\\\\n(1-p) \\sum_{i = 1}^n x_i & = p (n - \\sum_{i = 1}^n x_i) \\\\\n\\sum_{i = 1}^n x_i - p\\sum_{i = 1}^n x_i & = pn - p \\sum_{i = 1}^n x_i \\\\\n\\sum_{i = 1}^n x_i & = pn \\\\\n\\frac{1}{n} \\sum_{i = 1}^n x_i & = p\n\\end{align*}\\]\nand by solving for \\(p\\), we get that the MLE of \\(p\\) is equal to \\(\\frac{1}{n}\\sum_{i = 1}^n x_i\\). We will often see that the MLEs of parameters are functions of sample averages (in this case, just the identity function!).\n\nProblem 2: Suppose \\(X_1, X_2, \\dots, X_n\\) are a random sample from the Normal pdf with parameters \\(\\mu\\) and \\(\\sigma^2\\):\n\\[\nf_X(x ; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2},\n\\]\nfor \\(-\\infty &lt; x &lt; \\infty, \\ -\\infty &lt; \\mu &lt; \\infty,\\) and \\(\\sigma^2 &gt; 0\\). Find the MLEs of \\(\\mu\\) and \\(\\sigma^2\\). (Note that this is Question 5 on the MLE section of Problem Set 1! For your HW, try your best to do this problem from scratch, without looking at the course notes!)\n\n\nSolution:\n\nSince we are dealing with a likelihood with two parameters, we’ll need to solve a system of equations to obtain the MLEs for \\(\\mu\\) and \\(\\sigma^2\\).\n\\[\\begin{align*}\n    \\log(L(\\mu, \\sigma^2)) & = \\log( \\prod_{i = 1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x_i - \\mu)^2) ) \\\\\n    & = \\sum_{i = 1}^n \\left[ \\log(\\frac{1}{\\sqrt{2\\pi \\sigma^2}})  - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ -\\frac{1}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right] \\\\\n    & = \\frac{-n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i = 1}^n (x_i - \\mu)^2\n\\end{align*}\\]\nNow we need to find \\(\\frac{\\partial}{\\partial \\sigma^2}\\log(L(\\mu, \\sigma^2))\\) and \\(\\frac{\\partial}{\\partial \\mu}\\log(L(\\mu, \\sigma^2))\\). Let’s make our lives a little bit easier by setting \\(\\sigma^2 \\equiv \\theta\\) (so we don’t trip ourselves up with the exponent). We get\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\theta}\\log(L(\\mu, \\theta)) & = \\frac{\\partial}{\\partial \\theta} \\left(\\frac{-n}{2} \\log(2\\pi \\theta) - \\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i - \\mu)^2 \\right)\\\\\n    & = \\frac{-2\\pi n}{4 \\pi \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2}\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\mu}\\log(L(\\mu, \\theta)) & = \\frac{\\partial}{\\partial \\mu} \\left(\\frac{-n}{2} \\log(2\\pi \\theta) - \\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i - \\mu)^2 \\right)\\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} \\sum_{i = 1}^n (x_i^2 - 2 \\mu x_i + \\mu^2)\\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} ( \\sum_{i = 1}^n x_i^2 - 2 \\mu \\sum_{i = 1}^n x_i + n\\mu^2 )\\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{1}{2\\theta} (- 2 \\mu \\sum_{i = 1}^n x_i + n\\mu^2 ) \\right) \\\\\n    & = \\frac{\\partial}{\\partial \\mu} \\left(   \\frac{\\sum_{i = 1}^n x_i}{\\theta} \\mu - \\frac{n}{2\\theta}\\mu^2  \\right) \\\\\n    & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu\n\\end{align*}\\]\nWe now have the following system of equations to solve:\n\\[\\begin{align*}\n    0 & \\equiv \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    0 & \\equiv \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu\n\\end{align*}\\]\nTypically, we solve one of the equations for one of the parameters, plug that into the other equation, and then go from there. We’ll start by solving the second equation for \\(\\mu\\).\n\\[\\begin{align*}\n    0 & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} - \\frac{n}{\\theta} \\mu \\\\\n    \\frac{n}{\\theta} \\mu & = \\frac{\\sum_{i = 1}^n x_i}{\\theta} \\\\\n    \\mu & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nWell that’s convenient! We already have the MLE for \\(\\mu\\) as being just the sample average. Plugging this into the first equation in our system we obtain\n\\[\\begin{align*}\n    0 & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\mu)^2 }{2 \\theta^2} \\\\\n    0 & = \\frac{-n}{2 \\theta} + \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{2 \\theta^2} \\\\\n    \\frac{n}{2 \\theta} & = \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{2 \\theta^2} \\\\\n    n & = \\frac{\\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2 }{\\theta} \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\frac{1}{n} \\sum_{i = 1}^n x_i )^2] \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x} )^2\n\\end{align*}\\]\nwhere \\(\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i\\). And so finally, we have that the MLE for \\(\\sigma^2\\) is given by \\(\\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x} )^2\\), and the MLE for \\(\\mu\\) is given by \\(\\bar{x}\\)!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Maximum Likelihood Estimation</span>"
    ]
  },
  {
    "objectID": "mom.html",
    "href": "mom.html",
    "title": "3  Method of Moments",
    "section": "",
    "text": "Why do we need more than one approach to obtain estimators?\nAt this point in the course, we’ve now seen one (hopefully intuitive) way to obtain estimators for unknown parameters in probability distributions: maximum likelihood estimation. An alternative approach to producing a “reasonable” estimator for an unknown parameter is called the “Method of Moments.” As the name implies, this method uses moments to derive estimators! Recall from probability theory that the \\(r\\)th moment of a probability distribution for \\(X\\) is given by \\(E[X^r]\\). We can make use of relationships that between theoretical moments and sample moments to derive reasonable estimators!\nIn general, the steps involved in obtaining a MOM estimator are:\nWe already have maximum likelihood estimation, and it seems reasonable, so why might we want another approach to obtaining estimates? A few reasons!\nOne is that estimators vary with regards their theoretical “properties” (as we’ll see in the following chapters). These properties are one way to define how “good” an estimator is, and we ideally want our estimators to be the best of the best.\nAnother reason why we might sometimes want another approach to obtaining estimators, quite frankly, is that maximum likelihood estimators are sometimes a pain to calculate. In some cases, there isn’t even a closed form solution for the parameter we’re trying to estimate. In these scenarios, we need numerical optimization in order to obtain maximum likelihood estimators. While numerical optimization isn’t the end of the world (it’s actually often quite easy to implement), it can be very computationally intensive for more complex likelihoods. In general, if we can obtain a closed form estimator analytically (via calculus/algebra, for example), we’ll be better off in the long run.* With the method of moments approach, it is often much easier to obtain a closed form estimator analytically.\n* This is mostly a function the fact that much statistics research focuses on developing new methods for solving problems and analyzing data (think: linear regression but fancier, linear regression but new somehow, etc.). Statistics is inherently practical. You (probably) want any methods that you develop to be practically usable by people who are perhaps not statisticians. No one is going to use your method if it takes an unreasonably long time to compute an estimator. Imagine how irritating it would be if it took your machine two days to compute linear regression coefficients in R, for example.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#learning-objectives",
    "href": "mom.html#learning-objectives",
    "title": "3  Method of Moments",
    "section": "3.1 Learning Objectives",
    "text": "3.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive method of moments estimators for parameters of common probability density functions\nExplain (in plain English) why method of moments estimation is an intuitive approach to estimating unknown parameters",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#concept-questions",
    "href": "mom.html#concept-questions",
    "title": "3  Method of Moments",
    "section": "3.2 Concept Questions",
    "text": "3.2 Concept Questions\n\nWhat is the intuition behind the method of moments (MOM) procedure for estimating unknown parameters?\nWhat are the typical steps to find a MOM estimator?\nWhat advantages does the MOM approach offer compared to MLE?\nDo the MOM and MLE approaches always yield the same estimate?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#definitions",
    "href": "mom.html#definitions",
    "title": "3  Method of Moments",
    "section": "3.3 Definitions",
    "text": "3.3 Definitions\nYou are expected to know the following definitions:\nTheoretical Moment\nThe \\(r^{th}\\) theoretical moment of a probability distribution is given by \\(E[X^r]\\). For example, when \\(r = 1\\), the \\(r^{th}\\) moment is just the expectation of the random variable \\(X\\).\nSample Moment\nThe \\(r^{th}\\) sample moment of a probability distribution is given by \\(\\frac{1}{n} \\sum_{i = 1}^n x_i^r\\), for a random sample of observations \\(x_1, \\dots, x_n\\).\nMethod of Moments Estimates\nLet \\(x_1, \\dots, x_n\\) be a random sample of observations from the pdf \\(f_X(x \\mid \\boldsymbol{\\theta})\\). The method of moments estimates are then the solutions to the set of \\(s\\) equations given by\n\\[\\begin{align*}\n  E[X] & = \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n  E[X^2] & = \\frac{1}{n} \\sum_{i = 1}^n x_i^2 \\\\\n  & \\vdots \\\\\n  E[X^s] & = \\frac{1}{n} \\sum_{i = 1}^n x_i^s\n\\end{align*}\\]\nIf our pdf depends on only a single unknown parameter, we only need to solve the first equation. If we have two unknown parameters, we need to solve the system of the first two equations. So on and so forth.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#theorems",
    "href": "mom.html#theorems",
    "title": "3  Method of Moments",
    "section": "3.4 Theorems",
    "text": "3.4 Theorems\nNone for this chapter!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "mom.html#worked-examples",
    "href": "mom.html#worked-examples",
    "title": "3  Method of Moments",
    "section": "3.5 Worked Examples",
    "text": "3.5 Worked Examples\nIn general (for these worked examples as well as the problem sets), I do not expect you to calculate theoretical moments by hand. We practiced that in the probability review chapter, and now we can use those known theoretical moments to make our lives easier.\nProblem 1: Suppose \\(X_1, \\dots, X_n \\sim Poisson(\\lambda)\\). Find the MLE of \\(\\lambda\\) and the MOM estimator of \\(\\lambda\\).\n\n\nSolution:\n\nTo obtain the MLE, note that we can write the likelihood as\n\\[\nL(\\lambda) = \\prod_{i = 1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\n\\]\nand the log-likelihood as\n\\[\n\\log(L(\\lambda)) = \\sum_{i = 1}^n \\left[ x_i\\log(\\lambda) - \\lambda - \\log(x_i!)\\right]\n\\]\nwhere I’ve used one “log rule” in the above to simplify: \\(\\log(a^b) = b \\times log(a)\\). Taking the derivative of the log-likelihood and setting it equal to zero, we obtain\n\\[\\begin{align*}\n  \\frac{\\partial}{\\partial \\lambda} \\log(L(\\lambda)) & = \\frac{1}{\\lambda} \\sum_{i = 1}^n x_i - n \\\\\n  0 & \\equiv \\frac{1}{\\lambda} \\sum_{i = 1}^n x_i - n \\\\\n  n & = \\frac{1}{\\lambda} \\sum_{i = 1}^n x_i \\\\\n  \\lambda & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nand so the MLE for \\(\\lambda\\) is the sample average. To obtain the MOM estimator for \\(\\lambda\\), first note that the pdf contains only one parameter. Therefore, we only need to set the first theoretical moment equal to the first sample moment, and solve. Note that the first theoretical moment of a Poisson distribution is \\(E[X] = \\lambda\\), and so equating this to the first sample moment, we obtain that the MOM estimator for \\(\\lambda\\) is again, just the sample average! Much “easier” to compute than the MLE, in this case.\n\nProblem 2: Suppose \\(X_1, \\dots, X_n \\sim Bernoulli(\\theta)\\). Find the MOM estimator for \\(\\theta\\).\n\n\nSolution:\n\nNote that our pdf contains only one parameter. Then we only need to solve a “system” of one equation. We have\n\\[\\begin{align*}\n  E[X] & = \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n  \\theta & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nand we’re done! The system is pretty easy to “solve” when the theoretical moment is exactly the parameter we’re interested in.\n\nProblem 3: Suppose \\(Y_1, \\dots, Y_n \\sim Uniform(0, \\theta)\\). Find the MOM estimator for \\(\\theta\\).\n\n\nSolution:\n\nNote that our pdf contains only one parameter. Then we only need to solve a “system” of one equation. We have\n\\[\\begin{align*}\n  E[Y] & = \\frac{1}{n} \\sum_{i = 1}^n y_i \\\\\n  \\frac{\\theta}{2} & = \\frac{1}{n} \\sum_{i = 1}^n y_i \\\\\n  \\theta & = 2 \\bar{y}\n\\end{align*}\\]\nAnd so the MOM estimator for \\(\\theta\\) is 2 times the sample mean. Note that this is an example where the MOM estimator and MLE are not the same (you derived the MLE on your first problem set)!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method of Moments</span>"
    ]
  },
  {
    "objectID": "properties.html",
    "href": "properties.html",
    "title": "4  Properties of Estimators",
    "section": "",
    "text": "The Bias-Variance Trade-off\nNow that we’ve developed the tools for deriving estimators of unknown parameters, we can start thinking about different metrics for determining how “good” our estimators actually are. In general, we like our estimators to be:\nIf you are familiar with machine learning techniques or models for prediction purposes more generally (as opposed to inference), you may have stumbled upon the phrase “bias-variance trade-off.” In scenarios where we want to make good predictions for new observations using a statistical model, one way to measure how “well” our model is predicting new observations is through minimizing mean squared error. Intuitively, this is something we should want to minimize: “errors” (the difference between a predicted value and an observed value) are bad, we square them because the direction of the error (above or below) shouldn’t matter too much, and average over them because we need a summary measure of all our errors combined, and an average seems reasonable. In statistical terms, mean squared error has a very specific definition (see below) as the expected value of what is sometimes called a loss function (where in this case, loss is defined as squared error loss). We’ll return to this in the decision theory chapter of our course notes.\nIt just so happens that we can decompose mean squared error into a sum of two terms: the variance of our estimator + the bias of our estimator (squared). What this means for us is that two estimators may have the exact same MSE, but very different variances or biases (potentially). In general, if we hold MSE constant and imagine increasing the variance of our estimator, the bias would need to decrease accordingly to maintain the same MSE. This is where the “trade-off” comes from. MSE is an incredibly commonly used metric for assessing prediction models, but as we will see, doesn’t necessarily paint a full picture in terms of how “good” an estimator is. Smaller MSE does not automatically imply “better estimator,” just as smaller bias (in some cases) does not automatically imply “better estimator.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "properties.html#learning-objectives",
    "href": "properties.html#learning-objectives",
    "title": "4  Properties of Estimators",
    "section": "4.1 Learning Objectives",
    "text": "4.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nCalculate bias and variance of various estimators for unknown parameters\nExplain the distinction between bias and variance colloquially in terms of precision and accuracy, and why these properties are important\nCompare estimators in terms of their relative efficiency\nJustify why there exists a bias-variance trade-off, and explain what consequences this may have when comparing estimators",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "properties.html#concept-questions",
    "href": "properties.html#concept-questions",
    "title": "4  Properties of Estimators",
    "section": "4.2 Concept Questions",
    "text": "4.2 Concept Questions\n\nIntuitively, what is the difference between bias and precision?\nWhat are the typical steps to checking if an estimator is unbiased?\nHow can we construct unbiased estimators?\nIf an estimator is unbiased, is it also asymptotically unbiased? If an estimator is asymptotically unbiased, is it necessarily unbiased?\nWhen comparing estimators, how can we determine which estimator is more efficient?\nWhy might we care about sufficiency, particularly when thinking about the variance of unbiased estimators?\nDescribe, in your own words, what the Cramér-Rao inequality tells us.\nWhat is the difference between a UMVUE and an efficient estimator? Does one imply the other?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "properties.html#definitions",
    "href": "properties.html#definitions",
    "title": "4  Properties of Estimators",
    "section": "4.3 Definitions",
    "text": "4.3 Definitions\nYou are expected to know the following definitions:\nUnbiased\nAn estimator \\(\\hat{\\theta} = g(X_1, \\dots, X_n)\\) is an unbiased estimator for \\(\\theta\\) if \\(E[\\hat{\\theta}] = \\theta\\), for all \\(\\theta\\).\nAsymptotically Unbiased\nAn estimator \\(\\hat{\\theta} = g(X_1, \\dots, X_n)\\) is an asymptotically unbiased estimator for \\(\\theta\\) if \\(\\underset{n \\to \\infty}{\\text{lim}} E[\\hat{\\theta}] = \\theta\\).\nPrecision\nThe precision of a random variable \\(X\\) is given by \\(\\frac{1}{Var(X)}\\).\nMean Squared Error (MSE)\nThe mean squared error of an estimator is given by\n\\[\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + Bias(\\hat{\\theta})^2\n\\]\nSufficient\nFor some function \\(T\\), \\(T(X)\\) is a sufficient statistic for an unknown parameter \\(\\theta\\) if the conditional distribution of \\(X\\) given \\(T(X)\\) does not depend on \\(\\theta\\). A “looser” definition is that the distribution of \\(X\\) must depend on \\(\\theta\\) only through \\(T(X)\\).\nMinimal Sufficiency\nFor some function \\(T^*\\), \\(T^*(X)\\) is a minimal sufficient statistic for an unknown parameter \\(\\theta\\) if \\(T^*(X)\\) is sufficient, and for every other sufficient statistic \\(T(X)\\). \\(T^*(X) = f(T(X))\\) for some function \\(f\\).\nComplete\nA statistic \\(T(X)\\) is complete for an unknown parameter \\(\\theta\\) if \\[\nE[g(T(x))] \\text{ is } \\theta-\\text{free} \\implies g(T(x)) \\text{ is constant, almost everywhere}\n\\] for a nice function \\(g\\).\nImportantly, it is equivalent to say that \\(T(X)\\) is complete for an unknown parameter \\(\\theta\\) if\n\\[\nE[g(T(x))] = 0 \\implies g(T(x)) = 0 \\quad\\text{ almost everywhere}\n\\]\nRelative Efficiency\nThe relative efficiency of an estimator \\(\\hat{\\theta}_1\\) with respect to an estimator \\(\\hat{\\theta}_2\\) is the ratio \\(Var(\\hat{\\theta}_2)/Var(\\hat{\\theta}_1)\\).\nUniformly Minimum-Variance Unbiased Estimator (UMVUE)\nAn estimator \\(\\hat{\\theta}^*\\) is the UMVUE if, for all estimators \\(\\hat{\\theta}\\) in the class of unbiased estimators \\(\\Theta\\),\n\\[\nVar(\\hat{\\theta}^*) \\leq Var(\\hat{\\theta})\n\\]\nScore\nThe score is defined as the first partial derivative with respect to \\(\\theta\\) of the log-likelihood function, given by\n\\[\n\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x)\n\\]\nInformation Matrix\nThe information matrix* \\(I(\\theta)\\) for a collection of iid random variables \\(X_1, \\dots, X_n\\) is the variance of the score, given by\n\\[\nI(\\theta) = E \\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x) \\right)^2\\right] = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x)\\right]\n\\]\nNote that the above formula is in fact the variance of the score, since we can show that the expectation of the score is 0 (under some regularity conditions). This is shown as part of the proof of the C-R lower bound in the Theorems section of this chapter.\nThe information matrix is sometimes written in terms of a pdf for a single random variable as opposed to a likelihood. In this case, we have \\(I(\\theta) = n I_1(\\theta)\\), where the \\(I_1(\\theta)\\) on the right-hand side is defined as \\(E \\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log f_X(x \\mid \\theta) \\right)^2\\right]\\). Sometimes \\(I_1(\\theta)\\) is written without the subscript \\(1\\) which is a slight abuse of notation that is endlessly confusing (to me, at least). For this set of course notes, we’ll always specify the information matrix in terms of a pdf for a single random variable with the subscript \\(1\\), for clarity.\n*The information matrix is often referred to as the Fisher Information matrix, as it was developed by Sir Ronald Fisher. Fisher developed much of the core, statistical theory that we use today. He was also the founding chairman of the University of Cambridge Eugenics Society, and contributed to a large body of scientific work and public policy that promoted racist and classist ideals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "properties.html#theorems",
    "href": "properties.html#theorems",
    "title": "4  Properties of Estimators",
    "section": "4.4 Theorems",
    "text": "4.4 Theorems\nCovariance Inequality (based on the Cauchy-Schwarz inequality)\nLet \\(X\\) and \\(Y\\) be random variables. Then,\n\\[\nVar(X) \\geq \\frac{Cov(X, Y)^2}{Var(Y)}\n\\]\nThe proof is quite clear on Wikipedia.\nThe Factorization Criterion for sufficiency\nConsider a pdf for a random variable \\(X\\) that depends on an unknown parameter \\(\\theta\\), given by \\(\\pi(x \\mid \\theta)\\). The statistic \\(T(x)\\) is sufficient for \\(\\theta\\) if and only if \\(\\pi(x \\mid \\theta)\\) factors as:\n\\[\n\\pi(x \\mid \\theta) = g(T(x) \\mid \\theta) h(x)\n\\] where \\(g(T(x) \\mid \\theta)\\) depends on \\(x\\) only through \\(T(x)\\), and \\(h(x)\\) does not depend on \\(\\theta\\).\nNote that in the statistics literature this criterion is sometimes referred to as the Fisher-Neyman Factorization Criterion.\nTwo proofs available on Wikipedia. The one for the discrete-only case is more intuitive, if you’d like to look through one of them.\nLehmann-Scheffe Theorem\nSuppose that a random variable \\(X\\) has pdf given by \\(f(x \\mid \\theta)\\), and that \\(T^*(X)\\) is such that for every* pair of points \\((x,y)\\), the ratio of pdfs\n\\[\n\\frac{f(y \\mid \\theta)}{f(x \\mid \\theta)}\n\\] does not depend on \\(\\theta\\) if and only if \\(T^*(x) = T^*(y)\\). Then \\(T^*(X)\\) is a minimal sufficient statistic for \\(\\theta\\).\n*every pair of points that have the same support as \\(X\\).\n\n\nProof.\n\nWe’ll utilize something called a likelihood ratio (literally a ratio of likelihoods) to prove this theorem. We’ll also come back to likelihood ratios later in the Hypothesis Testing chapter!\nLet \\(\\theta_1\\) and \\(\\theta_2\\) be two possible values of our unknown parameter \\(\\theta\\). Then a likelihood ratio comparing densities evaluated at these two values is defined as\n\\[\nL_{\\theta_1, \\theta_2}(x) \\equiv \\frac{f(x \\mid \\theta_2)}{f(x \\mid \\theta_1)}\n\\] Our proof will proceed as follows:\n\nWe’ll show that if \\(T(X)\\) is sufficient, then \\(L_{\\theta_1, \\theta_2}(X)\\) is a function of \\(T(X)\\) \\(\\forall\\) \\(\\theta_1, \\theta_2\\).\nWe’ll show the converse: If \\(L_{\\theta_1, \\theta_2}(X)\\) is a function of \\(T(X)\\) \\(\\forall\\) \\(\\theta_1, \\theta_2\\), then \\(T(X)\\) is sufficient. This combined with (1) will show that \\(L_{\\theta_1, \\theta_2}(X)\\) is a minimal sufficient statistic.\nWe’ll use the above two statements to prove the theorem!\n\nFirst, suppose that \\(T(X)\\) is sufficient for \\(\\theta\\). Then, by definition we can write\n\\[\nL_{\\theta_1, \\theta_2}(x) = \\frac{f(x \\mid \\theta_2)}{f(x \\mid \\theta_1)} = \\frac{g(T(x) \\mid \\theta_1)h(x)}{g(T(x) \\mid \\theta_2)h(x)} = \\frac{g(T(x) \\mid \\theta_1)}{g(T(x) \\mid \\theta_2)}\n\\] and so \\(L_{\\theta_1, \\theta_2}(X)\\) is a function of \\(T(x)\\) \\(\\forall\\) \\(\\theta_1, \\theta_2\\).\nSecond, assume WLOG that \\(\\theta_1\\) is fixed, and denote our unknown parameter \\(\\theta_2 = \\theta\\). We can rearrange our likelihood ratio as\n\\[\\begin{align*}\n    L_{\\theta_1, \\theta}(x) & = \\frac{f(x \\mid \\theta)}{f(x \\mid \\theta_1)} \\\\\n    f(x \\mid \\theta) & = L_{\\theta_1, \\theta}(x) f(x \\mid \\theta_1)\n\\end{align*}\\]\nand note that \\(L_{\\theta_1, \\theta}(x)\\) is a function of \\(T(X)\\) by assumption, and \\(f(x \\mid \\theta_1)\\) is a function of \\(x\\) that does not depend on our unknown parameter \\(\\theta\\). Then \\(T(X)\\) satisfies the factorization criterion, and is therefore sufficient.\nLet \\(T^{**}(X) \\equiv L_{\\theta_1, \\theta_2}(X)\\). Then the first two statements we have shown give us that\n\\[\nT(X) \\text{ is sufficient } \\iff T^{**}(X) \\text{ is a function of } T(X)\n\\]\nand therefore \\(T^{**}(X)\\) is a minimal sufficient statistic, by definition.\nWe’ll now (officially) prove our theorem. By hypothesis of the theorem,\n\\[\\begin{align*}\n    T^*(x) = T^*(y) & \\iff \\frac{f(y \\mid \\theta)}{f(x \\mid \\theta)} \\text{ is } \\theta-free \\\\\n    & \\iff \\frac{f(y \\mid \\theta_1)}{f(x \\mid \\theta_1)} = \\frac{f(y \\mid \\theta_2)}{f(x \\mid \\theta_2)} \\quad \\forall \\theta_1, \\theta_2 \\\\\n    & \\iff \\frac{f(y \\mid \\theta_2)}{f(y \\mid \\theta_1)} = \\frac{f(x \\mid \\theta_2)}{f(x \\mid \\theta_1)} \\quad \\forall \\theta_1, \\theta_2 \\\\\n    & \\iff L_{\\theta_1, \\theta_2}(y) = L_{\\theta_1, \\theta_2} (x) \\quad \\forall \\theta_1, \\theta_2 \\\\\n    & \\iff T^{**}(y) = T^{**}(x)\n\\end{align*}\\]\nTherefore \\(T^*(X)\\) and \\(T^{**}(X)\\) are equivalent. Since \\(T^{**}(X)\\) is a minimal sufficient statistic, \\(T^*(X)\\) is therefore also minimal sufficient.\n\nComplete, Sufficient, Minimal\nIf \\(T(X)\\) is complete and sufficient, then \\(T(X)\\) is minimal sufficient.\n\n\nProof.\n\nJust kidding! Prove it on your own and show it to me, if you want bonus points in my heart :)\n\nRao-Blackwell-Lehmann-Scheffe (RBLS)\nLet \\(T(X)\\) be a complete and sufficient statistic for unknown parameter \\(\\theta\\), and let \\(\\tau(\\theta)\\) be some function of \\(\\theta\\). If there exists at least one unbiased estimator \\(\\tilde{\\tau}(X)\\) for \\(\\tau(\\theta)\\), then there exists a unique UMVUE \\(\\hat{\\tau}(T(X))\\) for \\(\\tau(\\theta)\\) given by\n\\[\n\\hat{\\tau}(T(X)) = E[\\tilde{\\tau}(X) \\mid T(X)]\n\\]\nWhy do we care? An important consequence of the RBLS Theorem is that if \\(T(X)\\) is a complete and sufficient statistic for \\(\\theta\\), then any function \\(\\phi(T(X))\\) is the UMVUE of its expectation \\(E[\\phi(T(X))]\\) (so long as the expectation is finite for all \\(\\theta\\)). This Theorem is therefore a very convenient way to find UMVUEs: (1) Find a complete and sufficient statistic for an unknown parameter, and (2) functions of that statistic are then the UMVUE for their expectation!\n\n\nProof.\n\nTo prove RBLS, we first must prove an Improvement Lemma and a Uniqueness Lemma.\nImprovement Lemma. Suppose that \\(T(X)\\) is a sufficient statistic for \\(\\theta\\). If \\(\\tilde{\\tau}(X)\\) is an unbiased estimator of \\(\\tau(\\theta)\\), then \\(E[\\tilde{\\tau}(X) \\mid T(X)]\\) does not depend on \\(\\theta\\) (by sufficiency) and is also an estimator of \\(\\tau(\\theta)\\), which (importantly) has smaller variance than \\(\\tilde{\\tau}(X)\\).\nProof of Lemma. First, note that \\(E[\\tilde{\\tau}(X) \\mid T(X)]\\) is an unbiased estimator for \\(\\tau(\\theta)\\), since \\[\\begin{align*}\n    E[E[\\tilde{\\tau}(X) \\mid T(X)]] & = E[\\tilde{\\tau}(X)] \\quad \\quad \\text{(Law of Iterated Expectation)} \\\\\n    & = \\tau(\\theta) \\quad \\quad (\\tilde{\\tau}(X) \\text{ is unbiased})\n\\end{align*}\\] Then, \\[\\begin{align*}\n    Var(\\tilde{\\tau}(X)) & = E[Var(\\tilde{\\tau}(X) \\mid T(X))] + Var(E[\\tilde{\\tau}(X) \\mid T(X)]) \\\\\n    & \\geq Var(E[\\tilde{\\tau}(X) \\mid T(X)])\n\\end{align*}\\] and we’re done! \\(E[\\tilde{\\tau}(X) \\mid T(X)]\\) has a smaller variance than \\(\\tilde{\\tau(X)}\\). Since both are unbiased, this is considered an “improvement” (hence the name of the Lemma).\nUniqueness Lemma. If \\(T(X)\\) is complete, then for some unknown parameter \\(\\theta\\) and function of it \\(\\tau(\\theta)\\), \\(\\tau(\\theta)\\) has at most one unbiased estimator \\(\\hat{\\tau}(T(X))\\) that depends on \\(T(X)\\).\nProof of Lemma. Suppose, toward contradiction, that \\(\\tau(\\theta)\\) has more than one unbiased estimator that depends on \\(T(X)\\), given by \\(\\tilde{\\tau}(T(X))\\) and \\(\\hat{\\tau}(T(X))\\), \\(\\tilde{\\tau}(T(X)) \\neq \\hat{\\tau}(T(X))\\). Then\n\\[\nE[\\tilde{\\tau}(T(X)) - \\hat{\\tau}(T(X))] = \\tau(\\theta) - \\tau(\\theta) = 0 \\quad \\forall \\theta\n\\] Let \\(g(T(X)) = \\tilde{\\tau}(T(X)) - \\hat{\\tau}(T(X))\\). Since \\(T(X)\\) is complete, and \\(E[g(T(X))] = 0\\), this implies \\(\\tilde{\\tau}(T(X)) - \\hat{\\tau}(T(X)) = 0\\), which means \\(\\tilde{\\tau}(T(X)) = \\hat{\\tau}(T(X))\\). Contradiction.\nBack to the proof of RBLS.\nWe’ve shown previously that \\(\\hat{\\tau}(T(X))\\) is an unbiased estimator for \\(\\tau{\\theta}\\) (law of iterated expectation). Let \\(\\tau_1(X)\\) be any other unbiased estimator for \\(\\tau(\\theta)\\), and let \\(\\tau_2(T(X)) = E[\\tau_1(X) \\mid T(X)]\\). Then \\(\\tau_2(T(X))\\) is also unbiased for \\(\\tau(\\theta)\\) (again, iterated expectation), and by the Uniqueness Lemma (since \\(T\\) is complete by supposition), \\(\\hat{\\tau}(T(X)) = \\tau_2(T(X))\\). But,\n\\[\\begin{align*}\n    Var(\\hat{\\tau}(T(X))) & = Var(\\tau_2(T(X))) \\quad \\quad (\\hat{\\tau} = \\tau_2) \\\\\n    & \\leq Var(\\tau_1(T(X))) \\quad \\quad \\text{(Improvement Lemma)}\n\\end{align*}\\] so \\(\\hat{\\tau}(T(X))\\) is the UMVUE for \\(\\tau(\\theta)\\), as desired.\n\nCramér-Rao Lower Bound\nLet \\(f_Y(y \\mid \\theta)\\) be a pdf with nice* conditions, and let \\(Y_1, \\dots, Y_n\\) be a random sample from \\(f_Y(y \\mid \\theta)\\). Let \\(\\hat{\\theta}\\) be any unbiased estimator of \\(\\theta\\). Then\n\\[\\begin{align*}\nVar(\\hat{\\theta}) & \\geq \\left\\{ E\\left[ \\left( \\frac{\\partial \\log( L(\\theta \\mid y))}{\\partial \\theta}\\right)^2\\right]\\right\\}^{-1} \\\\\n& = -\\left\\{ E\\left[ \\frac{\\partial^2 \\log( L(\\theta \\mid y))}{\\partial \\theta^2} \\right] \\right\\}^{-1} \\\\\n& = \\frac{1}{I(\\theta)}\n\\end{align*}\\]\n*our nice conditions that we need are that \\(f_Y(y \\mid \\theta)\\) has continuous first- and second-order derivatives, which would quickly discover we need by looking at the form for the C-R lower bound, and that the set of values \\(y\\) where \\(f_Y(y \\mid \\theta) \\neq 0\\) does not depend on \\(\\theta\\). If you are familiar with the concept of the “support” of a function, that is where this second condition comes from. The key here is that this condition allows to interchange derivatives and integrals, in particular, \\(\\frac{\\partial}{\\partial \\theta} \\int f(x) dx = \\int \\frac{\\partial}{\\partial \\theta} f(x)dx\\), which we’ll need to complete the proof.\n\n\nProof.\n\nLet \\(X = \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta}\\). By the Covariance Inequality,\n\\[\nVar(\\hat{\\theta}) \\geq \\frac{Cov(\\hat{\\theta},X)^2}{Var(X)}\n\\]\nand so if we can show\n\\[\\begin{align*}\n\\frac{Cov(\\hat{\\theta},X)^2}{Var(X)} & = \\left\\{ E\\left[ \\left( \\frac{\\partial \\log( L(\\theta \\mid \\textbf{y}))}{\\partial \\theta}\\right)^2\\right]\\right\\}^{-1}  \\\\\n& = \\frac{1}{I(\\theta)}\n\\end{align*}\\]\nthen we’re done, as this is the C-R lower bound. Note first that\n\\[\\begin{align*}\nE[X] & = \\int x f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right)  f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\left( \\frac{\\partial \\log f_Y(\\textbf{y} \\mid \\theta)}{\\partial \\theta} \\right)  f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\frac{\\frac{\\partial}{\\partial \\theta} f_Y(\\textbf{y} \\mid \\theta)}{ f_Y(\\textbf{y} \\mid \\theta)} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\int \\frac{\\partial}{\\partial \\theta} f_Y (\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\frac{\\partial}{\\partial \\theta} \\int f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n& = \\frac{\\partial}{\\partial \\theta} 1 \\\\\n& = 0\n\\end{align*}\\]\nThis means that\n\\[\\begin{align*}\n    Var[X] & = E[X^2] - E[X]^2 \\\\\n    & = E[X^2] \\\\\n    & = E \\left[ \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right)^2\\right ]\n\\end{align*}\\]\nand\n\\[\\begin{align*}\n    Cov(\\hat{\\theta}, X) & = E[\\hat{\\theta} X] - E[\\hat{\\theta}] E[X] \\\\\n    & = E[\\hat{\\theta}X] \\\\\n    & = \\int \\hat{\\theta} x f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right) f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\left( \\frac{\\partial \\log f_Y(\\textbf{y} \\mid \\theta)}{\\partial \\theta} \\right) f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\frac{\\frac{\\partial}{\\partial \\theta} f_Y(\\textbf{y} \\mid \\theta)}{ f_Y(\\textbf{y} \\mid \\theta)} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y} \\\\\n    & = \\int \\hat{\\theta} \\frac{\\partial}{\\partial \\theta} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y}  \\\\\n    & = \\frac{\\partial}{\\partial \\theta} \\int \\hat{\\theta} f_Y(\\textbf{y} \\mid \\theta) d\\textbf{y}  \\\\\n    & =  \\frac{\\partial}{\\partial \\theta} E[\\hat{\\theta}] \\\\\n    & = \\frac{\\partial}{\\partial \\theta} \\theta \\\\\n    & = 1\n\\end{align*}\\]\nwhere \\(E[\\hat{\\theta}] = \\theta\\) since our estimator is unbiased. Putting this all together, we have\n\\[\\begin{align*}\n    Var[\\hat{\\theta}] & \\geq \\frac{Cov(\\hat{\\theta},X)^2}{Var(X)} \\\\\n    & = \\frac{1^2}{E \\left[ \\left( \\frac{\\partial \\log L(\\theta \\mid \\textbf{y})}{\\partial \\theta} \\right)^2\\right ]} \\\\\n    & = \\frac{1}{I(\\theta)}\n\\end{align*}\\]\nas desired.\n\nComment: Note that what the Cramér-Rao lower bound tells us is that, if the variance of an unbiased estimator is equal to the Cramér-Rao lower bound, then that estimator has the minimum possible variance among all unbiased estimators there could possibly be. This allows us to prove, for example, whether or not an unbiased estimator is the UMVUE: If an unbiased estimator’s variance achieves the C-R lower bound, then it is optimal according to the UMVUE criterion.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "properties.html#worked-examples",
    "href": "properties.html#worked-examples",
    "title": "4  Properties of Estimators",
    "section": "4.5 Worked Examples",
    "text": "4.5 Worked Examples\nProblem 1: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Exponential(1/\\theta)\\). Compute the MLE of \\(\\theta\\), and show that it is an unbiased estimator of \\(\\theta\\).\n\n\nSolution:\n\nNote that we can write\n\\[\\begin{align*}\n    L(\\theta) & = \\prod_{i = 1}^n \\frac{1}{\\theta} e^{-x_i / \\theta} \\\\\n    \\log L(\\theta) & = \\sum_{i = 1}^n \\log(\\frac{1}{\\theta} e^{-x_i / \\theta}) \\\\\n    & = \\sum_{i = 1}^n  \\log(\\frac{1}{\\theta}) - \\sum_{i = 1}^n x_i / \\theta \\\\\n    & = -n \\log(\\theta) - \\frac{1}{\\theta} \\sum_{i = 1}^n x_i \\\\\n    \\frac{\\partial}{\\partial \\theta} \\log L(\\theta) & = \\frac{\\partial}{\\partial \\theta}  \\left( -n \\log(\\theta) - \\frac{1}{\\theta} \\sum_{i = 1}^n x_i \\right) \\\\\n    & = -\\frac{n}{\\theta} + \\frac{\\sum_{i = 1}^n x_i }{\\theta^2}\n\\end{align*}\\]\nSetting this equal to \\(0\\) and solving for \\(\\theta\\) we obtain\n\\[\\begin{align*}\n    0 & \\equiv -\\frac{n}{\\theta} + \\frac{\\sum_{i = 1}^n x_i }{\\theta^2}  \\\\\n    \\frac{n}{\\theta} & = \\frac{\\sum_{i = 1}^n x_i }{\\theta^2} \\\\\n    n & = \\frac{\\sum_{i = 1}^n x_i }{\\theta} \\\\\n    \\theta & = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\end{align*}\\]\nand so the MLE for \\(\\theta\\) is the sample mean. To show that the MLE is unbiased, we note that\n\\[\\begin{align*}\n    E \\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\right] & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i] = \\frac{1}{n} \\sum_{i = 1}^n \\theta  = \\theta\n\\end{align*}\\]\nas desired.\n\nProblem 2: Suppose again that \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Exponential(1/\\theta)\\). Let \\(\\hat{\\theta}_2 = Y_1\\), and \\(\\hat{\\theta}_3 = nY_{(1)}\\). Show that \\(\\hat{\\theta}_2\\) and \\(\\hat{\\theta}_3\\) are unbiased estimators of \\(\\theta\\). Hint: use the fact that \\(Y_{(1)} \\sim Exponential(n/\\theta)\\)\n\n\nSolution:\n\nNote that the mean of a random variable \\(Y \\sim Exponential(\\lambda)\\) is given by \\(1/\\lambda\\). Then we can write\n\\[\nE[\\hat{\\theta}_2] = E[Y_1] = \\frac{1}{1/\\theta} = \\theta\n\\]\nand\n\\[\nE[\\hat{\\theta}_3] = E[nY_{(1)}] = \\frac{n}{n/\\theta} = \\theta\n\\] as desired.\n\nProblem 3: Compare the variance of the estimators from Problems 1 and 2. Which is most efficient?\n\n\nSolution:\n\nRecall that the variance of a random variable \\(Y \\sim Exponential(\\lambda)\\) is given by \\(1/\\lambda^2\\). Let the MLE from Problem 1 be denoted \\(\\hat{\\theta}_1 = \\bar{X}\\). Then we can write\n\\[\nVar\\left[\\hat{\\theta}_1\\right] = Var\\left[\\frac{1}{n} \\sum_{i = 1}^n X_i\\right] = \\frac{1}{n^2} \\sum_{i = 1}^n Var[X_i] = \\frac{1}{n^2} \\left( \\frac{n}{(1/\\theta)^2} \\right) = \\frac{\\theta^2}{n}\n\\]\nand\n\\[\nVar\\left[\\hat{\\theta}_2\\right] = Var[Y_1] = \\frac{1}{(1/\\theta)^2} = \\theta^2\n\\]\nand\n\\[\nVar\\left[\\hat{\\theta}_3\\right] = Var[nY_{(1)}] = n^2 Var[Y_{(1)}] = \\frac{n^2}{(n/\\theta)^2} = \\theta^2\n\\]\nThus, the variance of the MLE, \\(\\hat{\\theta}_1\\), is most efficient, and is \\(n\\) times smaller than the variance of both \\(\\hat{\\theta}_2\\) and \\(\\hat{\\theta}_3\\).\n\nProblem 4: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Show that the estimator \\(\\hat{\\mu} = \\frac{1}{n} \\sum_{i = 1}^n X_i\\) and the estimator \\(\\hat{\\mu}_w = \\sum_{i = 1}^n w_i X_i\\) are both unbiased estimators of \\(\\mu\\), where \\(\\sum_{i = 1}^n w_i = 1\\).\n\n\nSolution:\n\nWe can write\n\\[\nE[\\hat{\\mu}] = E\\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\right] = \\frac{1}{n}\\sum_{i = 1}^n E[X_i] = \\frac{1}{n}\\sum_{i = 1}^n \\mu = \\mu\n\\]\nand\n\\[\nE[\\hat{\\mu}_w] = E \\left[ \\sum_{i = 1}^n w_i X_i \\right] = \\sum_{i = 1}^n w_i E \\left[ X_i \\right] = \\sum_{i = 1}^n w_i \\mu = \\mu \\sum_{i = 1}^n w_i = \\mu\n\\]\nas desired.\n\nProblem 5: Determine whether the estimator \\(\\hat{\\mu}\\) or \\(\\hat{\\mu}_w\\) is more efficient, in Problem 4, if we additionally impose the constraint \\(w_i \\geq 0\\) \\(\\forall i\\). (Hint: use the Cauchy-Schwarz inequality)\n\n\nSolution:\n\nTo determine relative efficiency, we must compute the variance of each estimator. We have\n\\[\nVar[\\hat{\\mu}] = Var \\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\right] = \\frac{1}{n^2} \\sum_{i = 1}^n Var[X_i] = \\frac{1}{n^2} \\sum_{i = 1}^n \\sigma^2 = \\sigma^2 / n\n\\]\nand\n\\[\\begin{align*}\n    Var[\\hat{\\mu}_w] & =  Var \\left[ \\sum_{i = 1}^n w_i X_i \\right] \\\\\n    & = \\sum_{i = 1}^n Var[w_i X_i] \\\\\n    & = \\sum_{i = 1}^n w_i^2 Var[X_i] \\\\\n    & = \\sum_{i = 1}^n w_i^2  \\sigma^2 \\\\\n    & = \\sigma^2 \\sum_{i = 1}^n w_i^2\n\\end{align*}\\]\nAnd so to determine which estimator is more efficient, we need to determine if \\(\\frac{1}{n}\\) is less than \\(\\sum_{i = 1}^n w_i^2\\) (or not). The Cauchy-Schwarz inequality tells us that\n\\[\\begin{align*}\n    \\left( \\sum_{i = 1}^n w_i \\cdot 1\\right)^2 & \\leq \\left( \\sum_{i = 1}^n w_i^2 \\right) \\left( \\sum_{i = 1}^n 1^2 \\right) \\\\\n    \\left( \\sum_{i = 1}^n w_i \\right)^2 & \\leq \\left( \\sum_{i = 1}^n w_i^2 \\right) n \\\\\n    1 & \\leq \\left( \\sum_{i = 1}^n w_i^2 \\right) n  \\\\\n    \\frac{1}{n} & \\leq \\sum_{i = 1}^n w_i^2\n\\end{align*}\\]\nand therefore, \\(\\hat{\\mu}\\) is more efficient than \\(\\hat{\\mu}_w\\).\n\nProblem 6: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Show that the MLE for \\(\\sigma^2\\) is biased, and suggest a modified variance estimator for \\(\\sigma^2\\) that is unbiased.\n\n\nSolution:\n\nRecall that the MLE for \\(\\sigma^2\\) is given by \\(\\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\). Then\n\\[\\begin{align*}\n    E\\left[ \\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\right] & = \\frac{1}{n} \\sum_{i = 1}^n E\\left[ (X_i - \\bar{X})^2\\right] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E\\left[ X_i^2 - 2X_i \\bar{X} + \\bar{X}^2\\right] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - 2 E\\left[ \\frac{1}{n} \\sum_{i = 1}^n X_i \\bar{X} \\right] + E[\\bar{X}^2] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - 2 E\\left[ \\bar{X} \\frac{1}{n} \\sum_{i = 1}^n X_i  \\right] + E[\\bar{X}^2] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - 2 E\\left[ \\bar{X}^2  \\right] + E[\\bar{X}^2] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - E\\left[ \\bar{X}^2  \\right]\n\\end{align*}\\]\nRecall that since \\(X_i \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\), \\(\\bar{X} \\sim N(\\mu, \\sigma^2/n)\\), and that we can write \\(Var[Y] + E[Y]^2 = E[Y^2]\\) (definition of variance). Then we can write\n\\[\\begin{align*}\n    E\\left[ \\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2 \\right] & = \\frac{1}{n} \\sum_{i = 1}^n E[X_i^2] - E\\left[ \\bar{X}^2  \\right] \\\\\n    & = \\frac{1}{n} \\sum_{i = 1}^n \\left( \\sigma^2 + \\mu^2 \\right) - \\left( \\frac{\\sigma^2}{n} + \\mu^2 \\right) \\\\\n    & = \\sigma^2 + \\mu^2 - \\frac{\\sigma^2}{n} - \\mu^2  \\\\\n    & = \\sigma^2 - \\frac{\\sigma^2}{n} \\\\\n    & = \\sigma^2 \\left( 1 - \\frac{1}{n} \\right) \\\\\n    & = \\sigma^2  \\left( \\frac{n-1}{n} \\right)\n\\end{align*}\\]\nTherefore, since \\(E[\\hat{\\sigma}^2_{MLE}] \\neq \\sigma^2\\), the MLE is unbiased. Note that\n\\[\\begin{align*}\n    E\\left[ \\left( \\frac{n}{n-1} \\right)\\frac{1}{n} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\right] & = \\left( \\frac{n}{n-1} \\right) \\left( \\frac{n-1}{n} \\right) \\sigma^2   \\\\\n    & = \\sigma^2\n\\end{align*}\\]\nand so the estimator \\(\\frac{1}{n-1} \\sum_{i = 1}^n (X_i - \\bar{X})^2\\) is an unbiased estimator for \\(\\sigma^2\\). This estimator is often called the “sample variance”, and is denoted by \\(S^2\\).\n\nProblem 7: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Uniform(0, \\theta)\\). Show that \\(X_{(n)}\\) is sufficient for \\(\\theta\\).\n\n\nSolution:\n\nWe can write the joint distribution of \\(X_1, \\dots, X_n\\) as\n\\[\\begin{align*}\n    f(x_1, \\dots, x_n \\mid \\theta) & = \\left( \\prod_{i = 1}^n \\frac{1}{\\theta} \\right) I \\{0 \\leq x_1, \\dots, x_n \\leq \\theta \\} \\\\\n    & = \\frac{1}{\\theta^n} I \\{0 \\leq x_{(1)}, \\dots, x_{(n)} \\leq \\theta \\} \\\\\n    & = \\frac{1}{\\theta^n} I \\{ x_{n} \\leq \\theta \\}\n\\end{align*}\\]\nsince if \\(x_{(n)} \\leq \\theta\\), all observations are less than \\(\\theta\\). Then \\(f(x_1, \\dots, x_n \\mid \\theta)\\) depends on \\(\\theta\\) only through \\(x_{(n)}\\), and there \\(x_{(n)}\\) is sufficient for \\(\\theta\\).\n\nProblem 8: Suppose that \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Gamma(\\alpha, \\beta)\\). Find a jointly sufficient statistic for \\((\\alpha, \\beta)\\).\n\n\nSolution:\n\nWe can write\n\\[\\begin{align*}\n    f(x_1, \\dots, x_n \\mid \\alpha, \\beta) & = \\prod_{i = 1}^n \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x_i^{\\alpha - 1} e^{-\\beta x_i} \\\\\n    & = \\left( \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\right)^n \\left( \\prod_{i = 1}^n x_i^{\\alpha - 1} \\right) e^{-\\beta \\sum_{i = 1}^n x_i} \\\\\n    & = \\left( \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\right)^n \\left( \\prod_{i = 1}^n x_i \\right)^{\\alpha - 1}  e^{-\\beta \\sum_{i = 1}^n x_i}\n\\end{align*}\\]\nand so \\(f(x_1, \\dots, x_n \\mid \\alpha, \\beta)\\) depends on \\((\\alpha, \\beta)\\) on \\(x_i\\) only through \\((\\prod_{i = 1}^n x_i, \\sum_{i = 1}^n x_i)\\), and therefore \\((\\prod_{i = 1}^n x_i, \\sum_{i = 1}^n x_i)\\) is sufficient for \\((\\alpha, \\beta)\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Properties of Estimators</span>"
    ]
  },
  {
    "objectID": "consistency.html",
    "href": "consistency.html",
    "title": "5  Consistency",
    "section": "",
    "text": "5.1 Learning Objectives\nThe properties of estimators that we’ve covered thus far (with the exception of asymptotic unbiasedness) have all been what are called finite sample properties. All we mean by this is that our sample size (\\(n\\)) is less than infinity (hence: finite). In practice of course, our sample size will always be less than infinity; however, it is still nice to know what happens to our estimators (in terms of bias, variance, etc.) as our sample size gets really really large. Much of Frequentist statistics relies on asymptotic (read: infinite sample size) theory to quantify our uncertainty via confidence intervals, for example.\nConsistency is one such asymptotic property that we care about. As opposed to asymptotic unbiasedness, consistency tells us something about the entire shape of a distribution, as opposed to just the center of our distribution. Prof. Brianna Heggeseth and Prof. Kelsey Grinde have a great visual for this below:\nThe idea here is that as our sample size gets large (as we move to the right on the x-axis), consistency tells us something about the entire distribution of our estimator being within some boundary. Asymptotic unbiasedness, on the other hand, only tells us about whether the center of our distribution (a single point!) lies where it “should” (at the truth).\nWhy do we care about consistency? Because we care about uncertainty! It would be really unfortunate if, in collecting more and more data, we didn’t get any more certain about the true parameter we’re trying to estimate. Intuitively, we want to be more confident (less uncertain) in our estimators when we have larger sample sizes. This is exactly what consistency is concerned with. How do we prove whether or not an estimator is consistent? (Typically) Chebyshev’s Inequality, which we state and prove below.\nBy the end of this chapter, you should be able to…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consistency</span>"
    ]
  },
  {
    "objectID": "consistency.html#learning-objectives",
    "href": "consistency.html#learning-objectives",
    "title": "5  Consistency",
    "section": "",
    "text": "Distinguish between finite sample properties and asymptotic properties of estimators\nProve (using Chebyshev’s inequality) whether or not an estimator is consistent",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consistency</span>"
    ]
  },
  {
    "objectID": "consistency.html#concept-questions",
    "href": "consistency.html#concept-questions",
    "title": "5  Consistency",
    "section": "5.2 Concept Questions",
    "text": "5.2 Concept Questions\n\nWhat is the distinction between a fixed sample property and an asymptotic property of an estimator?\nDescribe, in your own words, what it means for an estimator to be consistent.\nHow can we use Chebyshev’s inequality to show that an estimator is consistent?\nWhich of the estimation techniques we’ve seen so far yield consistent estimators?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consistency</span>"
    ]
  },
  {
    "objectID": "consistency.html#definitions",
    "href": "consistency.html#definitions",
    "title": "5  Consistency",
    "section": "5.3 Definitions",
    "text": "5.3 Definitions\nYou are expected to know the following definitions:\nAsymptotically Unbiased\nAn estimator \\(\\hat{\\theta} = g(X_1, \\dots, X_n)\\) is an asymptotically unbiased estimator for \\(\\theta\\) if \\(\\underset{n \\to \\infty}{\\text{lim}} E[\\hat{\\theta}] = \\theta\\).\nConsistent\nAn estimator \\(\\hat{\\theta}_n = h(X_1, \\dots, X_n)\\) is consistent for \\(\\theta\\) if it converges in probability to \\(\\theta\\). That is, for all \\(\\epsilon &gt; 0\\),\n\\[\n\\underset{n \\to \\infty}{\\text{lim}} \\Pr(| \\hat{\\theta}_n - \\theta | &lt; \\epsilon) = 1\n\\]\nNote that we write our estimator with a subscript \\(n\\) here to clarify that our estimator depends on our sample size. There is an alternative \\(\\epsilon-\\delta\\) definition of consistency, but we won’t focus on it for this course.\nWeak Law of Large Numbers\nFor independent and identically distributed random variables \\(X_1, \\dots, X_n\\) with finite expectation \\(\\mu &lt; \\infty\\),\n\\[\n\\underset{n \\to \\infty}{\\text{lim}} \\Pr(| \\overline{X} - \\mu | &lt; \\epsilon) = 1\n\\]\nAlternatively, we can write that as \\(n \\to \\infty\\), \\(\\overline{X} \\overset{p}{\\to} \\mu\\), where “\\(\\overset{p}{\\to}\\)” denotes convergence in probability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consistency</span>"
    ]
  },
  {
    "objectID": "consistency.html#theorems",
    "href": "consistency.html#theorems",
    "title": "5  Consistency",
    "section": "5.4 Theorems",
    "text": "5.4 Theorems\nChebyshev’s Inequality\nLet \\(W\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then for any \\(\\epsilon &gt; 0\\),\n\\[\n\\Pr(|W - \\mu| &lt; \\epsilon) \\geq 1 - \\frac{\\sigma^2}{\\epsilon^2},\n\\]\nor, equivalently,\n\\[\n\\Pr(|W - \\mu| \\geq \\epsilon) \\leq \\frac{\\sigma^2}{\\epsilon^2}.\n\\]\nProof.\nLet \\(\\epsilon &gt; 0\\). Then\n\\[\\begin{align*}\n\\sigma^2 & = \\text{Var}(W) \\\\\n& = \\int_{-\\infty}^\\infty (w-\\mu)^2 f_W(w)dw \\\\\n& = \\int_{-\\infty}^{\\mu - \\epsilon} (w-\\mu)^2 f_W(w)dw + \\int_{\\mu - \\epsilon}^{\\mu + \\epsilon} (w-\\mu)^2 f_W(w)dw + \\int_{\\mu + \\epsilon}^\\infty (w-\\mu)^2 f_W(w)dw \\\\\n& \\ge \\int_{-\\infty}^{\\mu - \\epsilon} (w-\\mu)^2 f_W(w)dw + 0  + \\int_{\\mu + \\epsilon}^\\infty (w-\\mu)^2 f_W(w)dw \\\\\n&= \\int_{|w-\\mu|\\ge \\epsilon} (w-\\mu)^2 f_W(w)dw \\\\\n& \\ge \\int_{|w-\\mu|\\ge \\epsilon} \\epsilon^2 f_W(w)dw \\\\\n& = \\epsilon^2 \\int_{|w-\\mu|\\ge \\epsilon} f_W(w)dw\\\\\n& = \\epsilon^2 P(|W-\\mu| \\ge \\epsilon)\n\\end{align*}\\]\nand rearranging yields\n\\[\\begin{align*}\n\\sigma^2 & \\geq \\epsilon^2 P(|W-\\mu| \\ge \\epsilon) \\\\\nP(|W-\\mu| \\geq \\epsilon) & \\leq \\frac{\\sigma^2}{\\epsilon^2}\n\\end{align*}\\]\nas desired.\nCorollary 1: If \\(\\hat{\\theta}_n\\) is an unbiased estimator for \\(\\theta\\) and \\(\\underset{n \\to \\infty}{\\text{lim}} Var(\\hat{\\theta}_n) = 0\\), then \\(\\hat{\\theta}_n\\) is consistent for \\(\\theta\\). (You’ll prove this corollary on a problem set!)\nCorollary 2: If \\(\\hat{\\theta}_n\\) is an asymptotically unbiased estimator for \\(\\theta\\) and \\(\\underset{n \\to \\infty}{\\text{lim}} Var(\\hat{\\theta}_n) = 0\\), then \\(\\hat{\\theta}_n\\) is consistent for \\(\\theta\\).\nNote that the second corollary is a bit stronger than the first one, in that the first corollary actually implies the second. If an estimator is unbiased, then it is certainly asymptotically unbiased as well.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consistency</span>"
    ]
  },
  {
    "objectID": "consistency.html#worked-examples",
    "href": "consistency.html#worked-examples",
    "title": "5  Consistency",
    "section": "5.5 Worked Examples",
    "text": "5.5 Worked Examples\nProblem 1: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\). Show that the MLE for \\(\\sigma^2\\) is asymptotically unbiased.\n\n\nSolution:\n\nThe MLE for \\(\\sigma^2\\) is given by \\(\\frac{1}{n} \\sum_{i = 1}^n (X_i - \\overline{X})^2\\) (see the MLE section of the course notes, worked example problem 2), and has expectation \\(\\left( \\frac{n-1}{n} \\right)\\sigma^2\\) (see the Properties section of the course notes, worked example problem 6). To show that this estimator is asymptotically unbiased, note that we have\n\\[\\begin{align*}\n    \\underset{n\\to \\infty}{\\text{lim}} E[\\hat{\\sigma^2}_{MLE}] & = \\underset{n\\to \\infty}{\\text{lim}} \\left( \\frac{n-1}{n} \\right) \\sigma^2 \\\\\n    & = \\left( 1 \\right) \\sigma^2 \\\\\n    & = \\sigma^2\n\\end{align*}\\]\nand therefore, the MLE for \\(\\sigma^2\\) is asymptotically unbiased.\n\nProblem 2: Suppose \\(Y_1, \\dots, Y_n \\overset{iid}{\\sim} Uniform(0, \\theta)\\), and recall that \\(\\hat{\\theta}_{MLE} = Y_{(n)}\\) with \\(f_{Y_{(n)}}(y \\mid \\theta) = \\frac{n}{\\theta^n} y^{n-1}\\), \\(0 \\leq y \\leq \\theta\\). Prove that \\(\\hat{\\theta}_{MLE}\\) is a consistent estimator for \\(\\theta\\).\n\n\nSolution:\n\nTo prove that \\(\\hat{\\theta}_{MLE}\\) is consistent, we must first show that \\(\\hat{\\theta}_{MLE}\\) is (either) unbiased or asymptotically unbiased, and then we must show that the variance of \\(\\hat{\\theta}_{MLE}\\) tends to zero as \\(n \\to \\infty\\). To begin, note that\n\\[\\begin{align*}\n    E\\left[\\hat{\\theta}_{MLE}\\right] & = \\int_{0}^\\theta y f_{Y_{(n)}} (y \\mid \\theta) dy \\\\\n    & = \\int_{0}^\\theta y \\left( \\frac{n}{\\theta^n} y^{n-1} \\right) dy \\\\\n    & = \\frac{n}{\\theta^n} \\int_0^\\theta y^n dy \\\\\n    & = \\frac{n}{\\theta^n} \\left( \\frac{y^{n + 1}}{n + 1} \\bigg|_0^\\theta \\right) \\\\\n    & = \\frac{n}{\\theta^n} \\left( \\frac{\\theta^{n + 1}}{n + 1} \\right) \\\\\n    & = \\left( \\frac{n}{n + 1} \\right) \\theta\n\\end{align*}\\]\nand so \\(\\hat{\\theta}_{MLE}\\) is biased. It is, however, asymptotically unbiased. Note that \\(\\left( \\frac{n}{n + 1} \\right) \\overset{n \\to \\infty}{\\to} 1\\), and therefore \\(E\\left[\\hat{\\theta}_{MLE}\\right] \\overset{n \\to \\infty}{\\to} \\theta\\).\nAll that’s left is to show that \\(Var\\left[\\hat{\\theta}_{MLE}\\right] \\overset{n \\to \\infty}{\\to} 0\\). We can write\n\\[\\begin{align*}\n        E\\left[Y_{(n)}^2\\right] & = \\int_0^\\theta y^2 \\frac{ny^{n-1}}{\\theta^n} dy \\\\\n        & = \\frac{n}{\\theta^n} \\int_0^\\theta y^{n + 1} dy \\\\\n        & = \\frac{n}{\\theta^n} \\left( \\frac{y^{n + 2}}{n + 2} \\bigg|_0^\\theta \\right) \\\\\n        & = \\frac{n}{\\theta^n} \\left( \\frac{\\theta^{n + 2}}{n + 2}\\right) \\\\\n        & = \\left( \\frac{n}{n + 2} \\right) \\theta^2\n    \\end{align*}\\]\nand therefore\n\\[\\begin{align*}\n    \\underset{n \\to \\infty}{\\text{lim}} Var\\left[\\hat{\\theta}_{MLE}\\right] & = \\underset{n \\to \\infty}{\\text{lim}} \\left[ E\\left[Y_{(n)}^2\\right] - E\\left[Y_{(n)}\\right]^2 \\right]\\\\\n    & = \\underset{n \\to \\infty}{\\text{lim}} \\left[ \\left( \\frac{n}{n + 2} \\right) \\theta^2 - \\left( \\frac{n}{n + 1} \\right)^2 \\theta^2 \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[ \\left( \\frac{n}{n + 2} \\right)  - \\left( \\frac{n}{n + 1} \\right)^2\\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n}{n + 2}   - \\frac{n^2}{(n + 1)^2} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n(n + 1)^2 - n^2 (n + 2)}{(n + 2)(n + 1)^2} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n(n^2 + 2n + 1) - n^3 -2n^2}{(n + 2)(n^2 + 2n + 1)} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n^3 + 2n^2 + n - n^3 -2n^2}{n^3 + 2n^2 + 2n^2 + 5n + 2} \\right] \\\\\n    & = \\theta^2 \\underset{n \\to \\infty}{\\text{lim}} \\left[  \\frac{n}{n^3 + 4n^2  + 5n + 2} \\right] \\\\\n    & = 0\n\\end{align*}\\]\nwhere the last term goes to zero because \\(\\frac{n}{n^3} \\to 0\\) as \\(n \\to \\infty\\). Therefore, \\(\\hat{\\theta}_{MLE}\\) is a consistent estimator for \\(\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Consistency</span>"
    ]
  },
  {
    "objectID": "asymptotics.html",
    "href": "asymptotics.html",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "",
    "text": "Confidence Intervals\nAsymptotic unbiasedness and consistency allow us to assess the behavior of estimators when sample sizes get large. Thus far, however, we’ve only discussed what happens to point estimates as \\(n\\) goes to infinity. Point estimates are great, but they don’t tell the whole story. In order to truly quantify uncertainty (which is arguably one of the main goals of statistics, if not the main goal), we need to be able to estimate a range of plausible values for our estimators: we need to be able to construct confidence intervals. This is made possible primarily by asymptotic normality, the Central Limit Theorem, and properties of the normal distribution.\nConfidence intervals are one of the most difficult concepts for a budding statistician to grasp, because they don’t have the intuitive, probabilistic definition we often want them to have (aka the probability that the truth lies within the interval). As with Frequentist statistics more generally, the definition of a confidence interval relies on the concept of repeated sampling from a population.\nA confidence interval either contains the true parameter, or it does not. There is no probability involved in that statement. Probability comes into play when considering that, under repeated sampling, if we construct confidence intervals each time we take a new sample and construct an estimator, a given percentage of those intervals will contain the true parameter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asymptotics & the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "asymptotics.html#learning-objectives",
    "href": "asymptotics.html#learning-objectives",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.1 Learning Objectives",
    "text": "6.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nExplain the usefulness of the Central Limit Theorem for Frequentist statistical theory\nManipulate asymptotic distributions to remove their dependence on unknown parameters using the delta-method and Slutsky’s theorem\n\n…and explain why such manipulation is important for confidence interval construction\n\nDerive confidence intervals for unknown parameters based on asymptotic or exact distributions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asymptotics & the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "asymptotics.html#concept-questions",
    "href": "asymptotics.html#concept-questions",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.2 Concept Questions",
    "text": "6.2 Concept Questions\n\nWhat feature of a confidence interval tells us about the precision of our estimator?\nWhy is “removing” unknown parameters from the asymptotic distribution of our estimators important when constructing confidence intervals?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asymptotics & the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "asymptotics.html#definitions",
    "href": "asymptotics.html#definitions",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.3 Definitions",
    "text": "6.3 Definitions\nAsymptotic Normality\nAn estimator \\(\\hat{\\theta}_n\\) is asymptotically normal if \\(\\hat{\\theta}_n\\) converges in distribution to a normally distributed random variable.\nAsymptotic Efficiency\nAn estimator \\(\\hat{\\theta}_n\\) is asymptotically efficient if it’s asymptotic variance attains the C-R Lower Bound. Note that this is the C-R Lower Bound for a single observation, and therefore the asymptotic distribution of an MLE looks something like this:\n\\[\n\\sqrt{n} (\\hat{\\theta}_n - \\theta) \\overset{d}{\\to} N\\left( 0, \\frac{1}{I_1(\\theta)}\\right)\n\\]\nConfidence Interval\nA 100(1 - \\(\\alpha\\))% confidence interval for a parameter \\(\\theta\\) is given by \\((a, b)\\), where \\(\\Pr(a \\leq \\theta \\leq b) = 1 - \\alpha\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asymptotics & the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "asymptotics.html#theorems",
    "href": "asymptotics.html#theorems",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.4 Theorems",
    "text": "6.4 Theorems\nCentral Limit Theorem (CLT)\nFor iid random variables \\(X_1, \\dots, X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\),\n\\[\n\\sqrt{n} (\\bar{X_n} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\nwhere “\\(\\overset{d}{\\to}\\)” denotes convergence in distribution.\nProof.\nNote that this proof is not completely rigorous, in that we will use the following theorem (without proof) in order to prove the CLT:\nTheorem: Let \\(W_1, \\dots, W_n\\) be a sequence of random variables with MGF of the sequence \\(W_n\\) given by \\(M_{W_n}(t)\\). Also, let \\(V\\) denote another random variable with MGF \\(M_V(t)\\). Then if \\(\\underset{n \\to \\infty}{\\text{lim}} M_{W_n}(t) = M_V(t)\\), for all values of \\(t\\) in some interval around \\(t = 0\\), then the sequence \\(W_1, \\dots, W_n\\) converges in distribution to \\(V\\).\nSuppose \\(X_1, \\dots, X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and let \\(Y_i = (X_i - \\mu)/\\sigma\\). Then \\(E[Y_i] = 0\\), and \\(Var[Y_i] = 1\\) since\n\\[\nE[Y_i] = E \\left[ (X_i - \\mu)/\\sigma\\right] = \\frac{1}{\\sigma} (E[X_i] - \\mu) = \\frac{1}{\\sigma} (\\mu - \\mu) = 0\n\\]\nand\n\\[\nVar[Y_i] = Var\\left[ (X_i - \\mu)/\\sigma\\right] = \\frac{1}{\\sigma^2} Var[X_i - \\mu] = \\frac{1}{\\sigma^2} Var[X_i] = \\frac{\\sigma^2}{\\sigma^2} = 1\n\\]\nFurther, let\n\\[\nZ_n = \\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} = \\frac{1}{\\sqrt{n}} \\sum_{i = 1}^n Y_i\n\\]\nwhere the last two terms are equal since\n\\[\\begin{align*}\n    \\frac{1}{\\sqrt{n}} \\sum_{i = 1}^n Y_i & = \\frac{1}{\\sqrt{n}} \\sum_{i = 1}^n \\left( \\frac{X_i - \\mu}{\\sigma}\\right)\\\\\n    & = \\frac{1}{\\sigma\\sqrt{n}} \\sum_{i = 1}^n (X_i - \\mu) \\\\\n    & = \\frac{1}{\\sigma\\sqrt{n}}  (n\\bar{X} - n\\mu) \\\\\n    & = \\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma}\n\\end{align*}\\]\nWe’ll show that \\(Z_n \\overset{d}{\\to} N(0,1)\\) by showing that the MGF of \\(Z_n\\) converges to the MGF of a standard normal distribution. Let \\(M_Y(t)\\) denote the MGF of each \\(Y_i\\). Then the MGF of \\(\\sum_{i = 1}^n Y_i\\) is given by\n\\[\nE[e^{t\\sum_{i = 1}^n Y_i}] = E[e^{tY_1}e^{tY_2} \\dots e^{tY_n}] = E[e^{tY_1}]E[e^{tY_2}] \\dots E[e^{tY_n}] = M_Y(t)^n\n\\]\nand the MGF of \\(Z_n\\) is\n\\[\nM_{Z_n}(t) = E[e^{tZ_n}] = E[e^{t\\frac{1}{\\sqrt{n}}\\sum_{i = 1}^n Y_i}] = M_Y\\left(\\frac{t}{\\sqrt{n}}\\right)^n\n\\]\nNow note that the Taylor expansion of the function \\(e^{tY}\\) about \\(0\\) is given by\n\\[\ne^{tY} = 1 + tY + \\frac{t^2Y^2}{2!} + \\frac{t^3Y^3}{3!} + \\dots\n\\]\nTaking the expectation of both sides, we obtain\n\\[\nE[e^{tY}] = 1 + tE[Y] + \\frac{t^2E[Y^2]}{2!} + \\frac{t^3E[Y^3]}{3!} + \\dots\n\\]\nand note now that the left hand side is the MGF for \\(Y\\). Recalling that \\(E[Y] = 0\\) and \\(Var[Y] = 1\\), we have\n\\[\nE[e^{tY}] = 1  + \\frac{t^2}{2!} + \\frac{t^3E[Y^3]}{3!} + \\dots\n\\]\nAnd therefore\n\\[\nE[e^{tZ_n}] = \\left[1  + \\frac{t^2}{2n} + \\frac{t^3E[Y^3]}{3!n^{3/2}} + \\dots \\right]^n\n\\]\nWe’ll now make use of a theorem regarding sequences of real numbers (without proof): Let \\(a_n\\) and \\(c_n\\) be sequences of real numbers such that \\(a_n \\overset{n \\to \\infty}{\\to} 0\\) and \\(c_na_n^2 \\overset{n \\to \\infty}{\\to} 0\\). Then if \\(a_nc_n \\overset{n \\to \\infty}{\\to} b\\), \\((1 + a_n)^{c_n} \\overset{n \\to \\infty}{\\to} e^b\\).\nLet \\(a_n = \\frac{t^2}{2n} + \\frac{t^3E[Y^3]}{3!n^{3/2}} + \\dots\\) and \\(c_n = n\\). Note that both \\(a_n \\overset{n \\to \\infty}{\\to} 0\\) and \\(c_na_n^2 \\overset{n \\to \\infty}{\\to} 0\\). Then\n\\[\n\\underset{n \\to \\infty}{\\text{lim}} a_n c_n = \\underset{n \\to \\infty}{\\text{lim}} \\left[ \\frac{t^2}{2} + \\frac{t^3E[Y^3]}{3!n^{1/2}} + \\dots\\right] = \\frac{t^2}{2}\n\\]\nand therefore\n\\[\nM_{Z_n}(t) = (1 + a_n)^{c_n} \\overset{n \\to \\infty}{\\to} e^{t^2/2}\n\\]\nwhere we note that the right hand side is the MGF of a standard normal distribution. Then finally, we have proved that\n\\[\n\\sqrt{n}(\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n\\]\nas desired.\nContinuous Mapping Theorem\nIf \\(X_n \\overset{p}{\\to} X\\), and \\(g\\) is a continuous function, then \\(g(X_n) \\overset{p}{\\to} g(X)\\). Similarly for convergence almost surely and convergence in distribution.\nProof. Left to the reader, but also on Wikipedia.\nSlutsky’s Theorem\nIf \\(g(X, Y)\\) is a jointly continuous function at every point \\((X, a)\\) for some fixed \\(a\\), and if \\(X_n \\overset{d}{\\to} X\\) and \\(Y_n \\overset{p}{\\to} a\\), then \\(g(X_n, Y_n) \\overset{d}{\\to} g(X, a)\\).\nProof. Beyond the scope of the course, unfortunately, but here’s a link to the Wikipedia page if you want to go down that rabbit hole in your spare time.\nDelta-method\nLet \\(\\sqrt{n} (Y - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\\). If \\(g(Y)\\) is differentiable at \\(\\mu\\) and \\(g'(\\mu) \\neq 0\\), then\n\\[\n\\sqrt{n} \\left( g(Y) - g(\\mu)\\right) \\overset{d}{\\to} N(0, [g'(\\mu)]^2 \\sigma^2)\n\\]\nProof.\nSince \\(g\\) is differentiable at \\(\\mu\\), it’s first-order Taylor expansion is given by\n\\[\ng(Y) = g(\\mu) + (Y - \\mu)g'(\\mu) + O(| Y - \\mu |^2)\n\\]\nwhere \\(O(f(x))\\), referred to as “Big O,” describes the limiting behavior of the function \\(f(x)\\). In this case, we use it to note that every term in the Taylor expansion after the first derivative evaluated at \\(\\mu\\) is growing no faster than \\(|Y - \\mu|^2\\) as \\(n \\to \\infty\\).\nRearranging, note that\n\\[\ng(Y) - g(\\mu) =  (Y - \\mu)g'(\\mu) + O(| Y - \\mu |^2)\n\\]\nand so\n\\[\\begin{align*}\n    \\sqrt{n}\\left( g(Y) - g(\\mu) \\right) = \\sqrt{n}(Y - \\mu) g'(\\mu) + O(\\sqrt{n} |Y - \\mu|^2)\n\\end{align*}\\]\nThen note that \\(\\sqrt{n}(Y - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\\), \\(g'(\\mu) \\overset{p}{\\to} g'(\\mu)\\) since it’s just a constant, and \\(O(\\sqrt{n} |Y - \\mu|^2) \\overset{p}{\\to} 0\\) (due to the \\(\\sqrt{n}\\) term). Then using two applications of Slutsky’s theorem, we can write that\n\\[\n\\sqrt{n}(Y - \\mu)g'(\\mu) \\overset{d}{\\to} N(0, \\sigma^2)g'(\\mu) = \\overset{d}{\\to} N(0, [g'(\\mu)]^2\\sigma^2)\n\\] and\n\\[\\begin{align*}\n    \\sqrt{n}(Y - \\mu)g'(\\mu) + O(\\sqrt{n} |Y - \\mu|^2) & \\overset{d}{\\to} N(0, \\sigma^2)g'(\\mu) + 0\\\\\n    & \\overset{d}{=} N(0, [g'(\\mu)]^2\\sigma^2)\n\\end{align*}\\]\nand so finally we have shown that \\[\n\\sqrt{n}\\left( g(Y) - g(\\mu) \\right) \\overset{d}{\\to}  N(0, [g'(\\mu)]^2\\sigma^2)\n\\] as desired.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asymptotics & the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "asymptotics.html#worked-examples",
    "href": "asymptotics.html#worked-examples",
    "title": "6  Asymptotics & the Central Limit Theorem",
    "section": "6.5 Worked Examples",
    "text": "6.5 Worked Examples\nProblem 1: Suppose \\(\\sqrt{n}(Y_n - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\\). Find the asymptotic distribution of \\(\\sqrt{n}(Y_n^2 - \\mu^2)\\) when \\(\\mu \\neq 0\\).\n\n\nSolution:\n\nWe can apply the delta-method with the function \\(g(x) = x^2\\). Note that \\(g'(x) = 2x\\), and therefore we can write\n\\[\\begin{align*}\n    \\sqrt{n}(Y_n - \\mu) & \\overset{d}{\\to} N(0, \\sigma^2) \\\\\n    \\sqrt{n}(g(Y_n) - g(\\mu)) & \\overset{d}{\\to} N(0, [g'(\\mu)]^2\\sigma^2) \\\\\n    \\sqrt{n}(Y_n^2 - \\mu^2) & \\overset{d}{\\to} N(0, [2\\mu]^2\\sigma^2) \\\\\n    \\sqrt{n}(Y_n^2 - \\mu^2) & \\overset{d}{\\to} N(0, 4 \\mu^2\\sigma^2)\n\\end{align*}\\]\n\nProblem 2: Suppose \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Bernoulli(p)\\), and recall that the MLE for \\(p\\) is given by \\(\\hat{p}_{MLE} = \\frac{1}{n} \\sum_{i = 1}^n X_i\\). Find the asymptotic distribution of \\(\\hat{p}_{MLE}\\) using the CLT and known properties of the Bernoulli distribution (expectation and variance, for example), and construct a 95% confidence interval for \\(p\\) based on this asymptotic distribution.\n\n\nSolution:\n\nWe know that \\(E[X_i] = p\\) and \\(Var[X_i] = p(1-p)\\). Then the CLT tell us that\n\\[\n\\sqrt{n}(\\hat{p}_{MLE} - p) \\overset{d}{\\to} N(0, p(1-p))\n\\]\nThe WLLN gives us that \\(\\hat{p}_{MLE} \\overset{p}{\\to} p\\), since \\(\\hat{p}_{MLE}\\) is a sample mean. We can then use the continuous mapping theorem to show that \\(\\frac{1}{\\sqrt{\\hat{p}_{MLE}(1-\\hat{p}_{MLE})}} \\overset{p}{\\to} \\frac{1}{\\sqrt{p(1 - p)}}\\). Applying Slutsky’s theorem, we then have\n\\[\n\\sqrt{n}\\left(\\frac{\\hat{p}_{MLE} - p}{\\sqrt{\\hat{p}_{MLE}(1-\\hat{p}_{MLE})}}\\right) \\overset{d}{\\to} N(0, 1)\n\\]\nand finally, (letting \\(\\hat{p} = \\hat{p}_{MLE}\\) for ease of notation)\n\\[\\begin{align*}\n    0.95 & = \\Pr\\left(-1.96 &lt; \\frac{\\hat{p} - p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}  &lt; 1.96\\right)  \\\\\n    & = \\Pr\\left(-1.96\\sqrt{\\hat{p}(1-\\hat{p})/n} &lt; \\hat{p} - p  &lt; 1.96\\sqrt{\\hat{p}(1-\\hat{p})/n}\\right) \\\\\n    & = \\Pr\\left(\\hat{p} -1.96\\sqrt{\\hat{p}(1-\\hat{p})/n} &lt;  p  &lt; \\hat{p} + 1.96\\sqrt{\\hat{p}(1-\\hat{p})/n}\\right)\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Asymptotics & the Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "7  Hypothesis Testing",
    "section": "",
    "text": "Wald Tests\nThe goal of hypothesis testing is to make a decision between two conflicting theories, or “hypotheses.” The process of hypothesis testing involves the following steps:\nThe first step is relatively straightforward. For the purposes of this course, our null hypothesis will always be that some unknown parameter we are interested in \\((\\theta)\\) is equal to a fixed point \\((\\theta_0)\\). We’ll consider two possible alternatives hypothesis:\nThe former is the simplest, non-trivial alternative hypothesis we can consider, and we can prove some nice things in this setting (and hence build intuition for hypothesis testing broadly). The latter is perhaps more relevant, particularly in linear regression.\nIf you recall from introductory statistics, the latter alternative provides the set-up we have when testing if the linear relationship between a predictor \\(X\\) and outcome \\(Y\\) are “statistically significantly” associated; we test the null hypothesis \\(H_0: \\beta_1 = 0\\) against the alternative, \\(H_1: \\beta_1 \\neq 0\\), where \\(E[Y \\mid X] = \\beta_0 + \\beta_1 X\\). In this example, we’d have the unknown parameter \\(\\beta_1\\), and the fixed point of our null hypothesis as \\(\\theta_0 = 0\\).\nThe second step of hypothesis testing is the investigation. In determining whether the data are compatible with the null hypothesis, we must first derive a test statistic. Test statistics are typically functions of (1) our estimators and (2) the distribution of our estimator under the null hypothesis. Intuitively, if we can determine the distribution of our estimator under the null hypothesis, we can then observe whether or not the data we actually have is “extreme” or not, given a certain threshold, \\(\\alpha\\), for our hypothesis test. This threshold \\(\\alpha\\) is directly related to a \\(100(1 - \\alpha)\\%\\) confidence interval, where anything observed outside the confidence interval bounds is considered to lie in the “rejection region” (where you would thus reject the null hypothesis).\nThere are three classical forms of test statistics that have varying finite-sample properties, and can be shown to be asymptotically equivalent: the Wald test, the likelihood ratio test (LRT), and the score test (also sometimes called the Lagrange multiplier test). Each of these is explained in further detail below.\nSuppose we are interested in testing the hypotheses \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta \\neq \\theta_0\\). The Wald test is the hypothesis test that uses the Wald test statistic \\(\\lambda_{W}\\), where\n\\[\n\\lambda_W = \\left( \\frac{\\hat{\\theta}_{MLE} - \\theta_0}{se(\\hat{\\theta}_{MLE})}\\right)^2.\n\\]\nIntuitively, the Wald test measures the difference between the estimated value for \\(\\theta\\) and the null value for \\(\\theta\\), standardized by the variation of your estimator. If this reminds you (once again) of a z-score, it should! In linear regression, with normally distributed standard errors, it turns out that \\(\\sqrt{W}\\) follows a \\(t\\) distribution (we’ll show this on your problem set!).\nWald tests statistics are extremely straightforward to compute from the Central Limit Theorem. The CLT states that, for iid \\(X_1, \\dots, X_n\\) with expectation \\(\\mu\\) and variance \\(\\sigma\\),\n\\[\n\\sqrt{n} (\\overline{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2).\n\\]\nSlutsky’s theorem allows us to write\n\\[\n\\left( \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}}\\right) \\overset{d}{\\to} N(0,1),\n\\]\nand note that the left-hand side is an estimator minus it’s expectation, divided by it’s standard error. When \\(\\overline{X}\\) is the MLE for \\(\\mu\\), this is the square root of the Wald test statistic! The final thing to note is that the right-hand side tells us this quantity converges in distribution to a standard normal distribution. Think about what we’ve previously shown about standard normals “squared” to intuit the asymptotic distribution of a Wald test statistic: a \\(\\chi^2_\\nu\\) random variable, where the degrees of freedom \\(\\nu\\) in this case is one! For a single parameter restriction (i.e. one hypothesis for one unknown parameter), the asymptotic distribution of a Wald test statistic will always be \\(\\chi^2_1\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#learning-objectives",
    "href": "hypothesis.html#learning-objectives",
    "title": "7  Hypothesis Testing",
    "section": "7.1 Learning Objectives",
    "text": "7.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive and implement a hypothesis test using each of the three classical test statistics to distinguish between two conflicting hypotheses\nDescribe the differences and relationships between Type I Error, Type II Error, and power, as well as the factors that influence each of them\nCalculate the power or Type II error for a given hypothesis test",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#concept-questions",
    "href": "hypothesis.html#concept-questions",
    "title": "7  Hypothesis Testing",
    "section": "7.2 Concept Questions",
    "text": "7.2 Concept Questions\n\nWhat is the goal of hypothesis testing?\nWhat are the typical steps to deriving a hypothesis test?\nWhat is the difference between a one-sided and a two-sided alternative hypothesis? How does this impact our hypothesis testing procedure? How does this impact our p-value?\nHow are test statistics and p-values related?\nHow is type I error related to the choice of significance level?\nWhat are the typical steps to calculating the probability of a type II error?\nHow is type II error related to the power of a hypothesis test?\nWhat factors influence the power of a test? In practice, which of these factors can we control?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#definitions",
    "href": "hypothesis.html#definitions",
    "title": "7  Hypothesis Testing",
    "section": "7.3 Definitions",
    "text": "7.3 Definitions\nWald Test Statistic\nThe Wald test statistic \\(\\lambda_W\\) for testing the hypothesis \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta \\neq \\theta_0\\) is given by\n\\[\n\\lambda_W = \\left(\\frac{\\hat{\\theta}_{MLE} - \\theta_0}{se(\\hat{\\theta}_{MLE})}\\right)^2,\n\\]\nwhere \\(\\hat{\\theta}_{MLE}\\) is a maximum likelihood estimator.\nLikelihood Ratio Test (LRT) Statistic\nThe likelihood ratio test statistic \\(\\lambda_{LRT}\\) for testing the hypothesis \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta \\neq \\theta_0\\) is given by\n\\[\n\\lambda_{LRT} = -2 \\log\\left(\\frac{\\underset{\\theta = \\theta_0}{\\text{sup}} \\hspace{1mm} L(\\theta)}{\\underset{\\theta \\in \\Theta}{\\text{sup}} \\hspace{1mm} L(\\theta)}\\right),\n\\]\nwhere we note that the denominator, \\(\\underset{\\theta \\in \\Theta}{\\text{sup}} \\hspace{1mm} L(\\theta)\\), is the likelihood evaluated at the maximum likelihood estimator.\nScore Test Statistic\nThe score test statistic \\(\\lambda_S\\) for testing the hypothesis \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta \\neq \\theta_0\\) is given by\n\\[\n\\lambda_S = \\frac{\\left( \\frac{\\partial}{\\partial \\theta_0} \\log L(\\theta_0 \\mid x) \\right)^2}{I(\\theta_0)}.\n\\]\nPower\nPower is the probability that we correctly reject the null hypothesis; aka, the probability that we reject the null hypothesis, when the null hypothesis is actually false. As a conditional probability statement: \\(\\Pr(\\text{Reject }H_0 \\mid H_0 \\text{ False})\\). Note that\n\\[\n\\text{Power} = 1 - \\text{Type II Error}\n\\]\nType I Error (“False positive”)\nType I Error is the probability that the null hypothesis is rejected, when the null hypothesis is actually true. As a conditional probability statement: \\(\\Pr(\\text{Reject }H_0 \\mid H_0 \\text{ True})\\)\nType II Error (“False negative”)\nType II Error is the probability that we fail to reject the null hypothesis, given that the null hypothesis is actually false. As a conditional probability statement: \\(\\Pr(\\text{Fail to reject }H_0 \\mid H_0 \\text{ False})\\)\nCritical Region / Rejection Region\nThe critical/rejection region is defined as the set of values for which the null hypothesis would be rejected. This set is often denoted with a capital \\(R\\).\nCritical Value\nThe critical value is the point that separates the rejection region from the “acceptance” region (i.e., the value at which the decision for your hypothesis test would change). Acceptance is in quotes because we should never “accept” the null hypothesis… but we still call the “fail-to-reject” region the acceptance region for short.\nSignificance Level\nThe significance level, denoted \\(\\alpha\\), is the probability that, under the null hypothesis, the test statistic lies in the critical/rejection region.\nP-value\nThe p-value associated with a test statistic is the probability of obtaining a value as or more extreme than the observed test statistic, under the null hypothesis.\nUniformly Most Powerful (UMP) Test\nA “most powerful” test is a hypothesis test that has the greatest power among all possible tests of a given significance threshold \\(\\alpha\\). A uniformly most powerful (UMP) test is a test that is most powerful for all possible values of parameters in the restricted parameter space, \\(\\Theta_0\\).\nMore formally, let the set \\(R\\) denote the rejection region of a hypothesis test. Let\n\\[\n\\phi(x) = \\begin{cases} 1 & \\quad \\text{if } x \\in R \\\\ 0 & \\quad \\text{if } x \\in R^c \\end{cases}\n\\]\nThen \\(\\phi(x)\\) is an indicator function. Recalling that expectations of indicator functions are probabilities, note that \\(E[\\phi(x)] = \\Pr(\\text{Reject } H_0)\\). \\(\\phi(x)\\) then represents our hypothesis test. A hypothesis test \\(\\phi(x)\\) is UMP of size \\(\\alpha\\) if, for any other hypothesis test \\(\\phi'(x)\\) of size (at most) \\(\\alpha\\),\n\\[\n\\underset{\\theta \\in \\Theta_0}{\\text{sup}} E[\\phi'(X) \\mid \\theta] \\leq \\underset{\\theta \\in \\Theta_0}{\\text{sup}} E[\\phi(X) \\mid \\theta]\n\\]\nwe have that \\(\\forall \\theta \\in \\Theta_1\\),\n\\[\nE[\\phi'(X) \\mid \\theta] \\leq E[\\phi(X) \\mid \\theta],\n\\]\nwhere \\(\\Theta_0\\) is the set of all values for \\(\\theta\\) that align with the null hypothesis (sometimes just a single point, sometimes a region), and \\(\\Theta_1\\) is the set of all values for \\(\\theta\\) that align with the alternative hypothesis (sometimes just a single point, sometimes a region). Note: In general, UMP tests do not exist for two-sided alternative hypotheses. The Neyman-Pearson lemma tells us about UMP tests for simple null and alternative hypotheses, and the Karlin-Rubin theorem extends this to one-sided null and alternative hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#theorems",
    "href": "hypothesis.html#theorems",
    "title": "7  Hypothesis Testing",
    "section": "7.4 Theorems",
    "text": "7.4 Theorems\nNeyman-Pearson Lemma\nConsider a hypothesis test with \\(H_0: \\theta = \\theta_0\\) and \\(H_1: \\theta = \\theta_1\\). Let \\(\\phi\\) be a likelihood ratio test of level \\(\\alpha\\), where \\(\\alpha = E[\\phi(X) \\mid \\theta_0]\\). Then \\(\\phi\\) is a UMP level \\(\\alpha\\) test for thee hypotheses \\(H_0: \\theta = \\theta_0\\) and \\(H_1: \\theta = \\theta_1\\).\nProof.\nLet \\(\\alpha = E[\\phi(X) \\mid \\theta_0]\\). Note that the LRT statistic is simplified in the case of these simple hypotheses, and can be written just as \\(\\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)}\\).* If the likelihood under the alternative is greater than some constant \\(c\\) (which depends on \\(\\alpha\\)), then we reject the null in favor of the alternative, and vice versa. Then the hypothesis testing function \\(\\phi\\) can be written as\n\\[\n\\phi(x) = \\begin{cases} 0 & \\quad \\text{if } \\lambda_{LRT} = \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &lt; c\\\\\n1 & \\quad \\text{if } \\lambda_{LRT} = \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &gt; c\\\\\n\\text{Flip a coin} & \\quad \\text{if } \\lambda_{LRT} = \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} = c\n\\end{cases}\n\\]Suppose \\(\\phi'\\) is any other test such that \\(E[\\phi'(X) \\mid \\theta_0] \\leq \\alpha\\) (another level \\(\\alpha\\) test). Then we must show that \\(E[\\phi'(X) \\mid \\theta_1] \\leq E[\\phi(X) \\mid \\theta_1]\\).\nBy assumption, we have\n\\[\\begin{align*}\n    E[\\phi(X) \\mid \\theta_0] &= \\int \\phi(x) f_X(x \\mid \\theta_0) dx = \\alpha \\\\\n    E[\\phi'(X) \\mid \\theta_0] &= \\int \\phi'(x) f_X(x \\mid \\theta_0) dx \\leq \\alpha\n\\end{align*}\\]\nTherefore we can write\n\\[\\begin{align*}\n    E[\\phi(X) & \\mid \\theta_1] - E[\\phi'(X) \\mid \\theta_1] \\\\\n    & = \\int \\phi(x) f_X(x \\mid \\theta_1) dx - \\int \\phi'(x) f_X(x \\mid \\theta_1) dx \\\\\n    & = \\int [\\phi(x) - \\phi'(x)] f_X(x \\mid \\theta_1) dx \\\\\n    & = \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &gt; c \\right\\}} \\underbrace{[\\phi(x) - \\phi'(x)]}_{\\geq 0} f_X(x \\mid \\theta_1) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &lt; c \\right\\}} \\underbrace{[\\phi(x) - \\phi'(x)]}_{\\leq 0} f_X(x \\mid \\theta_1) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} = c \\right\\}} [\\phi(x) - \\phi'(x)] f_X(x \\mid \\theta_1) dx \\\\\n    & \\geq \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &gt; c \\right\\}} [\\phi(x) - \\phi'(x)] cf_X(x \\mid \\theta_0) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} &lt; c \\right\\}} [\\phi(x) - \\phi'(x)] cf_X(x \\mid \\theta_0) dx + \\int_{\\left\\{ \\frac{f(x \\mid \\theta_1)}{f(x \\mid \\theta_0)} = c \\right\\}} [\\phi(x) - \\phi'(x)] cf_X(x \\mid \\theta_0) dx \\\\\n    & = c \\int [\\phi(x) - \\phi'(x)] f_X(x \\mid \\theta_0) dx \\\\\n    & = c \\int \\phi(x) f_X(x \\mid \\theta_0) dx - c \\int \\phi'(x) f_X(x \\mid \\theta_0) dx \\\\\n    & \\geq c(\\alpha - \\alpha) \\\\\n    & = 0\n\\end{align*}\\]\nAnd rearranging yields\n\\[\\begin{align*}\nE[\\phi(X)  \\mid \\theta_1] - E[\\phi'(X) \\mid \\theta_1] & \\geq 0 \\\\\nE[\\phi(X) \\mid \\theta_1] & \\geq E[\\phi'(X) \\mid \\theta_1]\n\\end{align*}\\]\nas desired.\n*Note: The \\(-2\\log(\\dots)\\) piece comes into play for the LRT statistic to ensure that the test statistic converges in distribution to a \\(\\chi^2\\) random variable. When we’re just comparing the LRT statistic to another LRT test statistic, we can (more simply) just compare the ratio of likelihoods. Think: comparing \\(X\\) vs. \\(Y\\) is equivalent to comparing \\(\\log(X)\\) vs. \\(\\log(Y)\\) if we are only interested in the direction of the difference between them, since \\(\\log\\) is a monotone function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#worked-examples",
    "href": "hypothesis.html#worked-examples",
    "title": "7  Hypothesis Testing",
    "section": "7.5 Worked Examples",
    "text": "7.5 Worked Examples\nProblem 1: Let \\(Y_i \\overset{iid}{\\sim} N(\\mu, \\sigma^2)\\), where \\(\\sigma^2 = 25\\) is known. Suppose we want to test the hypotheses \\(H_0: \\mu = 8\\) vs. \\(H_1: \\mu \\neq 8\\) and we observe \\(\\overline{Y} = 10\\) across \\(n = 64\\) observations. Can we reject \\(H_0\\), with a significance threshold of \\(\\alpha = 0.05\\)? (Use a Wald test statistic)\n\n\nSolution:\n\nOur hypotheses are already stated in the problem set-up. The next thing we should do is derive a Wald test statistic. We know that the MLE for \\(\\mu\\) is given by \\(\\hat{\\mu}_{MLE} = \\overline{Y}\\) (we have shown this is previous problem sets/worked examples). Then the Wald test statistic can be written as\n\\[\n\\lambda_W = \\left( \\frac{\\hat{\\mu}_{MLE} - \\mu_0}{\\sigma/\\sqrt{n}}\\right)^2 = \\left( \\frac{10 - 8}{5/\\sqrt{64}}\\right)^2 = 10.24\n\\]\nWe can compare this test statistic to the critical value from a \\(\\chi^2_1\\) distribution since, by properties of normal distributions and recalling that standard normal distributions squared are \\(\\chi^2_1\\),\n\\[\\begin{align*}\n    \\overline{Y} & \\sim N(\\mu, \\sigma^2/n) \\\\\n    \\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}} & \\sim N(0,1) \\\\\n    \\left( \\frac{\\overline{Y} - \\mu}{\\sigma/\\sqrt{n}}  \\right)^2 & \\sim \\chi^2_1\n\\end{align*}\\]\nTo calculate the critical value when \\(\\alpha = 0.05\\), we turn to R.\n\n# The quantile function for a given distribution gives us the value at which\n# a given percentage of the distributions lies ahead of that value, which\n# is exactly what we want in this case!\n\nqchisq(1 - 0.05, df = 1)\n\n[1] 3.841459\n\n\nFinally, noting that our test statistic is greater than the critical value, we reject \\(H_0\\).\n\nProblem 2: Suppose we wanted to use a different significance level \\(\\alpha\\). How would the procedure in Problem 1 change if we let \\(\\alpha = 0.001\\)? How would the procedure in Problem 1 change if we let \\(\\alpha = 0.1\\)?\n\n\nSolution:\n\nChanging the significance level changes the critical value, and may change whether or not we reject \\(H_0\\), depending on the difference between our critical value and the test statistic. We can calculate what the critical value would be if we let \\(\\alpha = 0.01\\) and \\(\\alpha = 0.1\\) again in R:\n\n# alpha = 0.01\nqchisq(1 - 0.001, df = 1)\n\n[1] 10.82757\n\n# alpha = 0.1\nqchisq(1 - 0.1, df = 1)\n\n[1] 2.705543\n\n\nNote that when \\(\\alpha = 0.1\\), we still reject \\(H_0\\). This should make intuitive sense, since increasing \\(\\alpha\\) only can only increase our rejection region. However, when \\(\\alpha = 0.001\\), we would fail to reject \\(H_0\\), as our test statistic is not “more extreme” (greater) than the critical value.\n\nProblem 3: Suppose we have a random sample \\(X_1, \\dots, X_n \\sim Bernoulli(p)\\), and we want to test the hypotheses \\(H_0:p = 0.5\\), \\(H_1:p \\neq 0.5\\). Suppose we calculate an estimator for \\(p\\) as \\(\\hat{p} = \\frac{1}{n} \\sum_{i = 1}^n X_i\\). Derive a Wald test statistic for this hypothesis testing scenario (simplifying as much as you can).\n\n\nSolution:\n\nRecall that \\(\\hat{p}\\) as defined in the problem set-up is the MLE for \\(p\\). Then the Wald test statistic can be written as\n\\[\n\\lambda_W = \\left( \\frac{\\hat{p} - p_0}{se(\\hat{p})}\\right)^2.\n\\]\nWe can simplify a little further by calculating \\(se(\\hat{p})\\) and plugging in \\(p_0\\). Recall from the CLT (and Slutsky) that we have\n\\[\n\\left( \\frac{\\hat{p} - p_0}{\\sqrt{\\hat{p}(1 - \\hat{p})/n}} \\right) \\overset{d}{\\to} N(0,1)\n\\]\nThen the standard error of \\(\\hat{p}\\) is given by \\(\\sqrt{\\hat{p}(1 - \\hat{p})/n}\\), and our Walt test statistic simplifies to\n\\[\n\\lambda_W = \\left( \\frac{\\hat{p} - 0.5}{\\sqrt{\\hat{p}(1 - \\hat{p})/n}}\\right)^2.\n\\]\n(Note that this is as “simplified” as we can get without knowing \\(\\hat{p}\\) or \\(n\\))\n\nProblem 4: Derive a LRT statistic for the hypothesis testing scenario described in Problem 3 (simplifying as much as you can).\n\n\nSolution:\n\nThe LRT statistic is given by\n\\[\n\\lambda_{LRT} = -2 \\log\\left(\\frac{\\underset{p = p_0}{\\text{sup}} \\hspace{1mm} L(p)}{\\underset{p \\in \\Theta}{\\text{sup}} \\hspace{1mm} L(p)} \\right)= -2 \\log\\left(\\frac{ L(0.5)} {L(\\hat{p}_{MLE})} \\right)\n\\]\nThe likelihood for our observations can be written as\n\\[\nL(p) = \\prod_{i = 1}^n p^{x_i} (1 - p)^{(1 - x_i)}\n\\]\nAnd so our LRT statistic simplifies to\n\\[\\begin{align*}\n\\lambda_{LRT} & = -2 \\log\\left(\\frac{ L(0.5)} {L(\\hat{p})} \\right) \\\\\n& = -2 \\left[\\log L(0.5) - \\log L(\\hat{p}) \\right] \\\\\n& = -2 \\left[ \\log(0.5) \\sum_{i = 1}^n X_i + \\log(1 - 0.5)\\sum_{i = 1}^n(1 - X_i) - \\log(\\hat{p}) \\sum_{i = 1}^n X_i - \\log(1 - \\hat{p})\\sum_{i = 1}^n(1 - X_i)\\right] \\\\\n& = -2 \\left[ \\log(0.5) \\left( \\sum_{i = 1}^n X_i + \\sum_{i = 1}^n(1 - X_i)\\right) - \\log(\\hat{p}) \\sum_{i = 1}^n X_i - \\log(1 - \\hat{p})\\sum_{i = 1}^n(1 - X_i)\\right] \\\\\n& = -2 \\left[ n\\log(0.5) - \\log(\\hat{p}) \\sum_{i = 1}^n X_i - \\log(1 - \\hat{p})\\sum_{i = 1}^n(1 - X_i)\\right] \\\\\n& = -2 \\left[ n\\log(0.5) - \\log(\\hat{p}) n \\overline{X} - \\log(1 - \\hat{p}) (n - n \\overline{X})\\right] \\\\\n& = -2 n \\left[ \\log(0.5) - \\log(\\hat{p}) \\hat{p} - \\log(1 - \\hat{p}) (1 -  \\hat{p})\\right]\n\\end{align*}\\]\n(Note that this is as “simplified” as we can get without knowing \\(\\hat{p}\\) or \\(n\\))\n\nProblem 5: Derive a score test statistic for the hypothesis testing scenario described in Problem 3 (simplifying as much as you can).\n\n\nSolution:\n\nThe score test statistic is given by\n\\[\n\\lambda_S = \\frac{\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) \\right)^2}{I(p_0)}.\n\\]\nWe can simplify by deriving the score and information matrix, and then plugging in \\(p_0 = 0.5\\). We have,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) & = \\frac{\\partial}{\\partial p_0} \\left[ \\log(p_0) \\sum_{i = 1}^n X_i + \\log(1 - p_0) \\sum_{i = 1}^n (1 - X_i) \\right] \\\\\n& = \\frac{\\sum_{i = 1}^n X_i}{p_0} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - p_0} \\\\\n\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) \\right)^2 & = \\left(\\frac{\\sum_{i = 1}^n X_i}{p_0} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - p_0} \\right)^2\n\\end{align*}\\]\nand plugging in \\(p_0 = 0.5\\), we have,\n\\[\n\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x)\\right)^2 = \\left(\\frac{\\sum_{i = 1}^n X_i}{0.5} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - 0.5} \\right)^2 = \\left( \\frac{-n + 2\\sum_{i = 1}^n X_i}{0.5}\\right)^2 = \\left( -2n + 4\\sum_{i = 1}^n X_i\\right)^2.\n\\]\nThe information matrix is given by \\(-E\\left[ \\frac{\\partial^2}{\\partial p_0^2} \\log L(p_0 \\mid x)\\right]\\). Piecing this together,\n\\[\\begin{align*}\n\\frac{\\partial^2}{\\partial p_0^2} \\log L(p_0 \\mid x)\n& =  \\frac{\\partial}{\\partial p_0} \\left[ \\frac{\\sum_{i = 1}^n X_i}{p_0} - \\frac{n - \\sum_{i = 1}^n  X_i}{1 - p_0} \\right] \\\\\n& = \\frac{-\\sum_{i = 1}^n X_i}{p_0^2} - \\frac{n - \\sum_{i = 1}^n  X_i}{(1 - p_0)^2}\n\\end{align*}\\]\nAnd to get \\(I(p_0)\\), we take the negative expectation of the above quantity under the null hypothesis (that is, where \\(E[X] = p_0\\)) to obtain\n\\[\\begin{align*}\n    I(p_0) & = -E \\left[ \\frac{-\\sum_{i = 1}^n X_i}{p_0^2} - \\frac{n - \\sum_{i = 1}^n  X_i}{(1 - p_0)^2} \\right] \\\\\n    & = \\frac{1}{p_0^2} \\sum_{i = 1}^n E[X_i] + \\frac{1}{(1 - p_0)^2} \\left( n - \\sum_{i = 1}^n E[X_i]\\right) \\\\\n    & = \\frac{1}{p_0^2} \\sum_{i = 1}^n p_0 + \\frac{1}{(1 - p_0)^2} \\left( n - \\sum_{i = 1}^n p_0\\right) \\\\\n    & = \\frac{n}{p_0}  + \\frac{n}{(1 - p_0)^2} \\left( 1 - p_0\\right) \\\\\n    & = \\frac{n}{p_0}  + \\frac{n}{(1 - p_0)}\n\\end{align*}\\]\nAnd plugging in \\(p_0 = 0.5\\) we have\n\\[\nI(0.5) = \\frac{n}{0.5}  + \\frac{n}{(1 - 0.5)} = 2n + 2n = 4n.\n\\]\nThen, finally, the score test statistic (simplified as much as possible) is given by\n\\[\\begin{align*}\n    \\lambda_S & = \\frac{\\left( \\frac{\\partial}{\\partial p_0} \\log L(p_0 \\mid x) \\right)^2}{I(p_0)} \\\\\n    & = \\frac{\\left( -2n + 4\\sum_{i = 1}^n X_i\\right)^2}{4n} \\\\\n    & = \\frac{\\left( -2n + 4n \\hat{p}\\right)^2}{4n} \\\\\n    & = \\frac{4n^2\\left( -1 + 2 \\hat{p}\\right)^2}{4n} \\\\\n    & = n\\left( -1 + 2 \\hat{p}\\right)^2\n\\end{align*}\\]\n\nProblem 6: For each of Problems 3, 4, and 5, calculate the p-values from each test when \\(\\hat{p} = 0.4\\) and \\(n = 300\\).\n\n\nSolution:\n\nWe’ll again use R to obtain the critical values for these hypothesis tests, noting that in each case, the test statistic follows a \\(\\chi^2_1\\) distribution asymptotically:\n\np_hat &lt;- 0.4\nn &lt;- 300\n\n# Wald test statistic\nlambda_w &lt;- ((p_hat - 0.5)/(sqrt(p_hat * (1 - p_hat) / n)))^2\n\n# LRT statistic\nlambda_lrt &lt;- -2 * n * (log(0.5) - log(p_hat) * p_hat - log(1 - p_hat) * (1 - p_hat))\n\n# Score test statistic\nlambda_s &lt;- n * (-1 + 2 * p_hat)^2\n\n# Compare statistics\nlambda_w\n\n[1] 12.5\n\nlambda_lrt\n\n[1] 12.08131\n\nlambda_s\n\n[1] 12\n\n# Calculate p-values\n# Recall: probability that we observe something *as or more extreme*\n1 - pchisq(lambda_w, df = 1)\n\n[1] 0.000406952\n\n1 - pchisq(lambda_lrt, df = 1)\n\n[1] 0.0005092985\n\n1 - pchisq(lambda_s, df = 1)\n\n[1] 0.0005320055\n\n\nThings to note:\n\nWhen \\(n\\) is large (300, in this case), each of the three classical test statistics are approximately equal! This makes sense, as they all converge in distribution to the same random variable, asymptotically.\nP-values are the probability that we would observe something as or more extreme than what we actually did observe, under the null hypothesis. In R, we can use the p function (for a given pdf) to calculate this.\n\n\nProblem 7: Repeat Problem 6 but with \\(\\hat{p} = 0.4\\) and \\(n = 95\\). If your significance threshold were \\(\\alpha = 0.05\\), would your conclusion to the hypothesis test be the same regardless of which test statistic you chose?\n\n\nSolution:\n\nTo answer this question, we can again calculate p-values, and compare them to 0.05 (note that we could have also calculated a critical value, and compared our test statistics to the critical value, as these are equivalent).\n\np_hat &lt;- 0.4\nn &lt;- 95\n\n# Wald test statistic\nlambda_w &lt;- ((p_hat - 0.5)/(sqrt(p_hat * (1 - p_hat) / n)))^2\n\n# LRT statistic\nlambda_lrt &lt;- -2 * n * (log(0.5) - log(p_hat) * p_hat - log(1 - p_hat) * (1 - p_hat))\n\n# Score test statistic\nlambda_s &lt;- n * (-1 + 2 * p_hat)^2\n\n# Compare statistics\nlambda_w\n\n[1] 3.958333\n\nlambda_lrt\n\n[1] 3.825748\n\nlambda_s\n\n[1] 3.8\n\n# Calculate p-values\n# Recall: probability that we observe something *as or more extreme*\n1 - pchisq(lambda_w, df = 1)\n\n[1] 0.04663986\n\n1 - pchisq(lambda_lrt, df = 1)\n\n[1] 0.05047083\n\n1 - pchisq(lambda_s, df = 1)\n\n[1] 0.05125258\n\n\nIn this case, we would reject \\(H_0\\) using the Wald test statistic, but fail to reject using the LRT statistic and score test statistic, since the only p-value that was below our significance threshold was the one calculated from the Wald test statistic. Finite-sample distributions of the three classical test statistics are generally unknown; only asymptotically have they been shown to be equivalent, and therefore, can provide different answers to hypothesis tests when sample sizes are relatively small.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "8  Bayesian Statistics",
    "section": "",
    "text": "Philosophy\nEverything that we have covered so far in this course (and likely what you have covered in your entire statistics education thus far) has been from a Frequentist perspective. Frequentist statistics relies on the underlying belief that, in reality, there is some fixed, unknown truth (parameter) that we attempt to estimate by sampling from a population, computing an estimate, and quantifying our uncertainty. Uncertainty quantification typically takes the form of a confidence interval, and relies on the idea of repeated sampling from a population. The term “Frequentist” comes from the idea of a probability being related to the “frequency” at which an event occurs.\nBayesian statistics is named for Thomas Bayes, who coined Bayes’ Theorem in 1763. At around a similar time, Pierre-Simon Laplace worked on very similar ideas, though all credit to Bayesian statistics is typically given to Thomas Bayes. While Bayes’ Theorem itself is not inherently Bayesian (it is quite literally just a probability rule), it provides us with a mathematical foundation for Bayesian philosophy.\nWhile Frequentists treat parameters as unknown, fixed constants, Bayesians instead treat parameters as random variables, such that parameters follow probability distributions. This distinction may seem subtle, but has large consequences on the interpretation of uncertainty in each paradigm, as well as the properties of Frequentist and Bayesian estimators (particularly in finite samples).\nRather than think of probability as being related to the frequency at which events occur, Bayesians instead think of probabilities in the more colloquial way: the plausibility that an event were to occur. In order to calculate the latter, we incorporate prior information or beliefs about the event and the data we observe to update our beliefs.\nNote that this is inherently subjective, as prior information / beliefs are involved in our estimation framework. This subjectivity is one of the main reasons why Bayesian statistics was historically rejected and frowned upon in the statistics community, in addition to computational challenges that have really only been alleviated with computational advances made in the last 50 or so years. From a purely philosophical standpoint, Frequentist and Bayesian inference provide an interesting case study of the Enlightenment period, and modern thinking more broadly, compared with post-modern thinking. Back in the day, Frequentists and Bayesians were distinct. Nowadays, most reasonable statisticians will agree that both Frequentist and Bayesian methods have a place in statistics, are subjective in their own ways, and are both useful in different circumstances.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayes.html#learning-objectives",
    "href": "bayes.html#learning-objectives",
    "title": "8  Bayesian Statistics",
    "section": "8.1 Learning Objectives",
    "text": "8.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nArticulate the differences in Frequentist and Bayesian philosophy\nDerive the posterior distribution for an unknown parameter based on a specified prior and likelihood\nEvaluate the properties of posterior means, medians, etc.\nArticulate the impact of the choice of prior distribution on Bayesian estimation",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayes.html#concept-questions",
    "href": "bayes.html#concept-questions",
    "title": "8  Bayesian Statistics",
    "section": "8.2 Concept Questions",
    "text": "8.2 Concept Questions\n\nWhat is the difference between the Bayesian and Frequentist philosophies?\nWhat are the typical steps to deriving a posterior distribution?\nHow is the posterior distribution impacted by the observed data and our choice of prior? What sorts of considerations should we keep in mind in choosing a prior?\nHow are Bayes and maximum likelihood estimators typically related?\nWhat are typical Frequentist properties (e.g., bias, asymptotic bias, consistency) of Bayesian estimators (posterior means, for example)?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayes.html#definitions",
    "href": "bayes.html#definitions",
    "title": "8  Bayesian Statistics",
    "section": "8.3 Definitions",
    "text": "8.3 Definitions\nBayes’ Theorem, Prior distribution, Posterior distribution\nFor two random variables \\(\\theta\\) and \\(\\textbf{X}\\), Bayes’ theorem states that,\n\\[\n\\pi(\\theta \\mid \\textbf{X}) = \\frac{\\pi(\\textbf{X} \\mid \\theta)\\pi(\\theta)}{\\pi(\\textbf{X})},\n\\]\nwhere \\(\\pi(\\theta)\\) denotes the prior distribution of \\(\\theta\\), \\(\\pi(\\textbf{X} \\mid \\theta)\\) denotes the likelihood, \\(\\pi(\\theta \\mid \\textbf{X})\\) denotes the posterior distribution of \\(\\theta\\), and \\(\\pi(\\textbf{X})\\) denotes the normalizing constant.\nImproper prior\nAn improper prior is a prior distribution that does not integrate to 1. This means that the prior is not a probability density function, since all pdfs must integrate to 1. In practice, some improper priors can still lead to proper posterior distributions, and as such, they are occasionally used as one type of non-informative prior. The most commonly used improper proper is the uniform distribution from \\(-\\infty\\) to \\(\\infty\\).\nConjugate prior\nA conjugate prior is a prior distribution that is in the same probability density family as the posterior distribution. Conjugate priors primarily used for computational convenience (as the posterior distributions then have closed form solutions), or when conjugacy makes sense in the context of the modeling problem. For examples of conjugate priors, the Wikipedia page linked here is quite complete.\nPosterior mode\nThe posterior mode is, as the name implies, the mode of the posterior distribution. In math, the posterior mode is the estimate \\(\\hat{\\theta}\\) that satisfies,\n\\[\n\\frac{\\partial}{\\partial \\theta}\\pi(\\theta \\mid \\textbf{X}) = 0.\n\\]\nPosterior median\nThe posterior median is, as the name implies, the median of the posterior distribution. In math, the posterior median is the estimate \\(\\hat{\\theta}\\) that satisfies,\n\\[\n\\int_{-\\infty}^{\\hat{\\theta}} \\pi(\\theta \\mid \\textbf{X}) d\\theta = 0.5\n\\]\nPosterior mean\nThe posterior mean is, as the name implies, the mean of the posterior distribution. In math, the posterior mean is the estimate\n\\[\n\\hat{\\theta} = E[\\theta \\mid \\textbf{X}] = \\int \\theta \\pi(\\theta \\mid \\textbf{X}) d\\theta\n\\]\nCredible interval\nA 100(1 - \\(\\alpha\\))% credible interval is an interval \\((\\Phi_{\\alpha/2}, \\Phi_{1 - \\alpha/2})\\) for a parameter \\(\\theta\\) is given by\n\\[\n\\int_{\\Phi_{\\alpha/2}}^{\\Phi_{1 - \\alpha/2}} \\pi(\\theta \\mid \\textbf{X}) d\\theta = 1 - \\alpha,\n\\]\nwhere \\(\\Phi_p\\) denotes the \\(p\\)th quantile of the posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayes.html#theorems",
    "href": "bayes.html#theorems",
    "title": "8  Bayesian Statistics",
    "section": "8.4 Theorems",
    "text": "8.4 Theorems\nNone for this chapter, other than Bayes’ theorem, which doesn’t really count as a theorem cause it’s just a probability rule!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "bayes.html#worked-examples",
    "href": "bayes.html#worked-examples",
    "title": "8  Bayesian Statistics",
    "section": "8.5 Worked Examples",
    "text": "8.5 Worked Examples\nProblem 1: Suppose we have a random sample \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Bernoulli(\\theta)\\), and choose a \\(Beta(\\alpha, \\beta)\\) prior for \\(\\theta\\). Derive the posterior distribution, \\(\\pi(\\theta \\mid X_1, \\dots, X_n)\\).\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\pi(\\theta \\mid X_1, \\dots, X_n) & \\propto \\left( \\prod_{i = 1}^n f(x_i) \\right) \\pi(\\theta) \\\\\n    & = \\left( \\prod_{i = 1}^n \\theta^{x_i} (1 - \\theta)^{1 - x_i} \\right) \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\\n    & = \\theta^{\\sum_{i = 1}^n x_i} (1 - \\theta)^{n - \\sum_{i = 1}^n x_i}  \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\\n    & \\propto \\theta^{\\sum_{i = 1}^n x_i + \\alpha - 1} (1 - \\theta)^{n - \\sum_{i = 1}^n x_i + \\beta - 1}\n\\end{align*}\\]\nwhere we recognize the kernel of a \\(Beta(\\sum_{i = 1}^n X_i + \\alpha, n - \\sum_{i = 1}^n X_i + \\beta)\\) distribution, and therefore this is the posterior distribution for \\(\\theta\\).\n\nProblem 2: Derive the posterior mean for \\(\\theta\\) in Problem 1.\n\n\nSolution:\n\nWe know that the expectation of a \\(Beta(a, b)\\) distribution is given by \\(\\frac{a}{a + b}\\), and so we have\n\\[\n\\hat{\\theta} = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{\\sum_{i = 1}^n X_i + \\alpha + n - \\sum_{i = 1}^n X_i + \\beta} = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{\\alpha + \\beta + n}\n\\]\n\nProblem 3: Write the posterior mean from Problem 2 as a function of the MLE, \\(\\hat{\\theta}_{MLE} = \\overline{X}\\), and the prior mean for \\(\\theta\\). What do you notice?\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\hat{\\theta} & = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{\\alpha + \\beta + n} \\\\\n    & = \\frac{\\sum_{i = 1}^n X_i }{\\alpha + \\beta + n} + \\frac{\\alpha}{\\alpha + \\beta + n} \\\\\n    & = \\frac{\\frac{n}{n} \\sum_{i = 1}^n X_i}{\\alpha + \\beta + n} + \\frac{\\frac{\\alpha (\\alpha + \\beta)}{\\alpha + \\beta}}{\\alpha + \\beta + n} \\\\\n    & = \\left( \\frac{n}{\\alpha + \\beta + n} \\right) \\overline{X} + \\left( \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\right) \\left( \\frac{\\alpha}{\\alpha + \\beta}\\right)\n\\end{align*}\\]\nand so we can see that the posterior mean is a weighted average of the prior mean and the MLE (in this case, the sample mean)!\n\nProblem 4: Suppose we have a random sample \\(X_1, \\dots, X_n \\overset{iid}{\\sim} Poisson(\\lambda)\\), and choose a \\(Gamma(\\alpha, \\beta)\\) prior for \\(\\lambda\\). Derive the posterior distribution, \\(\\pi(\\lambda \\mid X_1, \\dots, X_n)\\).\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\pi(\\lambda \\mid X_1, \\dots, X_n) \\\\\n    & \\propto \\left( \\prod_{i = 1}^n f(x_i) \\right) \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\\\\n    & = \\left( \\prod_{i = 1}^n \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!} \\right) \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\\\\n    & = \\frac{\\lambda^{\\sum_{i = 1}^n x_i}e^{-n\\lambda}}{\\prod_{i = 1}^n x_i!} \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\\\\n    & \\propto \\lambda^{\\sum_{i = 1}^n x_i + \\alpha - 1} e^{-n\\lambda - \\beta \\lambda} \\\\\n    & = \\lambda^{\\sum_{i = 1}^n x_i + \\alpha - 1} e^{-(n + \\beta)\\lambda}\n\\end{align*}\\]\nwhere we recognize the kernel of a \\(Gamma(\\sum_{i = 1}^n X_i + \\alpha, n + \\beta)\\) distribution, and therefore this is the posterior distribution for \\(\\lambda\\).\n\nProblem 5: Derive the posterior mean for \\(\\lambda\\) in Problem 4.\n\n\nSolution:\n\nWe know that the expectation of a \\(Gamma(a, b)\\) distribution is given by \\(\\frac{a}{b}\\), and so we have\n\\[\\begin{align*}\n    \\hat{\\lambda} = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{n + \\beta}\n\\end{align*}\\]\n\nProblem 6: Write the posterior mean from Problem 5 as a function of the MLE, \\(\\hat{\\lambda}_{MLE} = \\overline{X}\\), and the prior mean for \\(\\lambda\\). What do you notice?\n\n\nSolution:\n\nWe can write,\n\\[\\begin{align*}\n    \\hat{\\lambda} & = \\frac{\\sum_{i = 1}^n X_i + \\alpha}{n + \\beta} \\\\\n    & = \\frac{\\sum_{i = 1}^n X_i}{n + \\beta} + \\frac{\\alpha}{n + \\beta} \\\\\n    & = \\frac{n \\overline{X}}{n + \\beta} + \\frac{\\frac{\\beta\\alpha}{\\beta}}{n + \\beta} \\\\\n    & = \\left( \\frac{n}{n + \\beta}  \\right) \\overline{X} + \\left( \\frac{\\beta}{n + \\beta} \\right) \\frac{\\alpha}{\\beta}\n\\end{align*}\\]\nand so we can see (again) that the posterior mean is a weighted average of the prior mean and the MLE (in this case, the sample mean)!\n\nProblem 7: What is the asymptotic behavior of the posterior means calculated in Problems 2 and 5?\n\n\nSolution:\n\nIn both cases, as \\(n \\to \\infty\\), the posterior mean will approach the MLE! This is easiest to note after we observe that the posterior mean is a weighted average of the MLE and the prior mean. The weight on the prior mean will approach zero, as the weight on the MLE will approach 1, as \\(n\\) goes to infinity.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "decision.html",
    "href": "decision.html",
    "title": "9  Decision Theory",
    "section": "",
    "text": "Admissibility\nStatistical decision theory is the branch of statistics that concerns itself with figuring out the best possible choice to make in a given situation using probability theory. Colloquially, decisions often have pros and cons. We can quantify these pros and cons using a loss function, and calculate the expected loss of a given decision (formally called risk). As you might guess, risk is something we want to minimize. We can minimize risk (after formally defining it) using the same calculus techniques we’ve been using all semester!\nFor the purposes of this class, the “decisions” we make are our choice of estimator for an unknown parameters. This is one type of deterministic decision rule. At the beginning of this course, we learned about two different intuitive approaches to defining estimators (or “decisions”): maximum likelihood estimation and the method of moments. In this chapter, we’ll find estimators that minimize risk!\nWhile decision theory is not inherently Bayesian, it is one way to “justify” point estimates from posterior distributions. Bayes estimates are posterior point estimates that minimize a certain loss function. The posterior mean, median, and mode are all such point estimates, for example.\nAn important concept in decision theory is the idea of admissibility. An admissible decision rule is one that has the lowest possible risk out of all decision rules, for all possible parameter values. It is easier to define (in math) an inadmissible decision rule, and then note that an admissible decision rule is not inadmissible (double negative).\nAn decision rule \\(D\\) (think, \\(\\hat{\\theta}\\)) is inadmissible if there exists a rule \\(D'\\) (think, some other estimator) such that\n\\[\\begin{align*}\nR(D, \\theta) & \\leq R(D, \\theta) \\quad \\forall \\theta \\\\\nR(D', \\theta) & &lt; R(D, \\theta) \\quad \\text{ for some } \\theta\n\\end{align*}\\]\nwhere \\(R(D, \\theta)\\) is the risk of a decision \\(D\\) for a paramater \\(\\theta\\). If \\(D\\) is not inadmissible, it is admissible. In words, in order for a decision rule to be admissible, it must have risk at least as small as every other possible decision rule everywhere in the parameter space, and it must have strictly lower risk for at least one parameter value.\nOne of the most fascinating results to come out of decision theory (in my personal opinion) is that the sample mean is not an admissible decision rule for the mean of a Multivariate Normal distribution under MSE loss when the mean has greater than or equal to three dimensions! Relating this to what you know from introductory statistics, this means that (from a decision theory perspective) least squares is not an admissible approach to estimating the regression coefficients in a linear regression model with at least two covariates. The specific (biased) estimator of the mean that provides a lower MSE in this case is called the James-Stein estimator.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#learning-objectives",
    "href": "decision.html#learning-objectives",
    "title": "9  Decision Theory",
    "section": "9.1 Learning Objectives",
    "text": "9.1 Learning Objectives\nBy the end of this chapter, you should be able to…\n\nDerive a Bayes estimate for a common loss function\nDistinguish between admissible and inadmissible decision rules",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#concept-questions",
    "href": "decision.html#concept-questions",
    "title": "9  Decision Theory",
    "section": "9.2 Concept Questions",
    "text": "9.2 Concept Questions\n\n9.2.1 Reading Questions\n\nWhat are some examples of commonly-used loss functions?\nWhat are the typical steps to finding a Bayes estimate?\nWhat are the Bayes estimates for absolute error loss and squared error loss?\nWhat does it mean for a decision rule to be admissible (in colloquial language)?\nWhat does it mean for a decision rule to be minimax (in colloquial language)?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#definitions",
    "href": "decision.html#definitions",
    "title": "9  Decision Theory",
    "section": "9.3 Definitions",
    "text": "9.3 Definitions\nLoss Function\nLet \\(\\hat{\\theta}\\) be an estimator for \\(\\theta\\). A loss function associated with \\(\\hat{\\theta}\\) is denoted \\(L(\\hat{\\theta}, \\theta)\\), where \\(L(\\hat{\\theta}, \\theta) \\geq 0\\) and \\(L(\\theta, \\theta) = 0\\). A reasonable loss function will increase the further away \\(\\hat{\\theta}\\) and \\(\\theta\\) are from each other.\nDecision Rule\nFor the purposes of this class, an estimator! In the statistics literature, you will often see this denoted \\(D\\), but we can also denote the decision rule \\(\\hat{\\theta}\\) for this class.\nRisk\nIn words, risk is the expected loss of our decision, given our data. In math,\n\\[\nR(\\hat{\\theta}, \\theta) = E[L(\\hat{\\theta}, \\theta) \\mid \\textbf{Y}] = \\int L(\\hat{\\theta}, \\theta) \\pi(\\theta \\mid \\textbf{Y}) d\\theta\n\\]\nBayes Estimate\nA Bayes estimate is the estimate or decision rule that minimizes risk (expected posterior loss). This is sometimes called a “Bayes rule” in the literature.\nUnique Bayes Rule\nFor a given prior \\(\\pi(\\theta)\\), a decision rule \\(D_\\pi\\) is a unique Bayes rule (estimate) if, for all \\(\\theta\\), a decision rule is a Bayes rule if and only if it is equal to \\(D_\\pi\\). Bayes rules are unique when:\n\nThe loss function used is MSE loss\nThe risk of the Bayes rule is finite\nA \\(\\sigma\\)-field condition is satisfied (well beyond the scope of this course)\n\nFor what we consider in this course, whenever we use MSE loss in this course, the other two conditions will be satisfied.\nAdmissibility\nAn decision rule \\(D\\) is inadmissible if there exists a rule \\(D'\\) such that\n\\[\\begin{align*}\nR(D', \\theta) & \\leq R(D, \\theta) \\quad \\forall \\theta \\\\\nR(D', \\theta) & &lt; R(D, \\theta) \\quad \\text{ for some } \\theta\n\\end{align*}\\]\nwhere \\(R(D, \\theta)\\) is the risk of a decision \\(D\\) for a paramater \\(\\theta\\). If \\(D\\) is not inadmissible, it is admissible.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#theorems",
    "href": "decision.html#theorems",
    "title": "9  Decision Theory",
    "section": "9.4 Theorems",
    "text": "9.4 Theorems\nTheorem (Unique Bayes rules are admissible). Any unique Bayes rule is admissible.\nProof. We’ll prove this by contradiction!\nSuppose that \\(D_\\pi\\) is a unique Bayes rule with respect to some prior \\(\\pi(\\theta)\\), and that \\(D_\\pi\\) is inadmissible. Then there exists some other decision rule \\(D'\\) such that \\(R(D', \\theta) \\leq R(D_\\pi, \\theta)\\), for all \\(\\theta\\). Then,\n\\[\\begin{align*}\n    R(D', \\theta) & \\leq R(D_{\\pi}, \\theta) \\quad \\quad \\text{(inadmissibility)} \\\\\n    & = \\inf_D R(D, \\pi) \\quad \\quad \\text{($D_\\pi$ is Bayes)}\n\\end{align*}\\]\nand since \\(R(D', \\theta) \\leq \\inf_D R(D, \\pi)\\), \\(D'\\) is Bayes. But \\(D_\\pi\\) is unique Bayes by assumption, so this is a contradiction.\nTherefore, \\(D_\\pi\\) is admissible.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "decision.html#worked-examples",
    "href": "decision.html#worked-examples",
    "title": "9  Decision Theory",
    "section": "9.5 Worked Examples",
    "text": "9.5 Worked Examples\nProblem 1: Show that the posterior median is the decision rule that minimizes risk with respect to absolute loss, \\(L(\\hat{\\theta}, \\theta) = |\\hat{\\theta} - \\theta|\\).\n\n\nSolution:\n\nWe can write the risk with respect to absolute loss as\n\\[\\begin{align*}\n    R(\\theta_0, \\theta) & = E[L(\\theta_0, \\theta) \\mid \\textbf{Y}] \\\\\n    & = \\int L(\\theta_0, \\theta) \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int |\\theta_0 - \\theta| \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int_{I\\{\\theta_0 \\geq \\theta \\}} \\left( \\theta_0 - \\theta \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{I\\{\\theta_0 &lt; \\theta \\}} \\left(  \\theta - \\theta_0 \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int_{-\\infty}^{\\theta_0} \\left( \\theta_0 - \\theta \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{\\theta_0}^\\infty \\left( \\theta - \\theta_0 \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta\n\\end{align*}\\]\nTaking the derivative with respect to \\(\\theta_0\\), and setting this equal to zero we get\n\\[\\begin{align*}\n    0 & \\equiv \\frac{\\partial}{\\partial \\theta_0} R(\\theta_0, \\theta) \\\\\n    & = \\frac{\\partial}{\\partial \\theta_0} \\left( \\int_{-\\infty}^{\\theta_0} \\left( \\theta_0 - \\theta \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{\\theta_0}^\\infty \\left( \\theta - \\theta_0 \\right) \\pi(\\theta \\mid \\textbf{y}) d\\theta  \\right) \\\\\n    & =  \\left( \\theta_0 - \\theta_0 \\right) \\pi(\\theta_0 \\mid \\textbf{y}) - \\int_{-\\infty}^{\\theta_0} \\pi(\\theta \\mid \\textbf{y}) d\\theta - \\left( \\theta_0 - \\theta_0 \\right) \\pi(\\theta_0 \\mid \\textbf{y}) + \\int_{\\theta_0}^\\infty \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = - \\int_{-\\infty}^{\\theta_0} \\pi(\\theta \\mid \\textbf{y}) d\\theta + \\int_{\\theta_0}^\\infty \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    \\int_{-\\infty}^{\\theta_0} \\pi(\\theta \\mid \\textbf{y}) d\\theta & = \\int_{\\theta_0}^\\infty \\pi(\\theta \\mid \\textbf{y}) d\\theta\n\\end{align*}\\]\n(recalling that \\(\\int_{-\\infty}^x f(y) dy = f(x)\\) and \\(\\int_x^\\infty f(y) dy = -f(x)\\) and applying chain rule), and note that these two integrals are equal when \\(\\theta_0\\) is the posterior median.\n\nProblem 2: Show that the posterior mode is the decision rule that minimizes risk with respect to 0-1 loss,\n\\[\nL(\\hat{\\theta}, \\theta) = \\begin{cases} 0 & \\text{if $\\hat{\\theta} = \\theta$} \\\\ 1 & \\text{if $\\hat{\\theta} \\neq \\theta$}\\end{cases}\n\\]\nwhen \\(\\theta\\) is a discrete random variable.\n\n\nSolution:\n\nNote that we can rewrite the 0-1 loss function as \\(L(\\hat{\\theta},\\theta) = 1 - I\\{\\hat{\\theta} = \\theta\\}\\). Then we can write,\n\\[\\begin{align*}\n    R(\\theta_0, \\theta) & = E[L(\\theta_0, \\theta) \\mid \\textbf{Y}] \\\\\n    & = \\sum_{\\theta} L(\\theta_0, \\theta) \\pi(\\theta \\mid \\textbf{y})  \\\\\n    & = \\sum_{\\theta} \\left( 1 - I\\{\\theta_0 = \\theta\\} \\right) \\pi(\n    \\theta \\mid \\textbf{y} )  \\\\\n    & = \\sum_{\\theta} \\pi(\n    \\theta \\mid \\textbf{y} ) - \\sum_{\\theta} I\\{\\theta_0 = \\theta\\} \\pi(\n    \\theta \\mid \\textbf{y})  \\\\\n    & = 1 - \\pi(\\theta_0 \\mid \\textbf{y})\n\\end{align*}\\]\nsince pmfs sum to \\(1\\). Then taking the derivative and setting this equal to zero gives\n\\[\\begin{align*}\n    0 & \\equiv \\frac{\\partial}{\\partial \\theta_0} R(\\theta_0, \\theta) \\\\\n    & = \\frac{\\partial}{\\partial \\theta_0}  \\left( 1 - \\pi(\\theta_0 \\mid \\textbf{y}) \\right) \\\\\n    & = \\frac{\\partial}{\\partial \\theta_0} \\pi(\\theta_0 \\mid \\textbf{y})\n\\end{align*}\\]\nand the solution to this equation is, by definition, the posterior mode.\n\nNote: For the case where \\(\\theta\\) is a continuous random variable, we need something called a Dirac delta function to prove this. The reasoning for why we need this (and the proof, which is similar to the discrete case) is given below.\n\n\nSolution for continuous \\(\\theta\\) :\n\nNote that we can rewrite the 0-1 loss function as \\(L(\\hat{\\theta},\\theta) = 1 - \\delta\\{\\hat{\\theta} - \\theta\\}\\), where \\(\\delta\\) is the Dirac delta function. Then we can write,\n\\[\\begin{align*}\n    R(\\theta_0, \\theta) & = E[L(\\theta_0, \\theta) \\mid \\textbf{Y}] \\\\\n    & = \\int L(\\theta_0, \\theta) \\pi(\\theta \\mid \\textbf{y}) d\\theta \\\\\n    & = \\int \\left( 1 - \\delta(\\theta_0 - \\theta) \\right) \\pi(\n    \\theta \\mid \\textbf{y} ) d\\theta  \\\\\n    & = \\int \\pi(\n    \\theta \\mid \\textbf{y} ) d\\theta  - \\int \\delta(\\theta_0 - \\theta) \\pi(\n    \\theta \\mid \\textbf{y}) d\\theta   \\\\\n    & = 1 - \\pi(\\theta_0 \\mid \\textbf{y})\n\\end{align*}\\]\nsince pdfs integrate to \\(1\\). The reason why we can’t use the same indicator definition as for the discrete case is because the integral of an indicator that is only positive at a single observation is zero. The Dirac delta function, on the other hand, has positive mass (equal to 1) at \\(\\theta_0 - \\theta = 0\\). Take Projects in Real Analysis to learn more! Taking the derivative and setting this equal to zero gives the same result as in the discrete case.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Theory</span>"
    ]
  },
  {
    "objectID": "computation.html",
    "href": "computation.html",
    "title": "10  Computational Optimization",
    "section": "",
    "text": "10.1 Newton-Raphson\nWelcome to the last chapter of the course notes! There are no worked examples or concept questions for this chapter, which is instead focused on practical implementation of a handful of useful algorithms, and useful computational techniques that you may come across in your future, statistical career. Go forth and compute!\nRecall from the second chapter of the course notes the typical procedure for finding an MLE:\nWe previously saw that sometimes this procedure doesn’t work, in particular, when the support of the density function depends on our unknown parameters. In these cases, we noted that the MLE would be an order statistic. There are other situations, however, where neither the MLE is neither readily found analytically nor is it an order statistic. In these cases, we turn to computational techniques, such as Newton-Raphson.\nNewton-Raphson is a root-finding algorithm, and hence useful when trying to maximize a function (or a likelihood!). Suppose we want to find a root (i.e., the value of \\(x\\) such that \\(f(x) = 0\\)) of the function \\(f\\) with derivative denoted \\(f'\\). Newton-Raphson takes the following steps:\nA maximum likelihood estimator is the root of the first derivative of the log-likelihood (a.k.a. the value at which the derivative of the log-likelihood crosses zero). This means that, for finding MLEs, the Newton-Raphson algorithm replaces \\(f = \\frac{\\partial}{\\partial \\theta} \\log L(\\theta)\\).\nWe can visualize this process as follows:\nThe equation of the tangent line to the curve \\(y = f(x)\\) at a point \\(x = x_n\\) is \\[y = f'(x_n)(x-x_n) + f(x_n)\\]\nThe root of this tangent line (i.e., the place where it crosses the x-axis) is easy to find: \\[0 = f'(x_n)(x-x_n) + f(x_n) \\iff x = x_n - f(x_n)/f'(x_n)\\]\nTake this root of the tangent line as our next guess, then repeat…\n…and repeat…\n…and repeat…\n…and repeat…\n…and repeat…\n…and keep repeating until you’ve converged!\nThe multivariate version of Newton-Raphson is called the Scoring algorithm (also sometimes called Fisher’s scoring), and is used in \\(\\texttt{R}\\) to obtain estimates of logistic regression coefficients.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Computational Optimization</span>"
    ]
  },
  {
    "objectID": "computation.html#newton-raphson",
    "href": "computation.html#newton-raphson",
    "title": "10  Computational Optimization",
    "section": "",
    "text": "Find the log likelihood\nTake a derivative with respect to the unknown parameter(s)\nSet it equal to zero, and solve\n\n\n\n\nStart with an initial guess \\(x_0\\)\nUpdate your guess according to \\(x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\\)\nRepeat step 2 according to \\(x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}\\) until your guesses have “converged” (i.e. are very very similar)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotivating Example: Logistic Regression\nSuppose that we observe data \\((y_i, x_i)\\) where the outcome \\(y\\) is binary. A natural model for these data is to assume the statistical model\n\\[\\begin{align*}\ny_i & \\sim Bernoulli(p_i), \\\\\n\\text{log} \\left( \\frac{p_i}{1 - p_i} \\right) & = \\beta_0 + \\beta_1 x_i.\n\\end{align*}\\]\nThis is a simple logistic regression model, with unknown parameters given by the logistic regression coefficients \\(\\beta_0, \\beta_1\\). Let’s attempt to find MLEs for \\(\\beta_0\\) and \\(\\beta_1\\) analytically.\nNote that \\(p_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}\\). Then the likelihood of our Bernoulli observations \\(y_i\\) can be written as\n\\[\nL(\\beta_0, \\beta_1) = \\prod_{i = 1}^n \\left( \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right)^{y_i} \\left( \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right)^{1 - y_i}\n\\]\nFollowing the typical procedure, we log the likelihood…\n\\[\\begin{align*}\n    \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ y_i \\log(\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}) + (1 - y_i) \\log(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}) \\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ y_i (\\beta_0 + \\beta_1 x_i) - y_i \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) - \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) + y_i \\log(1 + e^{\\beta_0 + \\beta_1 x_i})\\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ y_i (\\beta_0 + \\beta_1 x_i)  - \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) \\right]\n\\end{align*}\\]\n…taking the partial derivatives with respect to \\(\\beta_0\\) and \\(\\beta_1\\) we get…\n\\[\\begin{align*}\n    \\frac{\\partial}{\\partial \\beta_0} \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}  \\right]\\\\\n    \\frac{\\partial}{\\partial \\beta_1} \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ x_i \\left( y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right) \\right]\n\\end{align*}\\]\n…and if you try to solve the system of equations given by\n\\[\\begin{align*}\n    0 & \\equiv \\sum_{i = 1}^n \\left[ y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}  \\right]\\\\\n    0 & \\equiv \\sum_{i = 1}^n \\left[ x_i \\left( y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right) \\right]\n\\end{align*}\\]\nyou’ll get nowhere! There is no analytical (sometimes called “closed-form”) solution. In this case, we’d need to use the Scoring algorithm to solve for the regression coefficient estimates, since we have more than one unknown parameter.\n\n\nWhy do anything analytically, if Newton-Raphson exists?\nYou may be wondering why you’ve been doing calculus/algebra the entire semester, when such an algorithm exists. The answer is two-fold.\n\nGoing through the steps of finding an MLE analytically helps build intuition. We saw that in the vast majority of cases, maximum likelihood estimators are functions of sample means. This is less obvious when doing everything numerically (using an algorithm). In addition to gaining insight from finding MLEs by hand, this practice also gave you the opportunity to learn/use common “tricks” in statistics, that will find their way into problems you complete down the road or research you may eventually conduct.\nNumerical optimization is slow. For simple cases like the ones we’ve seen in class, numerical optimization would techniques like Newton-Raphson would run relatively quickly. However, for more complex likelihoods with many unknown parameters, various optimization techniques can be so slow as to be computationally prohibitive. Even with continual improvements in computational power (and improvements in the algorithms themselves), computational speed is an important consideration when conducting statistical research or developing new methodology. If it takes someone two weeks to fit their regression model using numerical optimization, for example, that person may never fit a regression model ever again, or give up entirely. Especially when considering who has access to computational power, this can become an equity issue. If you can solve something analytically, do it. It’s significantly faster in the long-term, even it takes you some time to do the calculus/algebra.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Computational Optimization</span>"
    ]
  },
  {
    "objectID": "computation.html#simulation-studies",
    "href": "computation.html#simulation-studies",
    "title": "10  Computational Optimization",
    "section": "10.2 Simulation Studies",
    "text": "10.2 Simulation Studies\nSometimes proofs are hard. In such cases (and more generally), it can often be useful to “test” or observe properties of estimators in a computational setting, rather than in a rigorous mathematical context. This is where simulation studies come into play, and if you eventually find yourself conducting statistical research, knowing how to conduct a well-designed, reproducible, simulation study is an incredibly important skill.\nThe general idea of simulation study is to generate realistic settings (data) that could be observed in the real world, in order to compare properties of various estimators and their behavior in scenarios where the “truth” is known (because you generated the truth!). Steps include:\n\nDetermine your simulation settings (different parameter values, sample sizes, etc.)\nGenerate many data sets for each setting\nCompute your estimator / implement your method for each data set\nRecord the relevant property of that estimator / method for each data set\nSummarize your results across data sets and simulation settings\n\nThis can be a great way to get a feel for how certain estimators/methods behave in different settings without needing to rigorously prove something. Additionally, it can be used to inform more rigorous proofs down the line; if we can better understand how estimators/methods behave, we may be able to relate that behavior to existing proofs and build upon them!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Computational Optimization</span>"
    ]
  },
  {
    "objectID": "computation.html#gibbs-samplers",
    "href": "computation.html#gibbs-samplers",
    "title": "10  Computational Optimization",
    "section": "10.3 Gibbs Samplers",
    "text": "10.3 Gibbs Samplers\nNot everything is conjugate. In cases where we don’t have conjugate priors, posterior distributions may not have closed-form, analytical pdfs, and instead we rely on Markov-chain Monte-Carlo (MCMC) algorithms (or Laplace approximations) to generate samples from posteriors.\nAs noted in the Bayes chapter of our course notes, Bayes Rules! is a great place to go for an introduction to Bayesian statistics. Here, we’ll talk through one (classical) example of an MCMC approach to posterior inference; Gibbs Samplers.\nThe gist of Gibbs Sampling is that, when we have more than one unknown parameter, we can obtain the joint posterior distribution for all parameters by updating our guesses about each parameter, one at a time. This involves working with what are typically called full conditionals (the distribution of each parameter conditional on everything else).\nThe Gibbs Sampling algorithm is as follows:\n\nChoose initial values for each unknown parameter, \\(\\theta_1^{(0)}\\), \\(\\theta_2^{(0)}\\), …, \\(\\theta_p^{(0)}\\)\nSample \\(\\theta_{1}^{(0)} \\sim \\pi(\\theta_{1}^{(0)} \\mid \\theta_{2}^{(0)}, \\dots, \\theta_{p}^{(0)},\\textbf{y})\\)\nSample \\(\\theta_{2}^{(0)} \\sim \\pi(\\theta_{2}^{(0)} \\mid, \\theta_{1}^{(0)}, \\theta_{3}^{(0)}, \\dots, \\theta_{p}^{(0)},\\textbf{y})\\)\n…\nSample \\(\\theta_{p}^{(0)} \\sim \\pi(\\theta_{p}^{(0)} \\mid \\theta_{1}^{(0)}, \\dots, \\theta_{p-1}^{(0)},\\textbf{y})\\)\nRepeat many times, always sampling new observations conditional on your most recent guess (iteration) for each parameter!\n\nIt feels almost magical, but the end result is that we obtain many samples from the joint posterior distribution for all unknown parameters! MCMC methods such as Gibbs Samplers are what is known as “exact” methods for conducting Bayesian inference, because so long as sampling goes according to plan*, the posterior draws will be from the exactly correct, joint posterior distribution. This is opposed to Laplace approximation techniques which are, by definition, “approximate.”\n*Let’s define “according to plan.” Sometimes algorithms can go wrong. We saw an example of this with Newton-Raphson, where if we pick a terrible starting value, the algorithm can sometimes diverge. With Gibbs Samplers, we should be careful of checking convergence diagnostics. A visual tool for this is called a Trace Plot. Trace plots show the values of parameters that are being sampled across iterations. The values across iterations are referred to as chains.\nHere are some examples of chains that have converged:\n\nThere are many other convergence diagnostics you will need to consider if you end up doing research involving MCMC algorithms. A recent research paper on convergence diagnostics that is generally accepted now as best practice among Bayesian statisticians can be found here.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Computational Optimization</span>"
    ]
  }
]