{
  "hash": "f612dfd2486c0625a79e662721d24044",
  "result": {
    "markdown": "# Computational Optimization\n\nWelcome to the last chapter of the course notes! There are no worked examples for this chapter, which is instead focused on practical implementation of a handful of useful algorithms, and useful computational techniques that you may come across in your future, statistical career. Go forth and compute!\n\n## Newton-Raphson\n\nRecall from the second chapter of the course notes the typical procedure for finding an MLE:\n\n1.  Find the log likelihood\n\n2.  Take a derivative with respect to the unknown parameter(s)\n\n3.  Set it equal to zero, and solve\n\nWe previously saw that *sometimes* this procedure doesn't work, in particular, when the support of the density function depends on our unknown parameters. In these cases, we noted that the MLE would be an order statistic. There are *other* situations, however, where neither the MLE is neither readily found analytically nor is it an order statistic. In these cases, we turn to computational techniques, such as **Newton-Raphson**.\n\nNewton-Raphson is a root-finding algorithm, and hence useful when trying to maximize a function (or a likelihood!). Suppose we want to find a root (i.e., the value of $x$ such that $f(x) = 0$) of the function $f$ with derivative denoted $f'$. Newton-Raphson takes the following steps:\n\n1.  Start with an initial guess $x_0$\n\n2.  Update your guess according to $x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$\n\n3.  Repeat step 2 according to $x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}$ until your guesses have \"converged\" (i.e. are very very similar)\n\nWe can visualize this process as follows:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](computation_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=336}\n:::\n:::\n\n\nThe equation of the tangent line to the curve $y = f(x)$ at a point $x = x_n$ is $$y = f'(x_n)(x-x_n) + f(x_n)$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](computation_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=336}\n:::\n:::\n\n\nThe root of this tangent line (i.e., the place where it crosses the x-axis) is easy to find: $$0 = f'(x_n)(x-x_n) + f(x_n) \\iff x = x_n - f(x_n)/f'(x_n)$$\n\nTake this root of the tangent line as our next guess, then repeat...\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](computation_files/figure-html/newton-step1-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n...and repeat...\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](computation_files/figure-html/newton-step2-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n...and repeat...\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](computation_files/figure-html/newton-step3-1.png){fig-align='center' width=432}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](computation_files/figure-html/newton-step4-1.png){fig-align='center' width=432}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](computation_files/figure-html/newton-step5-1.png){fig-align='center' width=432}\n:::\n:::\n\n\n...until you've converged!\n\n### Motivating Example: Logistic Regression {.unnumbered .unlisted}\n\nSuppose that we observe data $(y_i, x_i)$ where the outcome $y$ is binary. A natural model for these data is to assume the statistical model\n\n\n```{=tex}\n\\begin{align*}\ny_i & \\sim Bernoulli(p_i), \\\\\n\\text{log} \\left( \\frac{p_i}{1 - p_i} \\right) & = \\beta_0 + \\beta_1 x_i.\n\\end{align*}\n```\n\nThis is a simple logistic regression model, with unknown parameters given by the logistic regression coefficients $\\beta_0, \\beta_1$. Let's attempt to find MLEs for $\\beta_0$ and $\\beta_1$ analytically.\n\nNote that $p_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}$. Then the likelihood of our Bernoulli observations $y_i$ can be written as\n\n$$\nL(\\beta_0, \\beta_1) = \\prod_{i = 1}^n \\left( \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right)^{y_i} \\left( \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right)^{1 - y_i}\n$$\n\nFollowing the typical procedure, we log the likelihood...\n\n\n```{=tex}\n\\begin{align*}\n    \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ y_i \\log(\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}) + (1 - y_i) \\log(\\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}) \\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ y_i (\\beta_0 + \\beta_1 x_i) - y_i \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) - \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) + y_i \\log(1 + e^{\\beta_0 + \\beta_1 x_i})\\right] \\\\\n    & = \\sum_{i = 1}^n \\left[ y_i (\\beta_0 + \\beta_1 x_i)  - \\log(1 + e^{\\beta_0 + \\beta_1 x_i}) \\right]\n\\end{align*}\n```\n\n...taking the partial derivatives with respect to $\\beta_0$ and $\\beta_1$ we get...\n\n\n```{=tex}\n\\begin{align*}\n    \\frac{\\partial}{\\partial \\beta_0} \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}  \\right]\\\\\n    \\frac{\\partial}{\\partial \\beta_1} \\log(L(\\beta_0, \\beta_1)) & = \\sum_{i = 1}^n \\left[ x_i \\left( y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right) \\right]\n\\end{align*}\n```\n\n...and if you try to solve the system of equations given by\n\n\n```{=tex}\n\\begin{align*}\n    0 & \\equiv \\sum_{i = 1}^n \\left[ y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}  \\right]\\\\\n    0 & \\equiv \\sum_{i = 1}^n \\left[ x_i \\left( y_i -\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} \\right) \\right]\n\\end{align*}\n```\n\nyou'll get nowhere! There is no analytical (sometimes called \"closed-form\") solution.\n\n### Why do anything analytically, if Newton-Raphson exists? {.unnumbered .unlisted}\n\n## Simulation Studies\n\n## Gibbs Samplers\n",
    "supporting": [
      "computation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}