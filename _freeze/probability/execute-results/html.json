{
  "hash": "77fc1a5e3b7f7423895d7e7b1c4c3da5",
  "result": {
    "markdown": "---\noutput: html_document\neditor_options: \n  chunk_output_type: inline\n---\n\n\n# Probability: A Brief Review\n\n*MATH/STAT 455* builds directly on topics covered in *MATH/STAT 354: Probability*. You're not expected to perfectly remember everything from *Probability*, but you will need to have sufficient facility with the following topics covered in this review Chapter in order to grasp the majority of concepts covered in *MATH/STAT 455*.\n\n## Learning Objectives\n\n\n::: {.cell}\n\n:::\n\n\nBy the end of this chapter, you should be able to...\n\n-   Distinguish between important probability models (e.g., Normal, Binomial)\n\n-   Derive the expectation and variance of a single random variable or a sum of random variables\n\n-   Define the moment generating function and use it to find moments or identify pdfs\n\n## Associated Readings\n\nChapters 2-4 (pages 15-277)\n\n## Definitions\n\nYou are expected to know the following definitions:\n\n-   Probability density function (discrete, continuous)\n\n    -   Note: I don't care if you call a pmf a pdf... I will probably do this continuously throughout the semester. We don't need to be picky about this in *MATH/STAT 455*.\n\n-   Cumulative distribution function (discrete, continuous)\n\n-   Joint probability density function\n\n-   Conditional probability density function\n\n-   Independence\n\n-   Random Variable\n\n-   Expected Value / Expectation\n\n-   Variance\n\n-   $r^{th}$ moment\n\n-   Covariance\n\n-   Random Sample\n\n-   Moment Generating Function\n\nYou are expected to know the following probability distributions:\n\n| Distribution        | PDF/PMF                                                                                                                    | Parameters                                                                           |\n|------------------------|------------------------|------------------------|\n| Uniform             | $\\pi(x) = \\frac{1}{\\beta - \\alpha}$                                                                                        | $\\alpha \\in \\mathbb{R}$, $\\beta\\in \\mathbb{R}$                                       |\n| Normal              | $\\pi(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{1}{2\\sigma^2} (x - \\mu)^2)$                                           | $\\mu \\in \\mathbb{R}$, $\\sigma > 0$                                                   |\n| Multivariate Normal | $\\pi(\\textbf{x}) - (2\\pi)^{-k/2} |\\Sigma|^{-1/2} \\exp(-\\frac{1}{2}(\\textbf{x} - \\mu)^\\top \\Sigma^{-1}(\\textbf{x} - \\mu)))$ | $\\mu \\in \\mathbb{R}^k$, $\\Sigma \\in \\mathbb{R}^{k\\times k}$ , positive semi-definite |\n| Gamma               | $\\pi(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}$                                                | $\\alpha \\text{ (shape)}, \\beta \\text{ (rate)} > 0$                                   |\n| Chi-square          | $\\pi(x) = \\frac{2^{-\\nu/2}}{\\Gamma(\\nu/2)} x^{\\nu/2 - 1}e^{-x/2})$                                                         | $\\nu > 0$                                                                            |\n| Exponential         | $\\pi(x) = \\beta e^{-\\beta x}$                                                                                              | $\\beta > 0$                                                                          |\n| Student-\\$t\\$       | $\\pi(x) = \\frac{\\Gamma((\\nu + 1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu \\pi}} (1 + \\frac{x^2}{\\nu})^{-(\\nu + 1)/2}$                   | $\\nu > 0$                                                                            |\n| Beta                | $\\pi(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha - 1}(1 - x)^{\\beta - 1}$                    | $\\alpha, \\beta > 0$                                                                  |\n| Poisson             | $\\pi(x) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$                                                                               | $\\lambda > 0$                                                                        |\n| Binomial            | $\\pi(x) = \\binom{n}{x} p^{x} (1 - p)^{n - x}$                                                                              | $p \\in [0,1], n = \\{0, 1, 2, \\dots\\}$                                                |\n| Multinomial         | $\\pi(\\textbf{x}) = \\frac{n!}{x_1! \\dots x_k!} p_1^{x_1} \\dots p_k^{x_k}$                                                   | $p_i > 0$, $p_1 + \\dots + p_k = 1$, $n = \\{0, 1, 2, \\dots \\}$                        |\n| Negative Binomial   | $\\pi(x) = \\binom{k + r - 1}{k} (1-p)^k p^r$                                                                                | $r > 0$, $p \\in [0,1]$                                                               |\n\n: Table of main probability distributions we will work with for *MATH/STAT 455*.\n\n## Theorems\n\n-   Law of Total Probability\n\n    $$\n    P(A) = \\sum_n P(A \\cap B_n),\n    $$or\n\n    $$\n    P(A) = \\sum_n P(A \\mid B_n) P(B_n)\n    $$\n\n-   Bayes' Theorem\n\n    $$\n    \\pi(A \\mid B) = \\frac{\\pi(B \\mid A) \\pi(A)}{\\pi(B)}\n    $$\n\n-   Relationship between pdf and cdf\n\n    $$\n    F_Y(y) = \\int_{-\\infty}^y f_Y(t)dt\n    $$\n\n    $$\n    \\frac{\\partial}{\\partial y}F_Y(y) = f_Y(y)\n    $$\n\n-   Expectation of random variables\n\n    $$\n    E[X] = \\int_{-\\infty}^\\infty x f(x) dx\n    $$\n\n    $$\n    E[X^2] = \\int_{-\\infty}^\\infty x^2 f(x) dx\n    $$\n\n-   Expectation and variance of linear transformations of random variables\n\n    $$\n    E[cX + b] = c E[X] + b\n    $$\n\n    $$\n    Var[cX + b] = c^2 Var[X]\n    $$\n\n-   Relationship between mean and variance\n\n    $$\n    Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n    $$\n\n    Also, recall that $Cov[X, X] = Var[X]$.\n\n-   Finding a marginal pdf from a joint pdf\n\n    $$\n    f_X(x) = \\int_{-\\infty}^\\infty f_{XY}(x, y) dy\n    $$\n\n-   Independence of random variables and joint pdfs\n\n    If two random variables are independent, their joint pdf will be *separable*. For example, if $X$ and $Y$ are independent, we could write\n\n    $$\n    f_{XY}(x, y) = f_{X}(x)f_Y(y)\n    $$\n\n-   Expected value of a product of independent random variables\n\n    Suppose random variables $X_1, \\dots, X_n$ are independent. Then we can write,\n\n    $$\n    E\\left[\\prod_{i = 1}^n X_i\\right] = \\prod_{i = 1}^n E[X_i]\n    $$\n\n-   Covariance of independent random variables\n\n    If $X$ and $Y$ are independent, then $Cov(X, Y) = 0$. We can show this by noting that\n\n$$\n\\begin{align}\nCov(X, Y) & = E[(X - E[X])(Y - E[Y])] \\\\\n& = E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\\\\n& = E[XY] - E[XE[Y]] - E[YE[X]] + E[X]E[Y] \\\\\n& =  2E[X]E[Y] - 2E[X]E[Y] \\\\\n& = 0\n\\end{align}\n$$\n\n-   Using MGFs to find moments\n\n    Recall that the moment generating function of a random variable $X$, denoted by $M_X(t)$ is\n\n    $$\n    M_X(t) = E[e^{tX}]\n    $$\n\n    Then the $n$th moment of the probability distribution for $X$ , $E[X^n]$, is given by\n\n    $$\n    \\frac{\\partial M_X}{\\partial t^n} \\Bigg|_{t = 0} \n    $$\n\n    where the above reads as \"the $n$th derivative of the moment generating function, evaluated at $t = 0$.\"\n\n-   Using MGFs to identify pdfs\n\n    MGFs uniquely identify probability density functions. If $X$ and $Y$ are two random variables where for all values of $t$, $M_X(t) = M_Y(t)$, then $F_X(x) = F_Y(y)$.\n\n-   Central Limit Theorem\n\n    The classical CLT states that for independent and identically distributed (iid) random variables $X_1, \\dots, X_n$, with expected value $E[X_i] = \\mu$ and $Var[X_i] = \\sigma^2 < \\infty$, the sample average (centered and standardized) converges in distribution to a standard normal distribution at a root-$n$ rate. Notationally, this is written as\n\n    $$\n    \\sqrt{n} (\\bar{X} - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)\n    $$\n\n    ![](images/chilipepper.png){width=\"20\" height=\"16\"} A fun aside: this is only *one* CLT, often referred to as the Levy CLT. There are other CLTs, such as the Lyapunov CLT and Lindeberg-Feller CLT!\n\n## Worked Examples\n\n**Problem 1:** Suppose $X \\sim Exponential(\\lambda)$. Calculate $E[X]$ and $Var[X]$.\n\n**Problem 2:** Show that an exponentially distributed random variable is \"memoryless\", i.e. show that $\\Pr(X > s + x \\mid X > s) = \\Pr(X > x)$, $\\forall s, t\\geq 0$.\n\n**Problem 3:** Suppose we have two, independent random variables $X, Y \\sim Exponential(\\lambda).$ Show that $\\frac{X}{X + Y} \\sim Uniform(0,1)$.\n\n**Problem 4:**\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}