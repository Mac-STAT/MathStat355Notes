# Maximum Likelihood Estimation

In *Probability*, you calculated probabilities of events by assuming a probability model for data and then *assuming you knew the value of the parameters* in that model. In *Mathematical Statistics*, we will similarly write down a probability model but then we will use observed data to *estimate the value of the parameters* in that model.

There is more than one technique that you can use to estimate the value of an unknown parameter. You're already familiar with one technique---**least squares estimation**---from *STAT 155*. We'll review the ideas behind that approach later in the course. To start, we\'ll explore two other widely used estimation techniques: **maximum likelihood estimation** (this chapter) and the **method of moments** (next chapter).

### Introduction to MLE

To understand maximum likelihood estimation, we can first break down each individual word in that phrase: (1) maximum, (2) likelihood, (3) estimation. We'll start in reverse order.

Recall from your introductory statistics course that we are (often) interested in estimating *true, unknown **parameters*** in statistics, using some data. Our best guess at the truth, based on the data we observe / sample that we have, is an ***estimate*** of the truth (given some modeling assumptions). This is all the "estimation" piece is getting at here. We're going to be learning about a method that produces estimates!

The likelihood piece may be less familiar to you. A likelihood is essentially a fancy form of a function (see the Definitions section for an *exact* definition), that combines an assumed probability distribution for your data, with some unknown parameters.\* The key here is that a likelihood is a *function*. It may *look* more complicated than a function like $y = mx + b$, but we can often manipulate them in a similar fashion, which comes in handy when trying to find the...

Maximum! We've maximized functions before, and we can do it again! There are ways to maximize functions numerically (using certain algorithms, such as Newton-Raphson for example, which we'll cover in a later chapter), but we will primarily focus on maximizing likelihoods *analytically* in this course to help us build intuition.

Recall from calculus: To maximize a function we...

1.  Take the derivative of the function

2.  Set the derivative equal to zero

3.  Solve!

4.  (double check that the second derivative is negative, so that it's actually a maximum as opposed to a minimum)

5.  (also check the endpoints)

The last two steps we'll often skip in this class, since things have a tendency to work out nicely with most likelihood functions. One final thing to note (before checking out worked examples and making sure you have a grasp on definitions and theorems) is that it is often *easier*

\*![](images/chilipepper.png){width="20" height="16"} Note that distributions are only involved in *parametric* methods, as opposed to non-parametric and semi-parametric methods, the latter of which are for independent study or a graduate course in statistics!

## Learning Objectives

By the end of this chapter, you should be able to...

-   Derive maximum likelihood estimators for parameters of common probability density functions

-   Calculate maximum likelihood estimators "by hand" for common probability density functions

-   Explain (in plain English) why maximum likelihood estimation is an intuitive approach to estimating unknown parameters using a combination of (1) observed data, and (2) a distributional assumption

## Reading Guide

Associated Readings: Chapter 5 (Introduction through Example 5.2.5)

### Reading Questions

1.  What is the intuition behind the maximum likelihood estimation (MLE) approach?

2.  What are the typical steps to find a MLE? (see Ex 5.2.1, 5.2.2, and
    Case Study 5.2.1; work through at least one of these examples in detail,
    filling in any steps that the textbook left out)

3.  Are there ever situations when the typical steps to finding a MLE
    don\'t work? If so, what can we do instead to find the MLE? (see Ex
    5.2.3, 5.2.4)

4.  How do the steps to finding a MLE change when we have more than one unknown parameter? (see Ex 5.2.5)

## Definitions

You are expected to know the following definitions:

**Parameter**

Explanation

**Statistic/Estimator**

Explanation

**Likelihood Function**

Explanation

**Maximum Likelihood Estimate (MLE)**

Explanation

**Log-likelihood**

Explanation

**Order Statistic**

Explanation

## Theorems

## Worked Examples

**Problem 1:**
